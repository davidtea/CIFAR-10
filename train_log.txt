2017-03-25 20:40:52.213840: step 0, loss = 4.68 (57.7 examples/sec; 2.217 sec/batch)
2017-03-25 20:40:53.710554: step 10, loss = 4.65 (855.2 examples/sec; 0.150 sec/batch)
2017-03-25 20:40:55.402811: step 20, loss = 4.47 (756.4 examples/sec; 0.169 sec/batch)
2017-03-25 20:40:57.152037: step 30, loss = 4.38 (731.7 examples/sec; 0.175 sec/batch)
2017-03-25 20:40:58.892787: step 40, loss = 4.33 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:41:00.625331: step 50, loss = 4.23 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:41:02.365615: step 60, loss = 4.19 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:41:04.115412: step 70, loss = 4.41 (731.5 examples/sec; 0.175 sec/batch)
2017-03-25 20:41:05.848707: step 80, loss = 4.18 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:41:07.593602: step 90, loss = 4.08 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:41:09.540612: step 100, loss = 4.01 (657.8 examples/sec; 0.195 sec/batch)
2017-03-25 20:41:11.109815: step 110, loss = 4.40 (815.0 examples/sec; 0.157 sec/batch)
2017-03-25 20:41:12.846828: step 120, loss = 3.89 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:41:14.581676: step 130, loss = 3.89 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:41:16.313706: step 140, loss = 4.01 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:41:18.044211: step 150, loss = 3.81 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:41:19.760888: step 160, loss = 4.01 (745.6 examples/sec; 0.172 sec/batch)
2017-03-25 20:41:21.486970: step 170, loss = 3.92 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:41:23.223789: step 180, loss = 3.91 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:41:24.952728: step 190, loss = 3.80 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:41:26.888276: step 200, loss = 4.03 (661.3 examples/sec; 0.194 sec/batch)
2017-03-25 20:41:28.483095: step 210, loss = 3.96 (802.6 examples/sec; 0.159 sec/batch)
2017-03-25 20:41:30.221266: step 220, loss = 3.82 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:41:31.949264: step 230, loss = 3.71 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:41:33.681684: step 240, loss = 3.75 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:41:35.423000: step 250, loss = 3.71 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 20:41:37.160379: step 260, loss = 3.63 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:41:38.905744: step 270, loss = 3.61 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 20:41:40.625890: step 280, loss = 3.49 (744.1 examples/sec; 0.172 sec/batch)
2017-03-25 20:41:42.373438: step 290, loss = 3.78 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 20:41:44.292648: step 300, loss = 3.45 (667.2 examples/sec; 0.192 sec/batch)
2017-03-25 20:41:45.885000: step 310, loss = 3.41 (803.5 examples/sec; 0.159 sec/batch)
2017-03-25 20:41:47.618768: step 320, loss = 3.54 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:41:49.348763: step 330, loss = 3.36 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:41:51.087555: step 340, loss = 3.60 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:41:52.821322: step 350, loss = 3.52 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:41:54.556226: step 360, loss = 3.48 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:41:56.294721: step 370, loss = 3.56 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:41:58.022941: step 380, loss = 3.34 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:41:59.772025: step 390, loss = 3.32 (731.8 examples/sec; 0.175 sec/batch)
2017-03-25 20:42:01.784525: step 400, loss = 3.30 (636.3 examples/sec; 0.201 sec/batch)
2017-03-25 20:42:03.253463: step 410, loss = 3.26 (870.9 examples/sec; 0.147 sec/batch)
2017-03-25 20:42:04.943340: step 420, loss = 3.28 (757.5 examples/sec; 0.169 sec/batch)
2017-03-25 20:42:06.675250: step 430, loss = 3.37 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:42:08.411186: step 440, loss = 3.37 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:42:10.132138: step 450, loss = 3.25 (743.8 examples/sec; 0.172 sec/batch)
2017-03-25 20:42:11.877159: step 460, loss = 3.26 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 20:42:13.617893: step 470, loss = 3.29 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:42:15.344238: step 480, loss = 3.35 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 20:42:17.080371: step 490, loss = 3.17 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:42:19.050031: step 500, loss = 3.37 (649.9 examples/sec; 0.197 sec/batch)
2017-03-25 20:42:20.522290: step 510, loss = 3.09 (869.4 examples/sec; 0.147 sec/batch)
2017-03-25 20:42:22.211364: step 520, loss = 3.06 (757.8 examples/sec; 0.169 sec/batch)
2017-03-25 20:42:23.907071: step 530, loss = 3.03 (754.9 examples/sec; 0.170 sec/batch)
2017-03-25 20:42:25.590988: step 540, loss = 3.10 (760.1 examples/sec; 0.168 sec/batch)
2017-03-25 20:42:27.278395: step 550, loss = 3.16 (758.6 examples/sec; 0.169 sec/batch)
2017-03-25 20:42:28.965448: step 560, loss = 3.08 (758.7 examples/sec; 0.169 sec/batch)
2017-03-25 20:42:30.646371: step 570, loss = 2.93 (761.5 examples/sec; 0.168 sec/batch)
2017-03-25 20:42:32.336470: step 580, loss = 2.96 (757.4 examples/sec; 0.169 sec/batch)
2017-03-25 20:42:34.018174: step 590, loss = 2.96 (761.1 examples/sec; 0.168 sec/batch)
2017-03-25 20:42:35.888221: step 600, loss = 3.02 (685.0 examples/sec; 0.187 sec/batch)
2017-03-25 20:42:37.424367: step 610, loss = 2.90 (832.7 examples/sec; 0.154 sec/batch)
2017-03-25 20:42:39.113240: step 620, loss = 2.94 (757.7 examples/sec; 0.169 sec/batch)
2017-03-25 20:42:40.791413: step 630, loss = 2.93 (762.7 examples/sec; 0.168 sec/batch)
2017-03-25 20:42:42.473625: step 640, loss = 2.99 (761.0 examples/sec; 0.168 sec/batch)
2017-03-25 20:42:44.150780: step 650, loss = 2.85 (763.1 examples/sec; 0.168 sec/batch)
2017-03-25 20:42:45.829337: step 660, loss = 3.20 (762.6 examples/sec; 0.168 sec/batch)
2017-03-25 20:42:47.503507: step 670, loss = 2.75 (764.6 examples/sec; 0.167 sec/batch)
2017-03-25 20:42:49.189458: step 680, loss = 2.79 (759.2 examples/sec; 0.169 sec/batch)
2017-03-25 20:42:50.875717: step 690, loss = 3.17 (759.1 examples/sec; 0.169 sec/batch)
2017-03-25 20:42:52.817181: step 700, loss = 2.84 (659.6 examples/sec; 0.194 sec/batch)
2017-03-25 20:42:54.370159: step 710, loss = 2.80 (823.8 examples/sec; 0.155 sec/batch)
2017-03-25 20:42:56.102435: step 720, loss = 2.67 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:42:57.853722: step 730, loss = 2.88 (730.8 examples/sec; 0.175 sec/batch)
2017-03-25 20:42:59.589370: step 740, loss = 2.69 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:43:01.327819: step 750, loss = 2.84 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:43:03.078965: step 760, loss = 2.84 (731.0 examples/sec; 0.175 sec/batch)
2017-03-25 20:43:04.818636: step 770, loss = 2.57 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 20:43:06.547007: step 780, loss = 2.73 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:43:08.285180: step 790, loss = 2.80 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:43:10.194837: step 800, loss = 2.68 (670.3 examples/sec; 0.191 sec/batch)
2017-03-25 20:43:11.800911: step 810, loss = 2.77 (797.0 examples/sec; 0.161 sec/batch)
2017-03-25 20:43:13.530820: step 820, loss = 2.67 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:43:15.260205: step 830, loss = 2.52 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:43:16.995543: step 840, loss = 2.73 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:43:18.733855: step 850, loss = 2.52 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:43:20.464807: step 860, loss = 2.57 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:43:22.206017: step 870, loss = 2.59 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:43:23.956885: step 880, loss = 2.69 (731.0 examples/sec; 0.175 sec/batch)
2017-03-25 20:43:25.667447: step 890, loss = 2.57 (748.3 examples/sec; 0.171 sec/batch)
2017-03-25 20:43:27.568852: step 900, loss = 2.58 (673.5 examples/sec; 0.190 sec/batch)
2017-03-25 20:43:29.164787: step 910, loss = 2.54 (801.6 examples/sec; 0.160 sec/batch)
2017-03-25 20:43:30.902235: step 920, loss = 2.69 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:43:32.635523: step 930, loss = 2.49 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:43:34.366503: step 940, loss = 2.60 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:43:36.113659: step 950, loss = 2.42 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 20:43:37.852185: step 960, loss = 2.63 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:43:39.585737: step 970, loss = 2.56 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:43:41.315879: step 980, loss = 2.48 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:43:43.054938: step 990, loss = 2.42 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:43:44.980071: step 1000, loss = 2.35 (664.9 examples/sec; 0.193 sec/batch)
2017-03-25 20:43:46.566816: step 1010, loss = 2.49 (806.4 examples/sec; 0.159 sec/batch)
2017-03-25 20:43:48.291862: step 1020, loss = 2.38 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:43:50.010678: step 1030, loss = 2.53 (744.7 examples/sec; 0.172 sec/batch)
2017-03-25 20:43:51.743675: step 1040, loss = 2.52 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:43:53.475533: step 1050, loss = 2.36 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:43:55.214405: step 1060, loss = 2.56 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 20:43:56.941978: step 1070, loss = 2.25 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:43:58.669537: step 1080, loss = 2.15 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:00.408897: step 1090, loss = 2.22 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:44:02.327214: step 1100, loss = 2.36 (667.3 examples/sec; 0.192 sec/batch)
2017-03-25 20:44:03.907118: step 1110, loss = 2.34 (810.2 examples/sec; 0.158 sec/batch)
2017-03-25 20:44:05.633378: step 1120, loss = 2.32 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:07.356367: step 1130, loss = 2.40 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 20:44:09.097916: step 1140, loss = 2.28 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:44:10.826141: step 1150, loss = 2.20 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:12.564649: step 1160, loss = 2.26 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:44:14.308040: step 1170, loss = 2.23 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:44:16.034432: step 1180, loss = 2.15 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:17.776669: step 1190, loss = 1.99 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:44:19.707403: step 1200, loss = 2.29 (663.0 examples/sec; 0.193 sec/batch)
2017-03-25 20:44:21.262613: step 1210, loss = 2.22 (823.0 examples/sec; 0.156 sec/batch)
2017-03-25 20:44:22.997512: step 1220, loss = 2.24 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:24.728643: step 1230, loss = 2.15 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:26.463112: step 1240, loss = 2.27 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:28.189603: step 1250, loss = 2.17 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:29.932902: step 1260, loss = 2.20 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:44:31.665396: step 1270, loss = 2.34 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:33.391712: step 1280, loss = 1.83 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:35.123957: step 1290, loss = 2.07 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:37.040870: step 1300, loss = 1.92 (667.7 examples/sec; 0.192 sec/batch)
2017-03-25 20:44:38.634205: step 1310, loss = 2.10 (803.3 examples/sec; 0.159 sec/batch)
2017-03-25 20:44:40.367935: step 1320, loss = 2.14 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:42.094112: step 1330, loss = 2.29 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:43.824629: step 1340, loss = 2.10 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:45.557661: step 1350, loss = 1.99 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:47.292283: step 1360, loss = 2.23 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:49.026311: step 1370, loss = 1.86 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:44:50.747774: step 1380, loss = 2.12 (743.8 examples/sec; 0.172 sec/batch)
2017-03-25 20:44:52.483792: step 1390, loss = 2.02 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 20:44:54.475382: step 1400, loss = 2.15 (642.7 examples/sec; 0.199 sec/batch)
2017-03-25 20:44:55.962268: step 1410, loss = 1.92 (860.9 examples/sec; 0.149 sec/batch)
2017-03-25 20:44:57.654639: step 1420, loss = 2.07 (756.3 examples/sec; 0.169 sec/batch)
2017-03-25 20:44:59.381196: step 1430, loss = 2.04 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 20:45:01.107131: step 1440, loss = 1.93 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:45:02.844477: step 1450, loss = 2.18 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 20:45:04.582111: step 1460, loss = 2.11 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:45:06.310327: step 1470, loss = 1.84 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:45:08.040762: step 1480, loss = 2.01 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:45:09.766548: step 1490, loss = 1.97 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:45:11.683474: step 1500, loss = 1.98 (667.7 examples/sec; 0.192 sec/batch)
2017-03-25 20:45:13.276255: step 1510, loss = 1.91 (803.6 examples/sec; 0.159 sec/batch)
2017-03-25 20:45:15.018583: step 1520, loss = 1.99 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:45:16.733783: step 1530, loss = 2.00 (746.3 examples/sec; 0.172 sec/batch)
2017-03-25 20:45:18.456995: step 1540, loss = 2.00 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 20:45:20.198871: step 1550, loss = 1.90 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 20:45:21.923808: step 1560, loss = 1.82 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 20:45:23.661944: step 1570, loss = 1.82 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:45:25.389690: step 1580, loss = 1.91 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:45:27.126466: step 1590, loss = 1.79 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:45:29.042964: step 1600, loss = 1.99 (667.9 examples/sec; 0.192 sec/batch)
2017-03-25 20:45:30.646844: step 1610, loss = 2.04 (798.1 examples/sec; 0.160 sec/batch)
2017-03-25 20:45:32.390549: step 1620, loss = 1.96 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 20:45:34.118554: step 1630, loss = 1.79 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:45:35.851333: step 1640, loss = 1.95 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:45:37.573682: step 1650, loss = 1.98 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 20:45:39.305515: step 1660, loss = 1.86 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:45:41.042901: step 1670, loss = 1.94 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:45:42.779970: step 1680, loss = 1.83 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:45:44.505990: step 1690, loss = 1.77 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:45:46.483471: step 1700, loss = 1.81 (647.5 examples/sec; 0.198 sec/batch)
2017-03-25 20:45:48.009746: step 1710, loss = 1.81 (838.3 examples/sec; 0.153 sec/batch)
2017-03-25 20:45:49.729787: step 1720, loss = 1.78 (744.2 examples/sec; 0.172 sec/batch)
2017-03-25 20:45:51.454451: step 1730, loss = 1.80 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 20:45:53.201681: step 1740, loss = 1.62 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 20:45:54.928588: step 1750, loss = 1.64 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:45:56.653517: step 1760, loss = 1.69 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 20:45:58.399024: step 1770, loss = 1.76 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 20:46:00.140462: step 1780, loss = 1.82 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:46:01.870799: step 1790, loss = 1.84 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:46:03.783267: step 1800, loss = 1.73 (669.3 examples/sec; 0.191 sec/batch)
2017-03-25 20:46:05.373821: step 1810, loss = 2.35 (804.7 examples/sec; 0.159 sec/batch)
2017-03-25 20:46:07.103037: step 1820, loss = 1.69 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:46:08.835923: step 1830, loss = 1.66 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:46:10.559574: step 1840, loss = 1.75 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 20:46:12.300884: step 1850, loss = 1.63 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:46:14.034918: step 1860, loss = 1.73 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:46:15.770220: step 1870, loss = 1.60 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:46:17.500040: step 1880, loss = 1.99 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:46:19.235496: step 1890, loss = 1.64 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:46:21.129775: step 1900, loss = 1.69 (675.7 examples/sec; 0.189 sec/batch)
2017-03-25 20:46:22.740708: step 1910, loss = 1.77 (794.6 examples/sec; 0.161 sec/batch)
2017-03-25 20:46:24.466818: step 1920, loss = 1.60 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:46:26.207333: step 1930, loss = 1.65 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:46:27.936804: step 1940, loss = 1.51 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:46:29.661244: step 1950, loss = 1.44 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 20:46:31.385826: step 1960, loss = 1.70 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 20:46:33.107528: step 1970, loss = 1.74 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 20:46:34.835811: step 1980, loss = 1.91 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:46:36.582901: step 1990, loss = 1.71 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 20:46:38.543981: step 2000, loss = 1.81 (653.1 examples/sec; 0.196 sec/batch)
2017-03-25 20:46:40.091216: step 2010, loss = 1.67 (826.7 examples/sec; 0.155 sec/batch)
2017-03-25 20:46:41.841511: step 2020, loss = 1.54 (731.4 examples/sec; 0.175 sec/batch)
2017-03-25 20:46:43.579209: step 2030, loss = 1.91 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:46:45.313872: step 2040, loss = 1.76 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:46:47.062464: step 2050, loss = 1.77 (732.0 examples/sec; 0.175 sec/batch)
2017-03-25 20:46:48.809144: step 2060, loss = 1.54 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 20:46:50.532630: step 2070, loss = 2.08 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 20:46:52.262360: step 2080, loss = 1.62 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:46:53.992718: step 2090, loss = 1.61 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:46:55.894851: step 2100, loss = 1.68 (672.9 examples/sec; 0.190 sec/batch)
2017-03-25 20:46:57.508404: step 2110, loss = 1.57 (793.3 examples/sec; 0.161 sec/batch)
2017-03-25 20:46:59.249181: step 2120, loss = 1.44 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:47:00.980622: step 2130, loss = 1.59 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:47:02.703828: step 2140, loss = 1.46 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 20:47:04.442515: step 2150, loss = 1.70 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:47:06.180443: step 2160, loss = 1.58 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:47:07.908575: step 2170, loss = 1.37 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:47:09.644160: step 2180, loss = 1.64 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:47:11.369153: step 2190, loss = 1.55 (742.0 examples/sec; 0.172 sec/batch)
2017-03-25 20:47:13.338780: step 2200, loss = 1.63 (650.0 examples/sec; 0.197 sec/batch)
2017-03-25 20:47:14.858628: step 2210, loss = 1.46 (841.9 examples/sec; 0.152 sec/batch)
2017-03-25 20:47:16.580305: step 2220, loss = 1.54 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 20:47:18.315531: step 2230, loss = 1.58 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:47:20.062839: step 2240, loss = 1.59 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 20:47:21.785547: step 2250, loss = 1.50 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 20:47:23.509471: step 2260, loss = 1.60 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 20:47:25.255436: step 2270, loss = 1.34 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 20:47:26.991005: step 2280, loss = 1.54 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:47:28.730741: step 2290, loss = 1.53 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:47:30.650573: step 2300, loss = 1.44 (667.2 examples/sec; 0.192 sec/batch)
2017-03-25 20:47:32.270637: step 2310, loss = 1.53 (789.4 examples/sec; 0.162 sec/batch)
2017-03-25 20:47:34.001492: step 2320, loss = 1.57 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:47:35.747660: step 2330, loss = 1.52 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 20:47:37.476291: step 2340, loss = 1.42 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:47:39.205554: step 2350, loss = 1.47 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:47:40.943487: step 2360, loss = 1.36 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:47:42.675513: step 2370, loss = 1.29 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:47:44.407042: step 2380, loss = 1.59 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:47:46.147371: step 2390, loss = 1.42 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:47:48.087617: step 2400, loss = 1.43 (659.8 examples/sec; 0.194 sec/batch)
2017-03-25 20:47:49.662889: step 2410, loss = 1.32 (812.4 examples/sec; 0.158 sec/batch)
2017-03-25 20:47:51.404569: step 2420, loss = 1.50 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:47:53.146782: step 2430, loss = 1.28 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:47:54.869914: step 2440, loss = 1.57 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 20:47:56.605272: step 2450, loss = 1.38 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:47:58.335040: step 2460, loss = 1.46 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:48:00.068368: step 2470, loss = 1.58 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:48:01.806764: step 2480, loss = 1.53 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:48:03.558225: step 2490, loss = 1.59 (730.8 examples/sec; 0.175 sec/batch)
2017-03-25 20:48:05.460059: step 2500, loss = 1.32 (673.3 examples/sec; 0.190 sec/batch)
2017-03-25 20:48:07.069978: step 2510, loss = 1.37 (794.8 examples/sec; 0.161 sec/batch)
2017-03-25 20:48:08.801354: step 2520, loss = 1.39 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:48:10.549131: step 2530, loss = 1.68 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 20:48:12.274556: step 2540, loss = 1.40 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:48:14.003257: step 2550, loss = 1.28 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 20:48:15.738417: step 2560, loss = 1.19 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:48:17.468824: step 2570, loss = 1.29 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:48:19.185370: step 2580, loss = 1.39 (745.8 examples/sec; 0.172 sec/batch)
2017-03-25 20:48:20.915936: step 2590, loss = 1.29 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:48:22.820703: step 2600, loss = 1.43 (671.9 examples/sec; 0.190 sec/batch)
2017-03-25 20:48:24.434261: step 2610, loss = 1.31 (793.2 examples/sec; 0.161 sec/batch)
2017-03-25 20:48:26.182915: step 2620, loss = 1.30 (732.1 examples/sec; 0.175 sec/batch)
2017-03-25 20:48:27.925924: step 2630, loss = 1.49 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:48:29.661568: step 2640, loss = 1.35 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:48:31.390436: step 2650, loss = 1.38 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:48:33.125815: step 2660, loss = 1.44 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:48:34.857947: step 2670, loss = 1.37 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:48:36.598866: step 2680, loss = 1.38 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:48:38.338753: step 2690, loss = 1.43 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:48:40.241989: step 2700, loss = 1.31 (672.8 examples/sec; 0.190 sec/batch)
2017-03-25 20:48:41.853629: step 2710, loss = 1.36 (793.9 examples/sec; 0.161 sec/batch)
2017-03-25 20:48:43.601235: step 2720, loss = 1.48 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 20:48:45.322487: step 2730, loss = 1.53 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 20:48:47.062818: step 2740, loss = 1.34 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:48:48.802639: step 2750, loss = 1.28 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:48:50.534734: step 2760, loss = 1.40 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:48:52.269837: step 2770, loss = 1.22 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:48:54.001743: step 2780, loss = 1.20 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:48:55.742763: step 2790, loss = 1.28 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:48:57.648070: step 2800, loss = 1.34 (671.8 examples/sec; 0.191 sec/batch)
2017-03-25 20:48:59.255148: step 2810, loss = 1.34 (796.5 examples/sec; 0.161 sec/batch)
2017-03-25 20:49:00.995439: step 2820, loss = 1.34 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:49:02.735714: step 2830, loss = 1.25 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:49:04.479048: step 2840, loss = 1.15 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 20:49:06.204832: step 2850, loss = 1.42 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:49:07.938890: step 2860, loss = 1.26 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:49:09.684335: step 2870, loss = 1.22 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 20:49:11.420339: step 2880, loss = 1.36 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:49:13.157069: step 2890, loss = 1.37 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:49:15.074046: step 2900, loss = 1.31 (668.0 examples/sec; 0.192 sec/batch)
2017-03-25 20:49:16.669450: step 2910, loss = 1.38 (801.9 examples/sec; 0.160 sec/batch)
2017-03-25 20:49:18.421370: step 2920, loss = 1.32 (730.6 examples/sec; 0.175 sec/batch)
2017-03-25 20:49:20.142113: step 2930, loss = 1.29 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 20:49:21.888393: step 2940, loss = 1.19 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 20:49:23.629978: step 2950, loss = 1.31 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:49:25.355814: step 2960, loss = 1.09 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:49:27.106075: step 2970, loss = 1.04 (731.3 examples/sec; 0.175 sec/batch)
2017-03-25 20:49:28.828853: step 2980, loss = 1.40 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 20:49:30.556431: step 2990, loss = 1.28 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:49:32.482188: step 3000, loss = 1.28 (665.0 examples/sec; 0.192 sec/batch)
2017-03-25 20:49:34.016193: step 3010, loss = 1.39 (833.9 examples/sec; 0.154 sec/batch)
2017-03-25 20:49:35.723674: step 3020, loss = 1.26 (749.6 examples/sec; 0.171 sec/batch)
2017-03-25 20:49:37.453919: step 3030, loss = 1.14 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:49:39.190484: step 3040, loss = 1.32 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:49:40.932806: step 3050, loss = 1.07 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:49:42.664259: step 3060, loss = 1.26 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:49:44.394879: step 3070, loss = 1.24 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:49:46.118567: step 3080, loss = 1.26 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 20:49:47.847435: step 3090, loss = 1.29 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 20:49:49.748706: step 3100, loss = 1.32 (673.7 examples/sec; 0.190 sec/batch)
2017-03-25 20:49:51.351974: step 3110, loss = 1.19 (797.7 examples/sec; 0.160 sec/batch)
2017-03-25 20:49:53.080002: step 3120, loss = 1.19 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:49:54.829758: step 3130, loss = 1.20 (731.5 examples/sec; 0.175 sec/batch)
2017-03-25 20:49:56.562897: step 3140, loss = 1.45 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:49:58.295425: step 3150, loss = 1.40 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:50:00.025184: step 3160, loss = 1.21 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:50:01.768475: step 3170, loss = 1.14 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:03.505272: step 3180, loss = 1.27 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:05.243891: step 3190, loss = 1.13 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:07.141831: step 3200, loss = 1.21 (674.4 examples/sec; 0.190 sec/batch)
2017-03-25 20:50:08.746651: step 3210, loss = 1.24 (797.6 examples/sec; 0.160 sec/batch)
2017-03-25 20:50:10.484061: step 3220, loss = 1.29 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:12.220185: step 3230, loss = 1.26 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:13.962281: step 3240, loss = 1.21 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:15.689967: step 3250, loss = 1.35 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:50:17.422628: step 3260, loss = 1.19 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:50:19.161805: step 3270, loss = 1.37 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:20.906001: step 3280, loss = 1.11 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:22.630365: step 3290, loss = 1.18 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 20:50:24.529659: step 3300, loss = 1.15 (673.9 examples/sec; 0.190 sec/batch)
2017-03-25 20:50:26.140927: step 3310, loss = 1.27 (794.4 examples/sec; 0.161 sec/batch)
2017-03-25 20:50:27.876620: step 3320, loss = 1.11 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:29.615822: step 3330, loss = 1.22 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:31.346334: step 3340, loss = 1.21 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:50:33.096689: step 3350, loss = 1.31 (731.3 examples/sec; 0.175 sec/batch)
2017-03-25 20:50:34.835993: step 3360, loss = 1.22 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:36.581393: step 3370, loss = 1.14 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 20:50:38.332046: step 3380, loss = 1.01 (731.2 examples/sec; 0.175 sec/batch)
2017-03-25 20:50:40.072529: step 3390, loss = 1.33 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:42.046589: step 3400, loss = 1.12 (648.4 examples/sec; 0.197 sec/batch)
2017-03-25 20:50:43.584393: step 3410, loss = 1.29 (832.4 examples/sec; 0.154 sec/batch)
2017-03-25 20:50:45.325874: step 3420, loss = 1.22 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:47.049437: step 3430, loss = 1.28 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 20:50:48.787100: step 3440, loss = 1.15 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:50.520943: step 3450, loss = 1.29 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 20:50:52.295874: step 3460, loss = 1.07 (721.0 examples/sec; 0.178 sec/batch)
2017-03-25 20:50:54.042726: step 3470, loss = 1.08 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 20:50:55.777780: step 3480, loss = 1.16 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:50:57.519255: step 3490, loss = 1.19 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:50:59.453346: step 3500, loss = 1.06 (661.7 examples/sec; 0.193 sec/batch)
2017-03-25 20:51:01.038864: step 3510, loss = 1.37 (807.3 examples/sec; 0.159 sec/batch)
2017-03-25 20:51:02.770045: step 3520, loss = 1.19 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:04.508492: step 3530, loss = 1.20 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:51:06.242395: step 3540, loss = 1.26 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:07.978324: step 3550, loss = 1.10 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:51:09.711083: step 3560, loss = 1.17 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:11.443049: step 3570, loss = 1.20 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:13.178660: step 3580, loss = 0.97 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:51:14.934153: step 3590, loss = 1.16 (729.1 examples/sec; 0.176 sec/batch)
2017-03-25 20:51:16.858809: step 3600, loss = 1.12 (665.0 examples/sec; 0.192 sec/batch)
2017-03-25 20:51:18.453302: step 3610, loss = 1.35 (802.8 examples/sec; 0.159 sec/batch)
2017-03-25 20:51:20.185812: step 3620, loss = 1.17 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:21.918313: step 3630, loss = 1.09 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:23.653299: step 3640, loss = 1.13 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:25.389899: step 3650, loss = 1.08 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 20:51:27.121980: step 3660, loss = 1.25 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:28.860499: step 3670, loss = 1.02 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:51:30.590572: step 3680, loss = 1.06 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:32.328880: step 3690, loss = 1.24 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:51:34.279798: step 3700, loss = 1.42 (656.2 examples/sec; 0.195 sec/batch)
2017-03-25 20:51:35.855415: step 3710, loss = 1.03 (812.0 examples/sec; 0.158 sec/batch)
2017-03-25 20:51:37.589298: step 3720, loss = 1.15 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:39.323447: step 3730, loss = 1.23 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:41.065060: step 3740, loss = 1.35 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:51:42.784800: step 3750, loss = 1.19 (744.3 examples/sec; 0.172 sec/batch)
2017-03-25 20:51:44.517211: step 3760, loss = 1.10 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:46.256114: step 3770, loss = 1.25 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 20:51:47.989102: step 3780, loss = 1.13 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:49.714872: step 3790, loss = 1.11 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:51.614611: step 3800, loss = 1.16 (674.0 examples/sec; 0.190 sec/batch)
2017-03-25 20:51:53.231887: step 3810, loss = 1.20 (791.2 examples/sec; 0.162 sec/batch)
2017-03-25 20:51:54.964433: step 3820, loss = 1.11 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:51:56.704092: step 3830, loss = 1.32 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 20:51:58.442965: step 3840, loss = 1.10 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:00.175268: step 3850, loss = 1.07 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:52:01.918276: step 3860, loss = 1.29 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:03.654030: step 3870, loss = 1.10 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:05.381129: step 3880, loss = 1.06 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:52:07.119859: step 3890, loss = 1.05 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:09.043841: step 3900, loss = 1.26 (665.5 examples/sec; 0.192 sec/batch)
2017-03-25 20:52:10.637826: step 3910, loss = 1.19 (802.7 examples/sec; 0.159 sec/batch)
2017-03-25 20:52:12.369773: step 3920, loss = 1.00 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:52:14.113689: step 3930, loss = 1.16 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:15.845848: step 3940, loss = 0.95 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:52:17.588486: step 3950, loss = 0.95 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:19.314504: step 3960, loss = 1.11 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:52:21.038374: step 3970, loss = 1.12 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 20:52:22.762650: step 3980, loss = 1.07 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 20:52:24.503082: step 3990, loss = 1.24 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:26.401338: step 4000, loss = 0.99 (674.3 examples/sec; 0.190 sec/batch)
2017-03-25 20:52:28.004347: step 4010, loss = 1.04 (798.5 examples/sec; 0.160 sec/batch)
2017-03-25 20:52:29.746709: step 4020, loss = 1.26 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:31.481746: step 4030, loss = 1.00 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:33.225552: step 4040, loss = 1.16 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:34.950156: step 4050, loss = 1.16 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 20:52:36.688644: step 4060, loss = 1.03 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:38.414112: step 4070, loss = 1.13 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:52:40.150143: step 4080, loss = 1.00 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:41.900592: step 4090, loss = 1.13 (731.2 examples/sec; 0.175 sec/batch)
2017-03-25 20:52:43.805109: step 4100, loss = 0.98 (672.1 examples/sec; 0.190 sec/batch)
2017-03-25 20:52:45.406271: step 4110, loss = 1.15 (799.7 examples/sec; 0.160 sec/batch)
2017-03-25 20:52:47.147571: step 4120, loss = 1.24 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:48.890202: step 4130, loss = 1.10 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:50.620471: step 4140, loss = 1.07 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:52:52.361419: step 4150, loss = 0.93 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:54.102476: step 4160, loss = 0.95 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:55.842876: step 4170, loss = 1.04 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:52:57.575185: step 4180, loss = 1.10 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:52:59.310564: step 4190, loss = 1.12 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:53:01.209402: step 4200, loss = 1.07 (674.1 examples/sec; 0.190 sec/batch)
2017-03-25 20:53:02.818525: step 4210, loss = 1.12 (795.5 examples/sec; 0.161 sec/batch)
2017-03-25 20:53:04.553173: step 4220, loss = 1.03 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:53:06.285935: step 4230, loss = 1.03 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:53:08.009838: step 4240, loss = 1.07 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 20:53:09.738949: step 4250, loss = 0.89 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:53:11.478881: step 4260, loss = 1.14 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:53:13.210629: step 4270, loss = 1.11 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:53:14.953037: step 4280, loss = 1.11 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:53:16.682983: step 4290, loss = 1.31 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:53:18.597206: step 4300, loss = 1.06 (668.7 examples/sec; 0.191 sec/batch)
2017-03-25 20:53:20.195067: step 4310, loss = 1.06 (801.1 examples/sec; 0.160 sec/batch)
2017-03-25 20:53:21.921334: step 4320, loss = 0.83 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:53:23.658821: step 4330, loss = 0.96 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:53:25.391265: step 4340, loss = 1.01 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:53:27.118729: step 4350, loss = 1.04 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:53:28.838355: step 4360, loss = 1.18 (744.3 examples/sec; 0.172 sec/batch)
2017-03-25 20:53:30.575960: step 4370, loss = 1.01 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:53:32.325127: step 4380, loss = 1.07 (731.7 examples/sec; 0.175 sec/batch)
2017-03-25 20:53:34.032876: step 4390, loss = 1.20 (749.5 examples/sec; 0.171 sec/batch)
2017-03-25 20:53:35.949040: step 4400, loss = 1.11 (668.0 examples/sec; 0.192 sec/batch)
2017-03-25 20:53:37.547938: step 4410, loss = 1.02 (800.6 examples/sec; 0.160 sec/batch)
2017-03-25 20:53:39.288577: step 4420, loss = 0.92 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:53:41.012763: step 4430, loss = 1.12 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 20:53:42.753183: step 4440, loss = 1.17 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:53:44.481286: step 4450, loss = 1.13 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:53:46.213644: step 4460, loss = 1.17 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:53:47.942749: step 4470, loss = 1.00 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:53:49.672905: step 4480, loss = 1.04 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:53:51.395274: step 4490, loss = 1.16 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 20:53:53.326617: step 4500, loss = 1.14 (662.7 examples/sec; 0.193 sec/batch)
2017-03-25 20:53:54.898898: step 4510, loss = 0.91 (814.1 examples/sec; 0.157 sec/batch)
2017-03-25 20:53:56.627798: step 4520, loss = 0.94 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 20:53:58.366216: step 4530, loss = 1.01 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:54:00.091789: step 4540, loss = 1.01 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:54:01.855891: step 4550, loss = 1.12 (725.6 examples/sec; 0.176 sec/batch)
2017-03-25 20:54:03.590469: step 4560, loss = 1.07 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:54:05.319016: step 4570, loss = 0.85 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:54:07.052416: step 4580, loss = 0.98 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 20:54:08.788460: step 4590, loss = 1.10 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:54:10.679808: step 4600, loss = 1.02 (676.7 examples/sec; 0.189 sec/batch)
2017-03-25 20:54:12.283796: step 4610, loss = 0.94 (798.0 examples/sec; 0.160 sec/batch)
2017-03-25 20:54:14.026810: step 4620, loss = 0.99 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:54:15.753120: step 4630, loss = 0.82 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:54:17.487887: step 4640, loss = 1.14 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:54:19.205284: step 4650, loss = 1.01 (745.3 examples/sec; 0.172 sec/batch)
2017-03-25 20:54:20.946540: step 4660, loss = 1.12 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 20:54:22.683494: step 4670, loss = 1.14 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:54:24.406760: step 4680, loss = 1.08 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 20:54:26.147170: step 4690, loss = 0.86 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:54:28.053986: step 4700, loss = 1.16 (671.5 examples/sec; 0.191 sec/batch)
2017-03-25 20:54:29.646661: step 4710, loss = 1.16 (803.3 examples/sec; 0.159 sec/batch)
2017-03-25 20:54:31.379173: step 4720, loss = 1.00 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:54:33.120617: step 4730, loss = 1.10 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:54:34.854622: step 4740, loss = 1.11 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:54:36.592459: step 4750, loss = 1.00 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:54:38.321156: step 4760, loss = 0.98 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 20:54:40.061062: step 4770, loss = 1.00 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:54:41.784749: step 4780, loss = 1.04 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 20:54:43.504827: step 4790, loss = 1.05 (744.2 examples/sec; 0.172 sec/batch)
2017-03-25 20:54:45.406496: step 4800, loss = 1.12 (673.1 examples/sec; 0.190 sec/batch)
2017-03-25 20:54:47.006637: step 4810, loss = 1.14 (799.9 examples/sec; 0.160 sec/batch)
2017-03-25 20:54:48.751845: step 4820, loss = 0.97 (733.4 examples/sec; 0.175 sec/batch)
2017-03-25 20:54:50.483668: step 4830, loss = 1.08 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:54:52.231851: step 4840, loss = 1.05 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 20:54:53.961576: step 4850, loss = 1.01 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:54:55.709804: step 4860, loss = 0.99 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 20:54:57.445591: step 4870, loss = 1.02 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:54:59.185853: step 4880, loss = 1.09 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:55:00.931555: step 4890, loss = 1.13 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 20:55:02.829573: step 4900, loss = 1.11 (674.7 examples/sec; 0.190 sec/batch)
2017-03-25 20:55:04.438307: step 4910, loss = 0.88 (795.2 examples/sec; 0.161 sec/batch)
2017-03-25 20:55:06.181326: step 4920, loss = 1.25 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:55:07.900296: step 4930, loss = 1.15 (744.6 examples/sec; 0.172 sec/batch)
2017-03-25 20:55:09.630426: step 4940, loss = 0.95 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:55:11.365179: step 4950, loss = 0.94 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:55:13.104361: step 4960, loss = 1.25 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:55:14.838336: step 4970, loss = 1.07 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:55:16.572944: step 4980, loss = 0.94 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:55:18.313919: step 4990, loss = 1.06 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:55:20.273354: step 5000, loss = 1.04 (653.5 examples/sec; 0.196 sec/batch)
2017-03-25 20:55:21.822501: step 5010, loss = 1.11 (825.9 examples/sec; 0.155 sec/batch)
2017-03-25 20:55:23.549410: step 5020, loss = 0.93 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:55:25.296520: step 5030, loss = 0.86 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 20:55:27.038605: step 5040, loss = 0.96 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 20:55:28.764215: step 5050, loss = 1.16 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:55:30.495150: step 5060, loss = 0.93 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:55:32.240798: step 5070, loss = 1.00 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 20:55:33.966107: step 5080, loss = 1.03 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:55:35.692914: step 5090, loss = 1.06 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:55:37.612874: step 5100, loss = 1.00 (667.0 examples/sec; 0.192 sec/batch)
2017-03-25 20:55:39.178679: step 5110, loss = 1.12 (817.0 examples/sec; 0.157 sec/batch)
2017-03-25 20:55:40.902943: step 5120, loss = 1.04 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 20:55:42.641809: step 5130, loss = 0.90 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 20:55:44.372599: step 5140, loss = 1.01 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:55:46.100695: step 5150, loss = 1.05 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:55:47.827291: step 5160, loss = 1.08 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:55:49.554987: step 5170, loss = 1.06 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:55:51.296714: step 5180, loss = 0.93 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:55:53.024286: step 5190, loss = 0.92 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:55:55.008492: step 5200, loss = 1.02 (645.1 examples/sec; 0.198 sec/batch)
2017-03-25 20:55:56.555393: step 5210, loss = 1.01 (827.4 examples/sec; 0.155 sec/batch)
2017-03-25 20:55:58.288293: step 5220, loss = 1.10 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:56:00.021437: step 5230, loss = 0.93 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:56:01.760835: step 5240, loss = 0.88 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:56:03.498992: step 5250, loss = 1.12 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:56:05.232712: step 5260, loss = 0.98 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:56:06.956237: step 5270, loss = 1.04 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 20:56:08.703450: step 5280, loss = 1.13 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 20:56:10.433669: step 5290, loss = 0.89 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:56:12.338456: step 5300, loss = 1.09 (672.0 examples/sec; 0.190 sec/batch)
2017-03-25 20:56:13.936873: step 5310, loss = 0.94 (800.8 examples/sec; 0.160 sec/batch)
2017-03-25 20:56:15.670354: step 5320, loss = 0.85 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 20:56:17.396318: step 5330, loss = 0.84 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:56:19.131753: step 5340, loss = 1.01 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:56:20.871476: step 5350, loss = 1.00 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 20:56:22.597003: step 5360, loss = 1.03 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:56:24.329415: step 5370, loss = 1.07 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:56:26.072762: step 5380, loss = 1.08 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:56:27.789236: step 5390, loss = 0.97 (745.7 examples/sec; 0.172 sec/batch)
2017-03-25 20:56:29.686886: step 5400, loss = 0.87 (674.9 examples/sec; 0.190 sec/batch)
2017-03-25 20:56:31.305088: step 5410, loss = 0.92 (790.5 examples/sec; 0.162 sec/batch)
2017-03-25 20:56:33.043470: step 5420, loss = 0.90 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:56:34.765096: step 5430, loss = 0.95 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 20:56:36.511628: step 5440, loss = 0.81 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 20:56:38.238389: step 5450, loss = 0.83 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:56:39.970275: step 5460, loss = 1.09 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:56:41.710783: step 5470, loss = 0.95 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:56:43.451258: step 5480, loss = 0.89 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:56:45.184191: step 5490, loss = 0.97 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:56:47.088409: step 5500, loss = 0.77 (672.4 examples/sec; 0.190 sec/batch)
2017-03-25 20:56:48.706223: step 5510, loss = 0.78 (790.9 examples/sec; 0.162 sec/batch)
2017-03-25 20:56:50.440809: step 5520, loss = 1.14 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:56:52.179486: step 5530, loss = 0.92 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:56:53.915475: step 5540, loss = 0.99 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:56:55.645138: step 5550, loss = 0.93 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:56:57.379939: step 5560, loss = 1.04 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:56:59.106902: step 5570, loss = 0.98 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:57:00.846230: step 5580, loss = 1.08 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:57:02.582402: step 5590, loss = 1.07 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:57:04.502053: step 5600, loss = 0.80 (666.8 examples/sec; 0.192 sec/batch)
2017-03-25 20:57:06.093847: step 5610, loss = 1.17 (804.1 examples/sec; 0.159 sec/batch)
2017-03-25 20:57:07.820703: step 5620, loss = 0.94 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:57:09.567362: step 5630, loss = 0.93 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 20:57:11.315299: step 5640, loss = 1.04 (732.3 examples/sec; 0.175 sec/batch)
2017-03-25 20:57:13.053230: step 5650, loss = 1.08 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 20:57:14.786906: step 5660, loss = 1.06 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:57:16.525370: step 5670, loss = 1.02 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 20:57:18.260065: step 5680, loss = 1.00 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:57:20.009783: step 5690, loss = 1.10 (731.5 examples/sec; 0.175 sec/batch)
2017-03-25 20:57:21.900298: step 5700, loss = 0.84 (677.3 examples/sec; 0.189 sec/batch)
2017-03-25 20:57:23.502082: step 5710, loss = 1.00 (798.7 examples/sec; 0.160 sec/batch)
2017-03-25 20:57:25.225246: step 5720, loss = 0.98 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 20:57:26.962064: step 5730, loss = 1.02 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:57:28.712983: step 5740, loss = 0.96 (731.1 examples/sec; 0.175 sec/batch)
2017-03-25 20:57:30.439893: step 5750, loss = 0.89 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:57:32.175605: step 5760, loss = 1.00 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:57:33.905883: step 5770, loss = 0.85 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:57:35.644671: step 5780, loss = 1.01 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:57:37.375647: step 5790, loss = 1.01 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:57:39.299684: step 5800, loss = 0.85 (665.2 examples/sec; 0.192 sec/batch)
2017-03-25 20:57:40.889823: step 5810, loss = 0.87 (805.0 examples/sec; 0.159 sec/batch)
2017-03-25 20:57:42.636224: step 5820, loss = 1.09 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 20:57:44.371099: step 5830, loss = 0.87 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:57:46.098176: step 5840, loss = 0.95 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 20:57:47.830564: step 5850, loss = 0.93 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:57:49.578291: step 5860, loss = 1.19 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 20:57:51.309913: step 5870, loss = 0.89 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:57:53.036810: step 5880, loss = 0.85 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:57:54.770345: step 5890, loss = 0.81 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 20:57:56.684896: step 5900, loss = 0.92 (669.2 examples/sec; 0.191 sec/batch)
2017-03-25 20:57:58.285328: step 5910, loss = 0.81 (798.8 examples/sec; 0.160 sec/batch)
2017-03-25 20:58:00.022329: step 5920, loss = 1.05 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:58:01.767517: step 5930, loss = 0.84 (733.4 examples/sec; 0.175 sec/batch)
2017-03-25 20:58:03.515342: step 5940, loss = 0.94 (732.3 examples/sec; 0.175 sec/batch)
2017-03-25 20:58:05.252504: step 5950, loss = 0.99 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:58:06.985334: step 5960, loss = 0.85 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 20:58:08.719241: step 5970, loss = 0.90 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:58:10.451558: step 5980, loss = 0.79 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:58:12.188330: step 5990, loss = 0.80 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 20:58:14.101109: step 6000, loss = 0.95 (669.2 examples/sec; 0.191 sec/batch)
2017-03-25 20:58:15.674717: step 6010, loss = 0.89 (813.4 examples/sec; 0.157 sec/batch)
2017-03-25 20:58:17.407438: step 6020, loss = 1.11 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:58:19.144715: step 6030, loss = 0.98 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:58:20.862023: step 6040, loss = 1.25 (745.2 examples/sec; 0.172 sec/batch)
2017-03-25 20:58:22.607248: step 6050, loss = 0.97 (733.4 examples/sec; 0.175 sec/batch)
2017-03-25 20:58:24.330781: step 6060, loss = 0.85 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 20:58:26.065853: step 6070, loss = 0.91 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:58:27.791308: step 6080, loss = 1.01 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:58:29.526495: step 6090, loss = 1.03 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:58:31.420104: step 6100, loss = 0.87 (676.2 examples/sec; 0.189 sec/batch)
2017-03-25 20:58:33.048614: step 6110, loss = 0.94 (785.6 examples/sec; 0.163 sec/batch)
2017-03-25 20:58:34.799043: step 6120, loss = 0.99 (731.2 examples/sec; 0.175 sec/batch)
2017-03-25 20:58:36.533636: step 6130, loss = 1.01 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 20:58:38.256876: step 6140, loss = 0.97 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 20:58:39.995490: step 6150, loss = 1.02 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 20:58:41.726589: step 6160, loss = 1.00 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 20:58:43.463746: step 6170, loss = 0.91 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 20:58:45.199048: step 6180, loss = 1.01 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:58:46.937503: step 6190, loss = 1.02 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:58:48.811683: step 6200, loss = 1.16 (683.0 examples/sec; 0.187 sec/batch)
2017-03-25 20:58:50.443468: step 6210, loss = 0.92 (784.4 examples/sec; 0.163 sec/batch)
2017-03-25 20:58:52.176722: step 6220, loss = 1.06 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:58:53.917221: step 6230, loss = 1.08 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:58:55.645270: step 6240, loss = 0.88 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:58:57.384759: step 6250, loss = 0.92 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 20:58:59.110196: step 6260, loss = 0.99 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:59:00.847341: step 6270, loss = 1.00 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 20:59:02.573199: step 6280, loss = 0.81 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:59:04.308032: step 6290, loss = 0.97 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 20:59:06.210514: step 6300, loss = 0.98 (672.8 examples/sec; 0.190 sec/batch)
2017-03-25 20:59:07.846058: step 6310, loss = 0.87 (782.6 examples/sec; 0.164 sec/batch)
2017-03-25 20:59:09.582776: step 6320, loss = 0.86 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 20:59:11.310672: step 6330, loss = 0.90 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:59:13.048379: step 6340, loss = 0.87 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:59:14.789431: step 6350, loss = 0.86 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:59:16.522797: step 6360, loss = 0.93 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 20:59:18.265315: step 6370, loss = 0.83 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:59:19.988956: step 6380, loss = 0.98 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 20:59:21.712859: step 6390, loss = 0.96 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 20:59:23.642477: step 6400, loss = 0.99 (663.3 examples/sec; 0.193 sec/batch)
2017-03-25 20:59:25.218976: step 6410, loss = 0.83 (811.9 examples/sec; 0.158 sec/batch)
2017-03-25 20:59:26.953419: step 6420, loss = 0.88 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 20:59:28.693535: step 6430, loss = 1.14 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 20:59:30.434081: step 6440, loss = 1.00 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 20:59:32.158158: step 6450, loss = 0.93 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 20:59:33.887448: step 6460, loss = 0.93 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 20:59:35.617884: step 6470, loss = 0.88 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 20:59:37.336003: step 6480, loss = 0.86 (745.0 examples/sec; 0.172 sec/batch)
2017-03-25 20:59:39.079115: step 6490, loss = 1.12 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 20:59:40.990098: step 6500, loss = 1.07 (669.8 examples/sec; 0.191 sec/batch)
2017-03-25 20:59:42.584972: step 6510, loss = 0.92 (802.6 examples/sec; 0.159 sec/batch)
2017-03-25 20:59:44.326854: step 6520, loss = 1.03 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 20:59:46.051531: step 6530, loss = 0.98 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 20:59:47.776054: step 6540, loss = 0.99 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 20:59:49.493793: step 6550, loss = 0.92 (745.2 examples/sec; 0.172 sec/batch)
2017-03-25 20:59:51.240558: step 6560, loss = 1.15 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 20:59:52.977857: step 6570, loss = 1.14 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 20:59:54.699348: step 6580, loss = 1.10 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 20:59:56.430122: step 6590, loss = 0.90 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 20:59:58.324581: step 6600, loss = 1.08 (676.0 examples/sec; 0.189 sec/batch)
2017-03-25 20:59:59.951335: step 6610, loss = 1.01 (786.4 examples/sec; 0.163 sec/batch)
2017-03-25 21:00:01.690074: step 6620, loss = 1.17 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:00:03.436811: step 6630, loss = 0.96 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 21:00:05.162406: step 6640, loss = 0.93 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:00:06.902788: step 6650, loss = 0.81 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:00:08.640960: step 6660, loss = 1.21 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:00:10.368968: step 6670, loss = 1.08 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:00:12.102994: step 6680, loss = 0.99 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:00:13.838970: step 6690, loss = 0.99 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:00:15.752934: step 6700, loss = 1.04 (668.8 examples/sec; 0.191 sec/batch)
2017-03-25 21:00:17.341893: step 6710, loss = 0.96 (805.6 examples/sec; 0.159 sec/batch)
2017-03-25 21:00:19.082413: step 6720, loss = 0.83 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:00:20.815669: step 6730, loss = 0.90 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:00:22.559235: step 6740, loss = 0.89 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:00:24.286004: step 6750, loss = 0.82 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:00:26.024918: step 6760, loss = 1.11 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:00:27.752829: step 6770, loss = 1.04 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:00:29.485961: step 6780, loss = 0.86 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:00:31.224307: step 6790, loss = 0.91 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:00:33.143750: step 6800, loss = 1.08 (667.1 examples/sec; 0.192 sec/batch)
2017-03-25 21:00:34.759432: step 6810, loss = 0.86 (791.9 examples/sec; 0.162 sec/batch)
2017-03-25 21:00:36.493050: step 6820, loss = 0.84 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:00:38.222286: step 6830, loss = 0.95 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:00:39.952192: step 6840, loss = 1.10 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:00:41.693475: step 6850, loss = 0.86 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:00:43.410746: step 6860, loss = 1.04 (745.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:00:45.139084: step 6870, loss = 0.78 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:00:46.876839: step 6880, loss = 0.92 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:00:48.614512: step 6890, loss = 0.99 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:00:50.584604: step 6900, loss = 0.78 (649.7 examples/sec; 0.197 sec/batch)
2017-03-25 21:00:52.279154: step 6910, loss = 0.96 (755.4 examples/sec; 0.169 sec/batch)
2017-03-25 21:00:53.881200: step 6920, loss = 1.19 (798.9 examples/sec; 0.160 sec/batch)
2017-03-25 21:00:55.635331: step 6930, loss = 0.94 (729.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:00:57.368404: step 6940, loss = 0.88 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:00:59.107765: step 6950, loss = 1.09 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:01:00.851769: step 6960, loss = 1.00 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:01:02.588342: step 6970, loss = 0.81 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:01:04.339368: step 6980, loss = 0.85 (731.0 examples/sec; 0.175 sec/batch)
2017-03-25 21:01:06.079203: step 6990, loss = 0.87 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:01:07.985866: step 7000, loss = 0.98 (671.3 examples/sec; 0.191 sec/batch)
2017-03-25 21:01:09.587474: step 7010, loss = 1.07 (799.2 examples/sec; 0.160 sec/batch)
2017-03-25 21:01:11.345957: step 7020, loss = 1.15 (727.9 examples/sec; 0.176 sec/batch)
2017-03-25 21:01:13.070201: step 7030, loss = 0.97 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:01:14.813319: step 7040, loss = 0.80 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:01:16.547120: step 7050, loss = 1.13 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:01:18.289004: step 7060, loss = 0.98 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:01:20.016246: step 7070, loss = 1.00 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:01:21.750857: step 7080, loss = 1.04 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:01:23.492987: step 7090, loss = 1.00 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:01:25.401996: step 7100, loss = 0.92 (670.4 examples/sec; 0.191 sec/batch)
2017-03-25 21:01:26.984061: step 7110, loss = 0.83 (809.1 examples/sec; 0.158 sec/batch)
2017-03-25 21:01:28.727503: step 7120, loss = 0.92 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:01:30.451797: step 7130, loss = 0.89 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:01:32.187324: step 7140, loss = 0.86 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:01:33.912986: step 7150, loss = 0.84 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:01:35.652528: step 7160, loss = 0.91 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:01:37.384476: step 7170, loss = 0.84 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:01:39.117963: step 7180, loss = 0.95 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:01:40.849726: step 7190, loss = 1.04 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:01:42.739644: step 7200, loss = 0.93 (677.6 examples/sec; 0.189 sec/batch)
2017-03-25 21:01:44.344979: step 7210, loss = 1.13 (796.9 examples/sec; 0.161 sec/batch)
2017-03-25 21:01:46.071701: step 7220, loss = 0.88 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:01:47.818735: step 7230, loss = 0.93 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:01:49.546351: step 7240, loss = 0.79 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:01:51.306833: step 7250, loss = 0.97 (727.1 examples/sec; 0.176 sec/batch)
2017-03-25 21:01:53.039296: step 7260, loss = 0.80 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:01:54.776524: step 7270, loss = 1.05 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:01:56.511706: step 7280, loss = 0.81 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:01:58.242877: step 7290, loss = 0.98 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:02:00.138168: step 7300, loss = 0.87 (675.4 examples/sec; 0.190 sec/batch)
2017-03-25 21:02:01.756336: step 7310, loss = 0.91 (791.0 examples/sec; 0.162 sec/batch)
2017-03-25 21:02:03.494618: step 7320, loss = 0.88 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:02:05.232201: step 7330, loss = 0.84 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:02:06.961788: step 7340, loss = 1.02 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:02:08.716692: step 7350, loss = 0.85 (729.4 examples/sec; 0.175 sec/batch)
2017-03-25 21:02:10.446409: step 7360, loss = 0.93 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:02:12.174780: step 7370, loss = 1.06 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:02:13.923387: step 7380, loss = 0.92 (732.0 examples/sec; 0.175 sec/batch)
2017-03-25 21:02:15.649261: step 7390, loss = 0.97 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:02:17.605815: step 7400, loss = 0.89 (654.2 examples/sec; 0.196 sec/batch)
2017-03-25 21:02:19.114610: step 7410, loss = 0.96 (848.4 examples/sec; 0.151 sec/batch)
2017-03-25 21:02:20.810661: step 7420, loss = 0.87 (754.7 examples/sec; 0.170 sec/batch)
2017-03-25 21:02:22.554246: step 7430, loss = 0.93 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:02:24.298642: step 7440, loss = 0.89 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:02:26.026445: step 7450, loss = 0.84 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:02:27.755218: step 7460, loss = 1.02 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:02:29.475784: step 7470, loss = 0.85 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 21:02:31.216834: step 7480, loss = 0.96 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:02:32.949756: step 7490, loss = 0.86 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:02:34.849301: step 7500, loss = 0.98 (674.1 examples/sec; 0.190 sec/batch)
2017-03-25 21:02:36.412169: step 7510, loss = 0.87 (818.8 examples/sec; 0.156 sec/batch)
2017-03-25 21:02:38.139928: step 7520, loss = 0.74 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:02:39.867735: step 7530, loss = 1.13 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:02:41.598684: step 7540, loss = 0.87 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:02:43.311005: step 7550, loss = 1.06 (747.5 examples/sec; 0.171 sec/batch)
2017-03-25 21:02:45.033832: step 7560, loss = 1.04 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:02:46.750645: step 7570, loss = 0.85 (745.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:02:48.475452: step 7580, loss = 0.91 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:02:50.210473: step 7590, loss = 0.99 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:02:52.102380: step 7600, loss = 0.99 (676.6 examples/sec; 0.189 sec/batch)
2017-03-25 21:02:53.700357: step 7610, loss = 0.80 (801.0 examples/sec; 0.160 sec/batch)
2017-03-25 21:02:55.412546: step 7620, loss = 1.00 (747.7 examples/sec; 0.171 sec/batch)
2017-03-25 21:02:57.135683: step 7630, loss = 0.90 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:02:58.856762: step 7640, loss = 0.92 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:03:00.575437: step 7650, loss = 0.91 (744.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:03:02.311398: step 7660, loss = 0.89 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:03:04.033952: step 7670, loss = 0.86 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:03:05.757099: step 7680, loss = 0.97 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:03:07.486385: step 7690, loss = 1.00 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:03:09.380152: step 7700, loss = 0.95 (675.9 examples/sec; 0.189 sec/batch)
2017-03-25 21:03:11.009830: step 7710, loss = 0.94 (785.4 examples/sec; 0.163 sec/batch)
2017-03-25 21:03:12.762060: step 7720, loss = 0.81 (730.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:03:14.484192: step 7730, loss = 0.92 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:03:16.228612: step 7740, loss = 1.02 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:03:17.957091: step 7750, loss = 0.97 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:03:19.681869: step 7760, loss = 0.83 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:03:21.417460: step 7770, loss = 0.88 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:03:23.168235: step 7780, loss = 0.98 (731.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:03:24.901905: step 7790, loss = 0.89 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:03:26.801528: step 7800, loss = 0.95 (674.2 examples/sec; 0.190 sec/batch)
2017-03-25 21:03:28.404378: step 7810, loss = 1.03 (798.0 examples/sec; 0.160 sec/batch)
2017-03-25 21:03:30.144724: step 7820, loss = 0.80 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:03:31.877604: step 7830, loss = 1.08 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:03:33.606753: step 7840, loss = 0.83 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:03:35.339147: step 7850, loss = 0.97 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:03:37.070458: step 7860, loss = 0.96 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:03:38.810457: step 7870, loss = 0.93 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:03:40.544355: step 7880, loss = 0.98 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:03:42.268371: step 7890, loss = 0.98 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:03:44.168593: step 7900, loss = 0.94 (674.0 examples/sec; 0.190 sec/batch)
2017-03-25 21:03:45.794479: step 7910, loss = 1.06 (786.7 examples/sec; 0.163 sec/batch)
2017-03-25 21:03:47.519517: step 7920, loss = 0.88 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:03:49.257194: step 7930, loss = 0.97 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:03:50.989378: step 7940, loss = 0.81 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:03:52.726480: step 7950, loss = 0.99 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:03:54.465929: step 7960, loss = 0.82 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:03:56.194485: step 7970, loss = 0.92 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:03:57.923137: step 7980, loss = 0.92 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:03:59.661455: step 7990, loss = 0.93 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:04:01.552035: step 8000, loss = 0.72 (677.3 examples/sec; 0.189 sec/batch)
2017-03-25 21:04:03.185281: step 8010, loss = 0.89 (783.3 examples/sec; 0.163 sec/batch)
2017-03-25 21:04:04.914308: step 8020, loss = 0.79 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:06.645656: step 8030, loss = 0.88 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:08.377024: step 8040, loss = 0.78 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:10.110354: step 8050, loss = 0.84 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:11.840683: step 8060, loss = 0.84 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:13.575936: step 8070, loss = 0.85 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:04:15.313797: step 8080, loss = 0.86 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:04:17.045862: step 8090, loss = 1.16 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:18.950970: step 8100, loss = 0.82 (671.9 examples/sec; 0.191 sec/batch)
2017-03-25 21:04:20.554498: step 8110, loss = 0.92 (798.2 examples/sec; 0.160 sec/batch)
2017-03-25 21:04:22.302316: step 8120, loss = 0.71 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 21:04:24.041989: step 8130, loss = 0.67 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:04:25.770600: step 8140, loss = 0.80 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:27.512968: step 8150, loss = 0.87 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:04:29.238676: step 8160, loss = 1.04 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:30.977562: step 8170, loss = 0.84 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:04:32.710510: step 8180, loss = 0.86 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:34.460893: step 8190, loss = 0.89 (731.3 examples/sec; 0.175 sec/batch)
2017-03-25 21:04:36.351823: step 8200, loss = 0.94 (676.9 examples/sec; 0.189 sec/batch)
2017-03-25 21:04:37.967611: step 8210, loss = 1.10 (792.2 examples/sec; 0.162 sec/batch)
2017-03-25 21:04:39.714325: step 8220, loss = 0.87 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 21:04:41.441925: step 8230, loss = 0.97 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:43.194230: step 8240, loss = 0.96 (730.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:04:44.927216: step 8250, loss = 0.73 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:46.652788: step 8260, loss = 0.87 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:48.387349: step 8270, loss = 0.91 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:50.115771: step 8280, loss = 0.77 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:51.853621: step 8290, loss = 0.94 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:04:53.741260: step 8300, loss = 1.09 (678.3 examples/sec; 0.189 sec/batch)
2017-03-25 21:04:55.349308: step 8310, loss = 0.88 (795.7 examples/sec; 0.161 sec/batch)
2017-03-25 21:04:57.080129: step 8320, loss = 0.94 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:04:58.814952: step 8330, loss = 0.78 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:05:00.557210: step 8340, loss = 0.97 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:05:02.306463: step 8350, loss = 1.04 (731.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:05:04.036385: step 8360, loss = 0.89 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:05:05.770019: step 8370, loss = 0.93 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:05:07.503165: step 8380, loss = 0.86 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:05:09.240232: step 8390, loss = 0.91 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:05:11.132370: step 8400, loss = 0.91 (676.5 examples/sec; 0.189 sec/batch)
2017-03-25 21:05:12.765906: step 8410, loss = 0.94 (783.6 examples/sec; 0.163 sec/batch)
2017-03-25 21:05:14.503274: step 8420, loss = 0.87 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:05:16.228689: step 8430, loss = 1.01 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:05:17.970900: step 8440, loss = 0.85 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:05:19.700539: step 8450, loss = 0.87 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:05:21.430828: step 8460, loss = 0.88 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:05:23.175270: step 8470, loss = 0.95 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:05:24.903177: step 8480, loss = 0.91 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:05:26.628058: step 8490, loss = 1.07 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:05:28.534386: step 8500, loss = 0.91 (671.5 examples/sec; 0.191 sec/batch)
2017-03-25 21:05:30.141560: step 8510, loss = 0.77 (796.6 examples/sec; 0.161 sec/batch)
2017-03-25 21:05:31.876285: step 8520, loss = 1.04 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:05:33.612823: step 8530, loss = 0.73 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:05:35.343821: step 8540, loss = 0.77 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:05:37.080474: step 8550, loss = 0.83 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:05:38.817827: step 8560, loss = 1.13 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:05:40.541042: step 8570, loss = 0.66 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:05:42.277271: step 8580, loss = 0.87 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:05:44.008964: step 8590, loss = 1.00 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:05:45.918038: step 8600, loss = 0.76 (671.2 examples/sec; 0.191 sec/batch)
2017-03-25 21:05:47.519679: step 8610, loss = 1.08 (798.2 examples/sec; 0.160 sec/batch)
2017-03-25 21:05:49.249981: step 8620, loss = 0.85 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:05:50.996772: step 8630, loss = 0.78 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 21:05:52.730411: step 8640, loss = 0.76 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:05:54.468750: step 8650, loss = 0.96 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:05:56.208161: step 8660, loss = 0.96 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:05:57.933823: step 8670, loss = 0.90 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:05:59.672373: step 8680, loss = 0.85 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:06:01.417030: step 8690, loss = 0.83 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:06:03.376565: step 8700, loss = 0.93 (653.2 examples/sec; 0.196 sec/batch)
2017-03-25 21:06:04.900168: step 8710, loss = 0.80 (840.1 examples/sec; 0.152 sec/batch)
2017-03-25 21:06:06.605823: step 8720, loss = 0.75 (750.4 examples/sec; 0.171 sec/batch)
2017-03-25 21:06:08.327458: step 8730, loss = 0.71 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:06:10.059443: step 8740, loss = 1.04 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:06:11.796446: step 8750, loss = 0.78 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:06:13.539055: step 8760, loss = 0.98 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:06:15.270861: step 8770, loss = 0.81 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:06:17.003667: step 8780, loss = 1.00 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:06:18.740995: step 8790, loss = 0.91 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:06:20.633281: step 8800, loss = 0.78 (676.4 examples/sec; 0.189 sec/batch)
2017-03-25 21:06:22.246320: step 8810, loss = 0.83 (793.5 examples/sec; 0.161 sec/batch)
2017-03-25 21:06:23.972850: step 8820, loss = 0.91 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:06:25.697007: step 8830, loss = 0.90 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:06:27.430956: step 8840, loss = 0.99 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:06:29.162717: step 8850, loss = 1.10 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:06:30.892252: step 8860, loss = 0.87 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:06:32.639313: step 8870, loss = 0.91 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:06:34.383101: step 8880, loss = 0.80 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:06:36.106156: step 8890, loss = 0.96 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 21:06:38.014799: step 8900, loss = 0.81 (670.6 examples/sec; 0.191 sec/batch)
2017-03-25 21:06:39.626612: step 8910, loss = 0.80 (794.1 examples/sec; 0.161 sec/batch)
2017-03-25 21:06:41.357585: step 8920, loss = 0.89 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:06:43.088210: step 8930, loss = 0.86 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:06:44.828415: step 8940, loss = 1.03 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:06:46.568167: step 8950, loss = 0.87 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:06:48.288352: step 8960, loss = 0.97 (744.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:06:50.016462: step 8970, loss = 0.80 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:06:51.751118: step 8980, loss = 1.00 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:06:53.490756: step 8990, loss = 0.88 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:06:55.379169: step 9000, loss = 1.05 (678.1 examples/sec; 0.189 sec/batch)
2017-03-25 21:06:56.987760: step 9010, loss = 0.84 (795.5 examples/sec; 0.161 sec/batch)
2017-03-25 21:06:58.718702: step 9020, loss = 0.94 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:00.456933: step 9030, loss = 0.84 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:07:02.187313: step 9040, loss = 0.88 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:03.923748: step 9050, loss = 0.72 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:07:05.671097: step 9060, loss = 0.83 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:07:07.402615: step 9070, loss = 0.84 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:09.133348: step 9080, loss = 0.89 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:10.860093: step 9090, loss = 0.78 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:12.748064: step 9100, loss = 0.90 (678.0 examples/sec; 0.189 sec/batch)
2017-03-25 21:07:14.350597: step 9110, loss = 0.87 (798.7 examples/sec; 0.160 sec/batch)
2017-03-25 21:07:16.080245: step 9120, loss = 0.87 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:17.807065: step 9130, loss = 0.87 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:19.532397: step 9140, loss = 0.80 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:21.266705: step 9150, loss = 0.86 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:22.997893: step 9160, loss = 0.91 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:24.733059: step 9170, loss = 0.68 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:07:26.462564: step 9180, loss = 0.79 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:28.191552: step 9190, loss = 0.96 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:30.080531: step 9200, loss = 0.91 (677.9 examples/sec; 0.189 sec/batch)
2017-03-25 21:07:31.688330: step 9210, loss = 0.96 (795.7 examples/sec; 0.161 sec/batch)
2017-03-25 21:07:33.428857: step 9220, loss = 1.01 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:07:35.162385: step 9230, loss = 0.94 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:36.890922: step 9240, loss = 0.96 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:38.633485: step 9250, loss = 0.99 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:07:40.363085: step 9260, loss = 0.84 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:42.098564: step 9270, loss = 0.96 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:07:43.834930: step 9280, loss = 0.88 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:07:45.572657: step 9290, loss = 0.87 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:07:47.476783: step 9300, loss = 1.06 (672.2 examples/sec; 0.190 sec/batch)
2017-03-25 21:07:49.085934: step 9310, loss = 0.97 (795.4 examples/sec; 0.161 sec/batch)
2017-03-25 21:07:50.827813: step 9320, loss = 0.92 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:07:52.564122: step 9330, loss = 0.95 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:07:54.291731: step 9340, loss = 0.96 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:56.023972: step 9350, loss = 0.97 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:07:57.767818: step 9360, loss = 0.88 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:07:59.489717: step 9370, loss = 0.79 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:08:01.229568: step 9380, loss = 0.82 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:02.958882: step 9390, loss = 0.96 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:08:04.914458: step 9400, loss = 1.07 (654.5 examples/sec; 0.196 sec/batch)
2017-03-25 21:08:06.469107: step 9410, loss = 0.93 (823.3 examples/sec; 0.155 sec/batch)
2017-03-25 21:08:08.206285: step 9420, loss = 0.86 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:09.941631: step 9430, loss = 0.84 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:11.669789: step 9440, loss = 0.79 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:08:13.417947: step 9450, loss = 1.00 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 21:08:15.142065: step 9460, loss = 0.95 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:08:16.888037: step 9470, loss = 1.04 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:08:18.630135: step 9480, loss = 0.80 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:20.368113: step 9490, loss = 0.85 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:22.287927: step 9500, loss = 1.01 (667.0 examples/sec; 0.192 sec/batch)
2017-03-25 21:08:23.882549: step 9510, loss = 0.66 (802.3 examples/sec; 0.160 sec/batch)
2017-03-25 21:08:25.627080: step 9520, loss = 0.77 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:27.346542: step 9530, loss = 1.14 (744.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:08:29.084182: step 9540, loss = 1.03 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:30.820573: step 9550, loss = 0.96 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:32.553033: step 9560, loss = 0.77 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:08:34.286731: step 9570, loss = 0.93 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:08:36.023414: step 9580, loss = 0.91 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:37.763331: step 9590, loss = 0.83 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:39.666691: step 9600, loss = 0.89 (672.4 examples/sec; 0.190 sec/batch)
2017-03-25 21:08:41.254206: step 9610, loss = 0.87 (806.5 examples/sec; 0.159 sec/batch)
2017-03-25 21:08:42.990157: step 9620, loss = 1.03 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:44.720606: step 9630, loss = 0.87 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:08:46.456745: step 9640, loss = 0.77 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:48.191583: step 9650, loss = 0.93 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:49.919810: step 9660, loss = 1.01 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:08:51.655048: step 9670, loss = 0.88 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:53.392632: step 9680, loss = 0.93 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:08:55.120491: step 9690, loss = 0.66 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:08:57.113832: step 9700, loss = 0.99 (642.6 examples/sec; 0.199 sec/batch)
2017-03-25 21:08:58.600298: step 9710, loss = 0.99 (860.3 examples/sec; 0.149 sec/batch)
2017-03-25 21:09:00.305193: step 9720, loss = 0.68 (750.8 examples/sec; 0.170 sec/batch)
2017-03-25 21:09:02.042711: step 9730, loss = 0.94 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:09:03.780363: step 9740, loss = 0.92 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:09:05.526438: step 9750, loss = 0.92 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:09:07.262966: step 9760, loss = 0.97 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:09:08.988702: step 9770, loss = 0.93 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:10.728012: step 9780, loss = 0.80 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:09:12.457779: step 9790, loss = 0.82 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:14.352875: step 9800, loss = 0.80 (675.4 examples/sec; 0.190 sec/batch)
2017-03-25 21:09:15.981291: step 9810, loss = 0.89 (786.0 examples/sec; 0.163 sec/batch)
2017-03-25 21:09:17.713518: step 9820, loss = 0.97 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:19.444372: step 9830, loss = 0.89 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:21.177537: step 9840, loss = 0.95 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:22.909518: step 9850, loss = 0.74 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:24.637017: step 9860, loss = 0.80 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:26.363171: step 9870, loss = 0.65 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:28.108245: step 9880, loss = 1.10 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:09:29.827747: step 9890, loss = 0.93 (744.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:09:31.736336: step 9900, loss = 0.90 (670.9 examples/sec; 0.191 sec/batch)
2017-03-25 21:09:33.330413: step 9910, loss = 0.79 (802.6 examples/sec; 0.159 sec/batch)
2017-03-25 21:09:35.061340: step 9920, loss = 0.87 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:36.792244: step 9930, loss = 0.87 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:38.532326: step 9940, loss = 0.93 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:09:40.262101: step 9950, loss = 0.91 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:41.993432: step 9960, loss = 0.74 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:43.728723: step 9970, loss = 0.91 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:09:45.457220: step 9980, loss = 0.95 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:47.202404: step 9990, loss = 0.90 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:09:49.092576: step 10000, loss = 0.88 (677.2 examples/sec; 0.189 sec/batch)
2017-03-25 21:09:50.706593: step 10010, loss = 1.02 (793.1 examples/sec; 0.161 sec/batch)
2017-03-25 21:09:52.436337: step 10020, loss = 0.77 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:54.185304: step 10030, loss = 0.81 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 21:09:55.910295: step 10040, loss = 0.93 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:09:57.644552: step 10050, loss = 0.84 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:09:59.390301: step 10060, loss = 0.75 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 21:10:01.132537: step 10070, loss = 0.74 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:02.867761: step 10080, loss = 0.85 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:04.604049: step 10090, loss = 0.81 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:06.502381: step 10100, loss = 0.77 (674.3 examples/sec; 0.190 sec/batch)
2017-03-25 21:10:08.111959: step 10110, loss = 0.93 (795.2 examples/sec; 0.161 sec/batch)
2017-03-25 21:10:09.833109: step 10120, loss = 0.95 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:10:11.568226: step 10130, loss = 0.92 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:13.303299: step 10140, loss = 1.03 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:15.050084: step 10150, loss = 0.86 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 21:10:16.792857: step 10160, loss = 1.01 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:18.527078: step 10170, loss = 1.04 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:10:20.258017: step 10180, loss = 0.83 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:10:21.995186: step 10190, loss = 0.72 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:23.901308: step 10200, loss = 0.81 (671.5 examples/sec; 0.191 sec/batch)
2017-03-25 21:10:25.527301: step 10210, loss = 0.88 (787.2 examples/sec; 0.163 sec/batch)
2017-03-25 21:10:27.268639: step 10220, loss = 0.96 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:28.993909: step 10230, loss = 0.97 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:10:30.732874: step 10240, loss = 0.95 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:32.474744: step 10250, loss = 0.90 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:34.205373: step 10260, loss = 1.01 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:10:35.949279: step 10270, loss = 0.71 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:37.674987: step 10280, loss = 0.87 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:10:39.403265: step 10290, loss = 0.78 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:10:41.281254: step 10300, loss = 0.81 (681.9 examples/sec; 0.188 sec/batch)
2017-03-25 21:10:42.905225: step 10310, loss = 0.89 (787.8 examples/sec; 0.162 sec/batch)
2017-03-25 21:10:44.643612: step 10320, loss = 0.80 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:46.401938: step 10330, loss = 0.85 (728.0 examples/sec; 0.176 sec/batch)
2017-03-25 21:10:48.149761: step 10340, loss = 0.85 (732.3 examples/sec; 0.175 sec/batch)
2017-03-25 21:10:49.889430: step 10350, loss = 0.90 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:51.637135: step 10360, loss = 0.75 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 21:10:53.380925: step 10370, loss = 0.90 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:55.129299: step 10380, loss = 1.14 (732.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:10:56.869967: step 10390, loss = 0.87 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:10:58.780306: step 10400, loss = 0.88 (670.0 examples/sec; 0.191 sec/batch)
2017-03-25 21:11:00.395201: step 10410, loss = 0.88 (792.6 examples/sec; 0.161 sec/batch)
2017-03-25 21:11:02.135535: step 10420, loss = 0.94 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:03.875952: step 10430, loss = 0.91 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:05.608835: step 10440, loss = 0.81 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:11:07.347492: step 10450, loss = 0.94 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:09.097948: step 10460, loss = 0.84 (731.2 examples/sec; 0.175 sec/batch)
2017-03-25 21:11:10.832270: step 10470, loss = 0.79 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:11:12.576468: step 10480, loss = 0.78 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:14.321132: step 10490, loss = 0.98 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:16.240034: step 10500, loss = 0.83 (667.6 examples/sec; 0.192 sec/batch)
2017-03-25 21:11:17.821718: step 10510, loss = 0.69 (808.5 examples/sec; 0.158 sec/batch)
2017-03-25 21:11:19.556930: step 10520, loss = 0.89 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:21.301141: step 10530, loss = 0.86 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:23.044929: step 10540, loss = 0.95 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:24.785236: step 10550, loss = 0.80 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:26.521939: step 10560, loss = 0.85 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:28.270939: step 10570, loss = 0.92 (731.8 examples/sec; 0.175 sec/batch)
2017-03-25 21:11:30.003166: step 10580, loss = 1.00 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:11:31.728518: step 10590, loss = 0.91 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:11:33.636606: step 10600, loss = 1.02 (670.8 examples/sec; 0.191 sec/batch)
2017-03-25 21:11:35.228819: step 10610, loss = 0.84 (803.9 examples/sec; 0.159 sec/batch)
2017-03-25 21:11:36.971768: step 10620, loss = 0.92 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:38.713807: step 10630, loss = 0.77 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:40.465196: step 10640, loss = 0.98 (730.9 examples/sec; 0.175 sec/batch)
2017-03-25 21:11:42.197457: step 10650, loss = 0.75 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:11:43.942180: step 10660, loss = 0.92 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:45.671108: step 10670, loss = 1.08 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:11:47.421798: step 10680, loss = 0.86 (731.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:11:49.142927: step 10690, loss = 0.73 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:11:51.046538: step 10700, loss = 0.94 (672.7 examples/sec; 0.190 sec/batch)
2017-03-25 21:11:52.683377: step 10710, loss = 0.71 (781.5 examples/sec; 0.164 sec/batch)
2017-03-25 21:11:54.419747: step 10720, loss = 0.84 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:56.149173: step 10730, loss = 0.85 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:11:57.884198: step 10740, loss = 0.90 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:11:59.612539: step 10750, loss = 0.81 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:01.345819: step 10760, loss = 0.92 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:03.084329: step 10770, loss = 0.90 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:12:04.806768: step 10780, loss = 0.77 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:12:06.537292: step 10790, loss = 1.03 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:08.445855: step 10800, loss = 0.92 (670.7 examples/sec; 0.191 sec/batch)
2017-03-25 21:12:10.042568: step 10810, loss = 0.87 (801.6 examples/sec; 0.160 sec/batch)
2017-03-25 21:12:11.777493: step 10820, loss = 0.88 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:13.519950: step 10830, loss = 0.93 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:12:15.252466: step 10840, loss = 0.81 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:16.981415: step 10850, loss = 0.93 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:18.723132: step 10860, loss = 0.85 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:12:20.456619: step 10870, loss = 0.80 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:22.175565: step 10880, loss = 0.78 (744.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:12:23.905055: step 10890, loss = 0.73 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:25.821611: step 10900, loss = 0.86 (667.9 examples/sec; 0.192 sec/batch)
2017-03-25 21:12:27.413071: step 10910, loss = 0.93 (804.3 examples/sec; 0.159 sec/batch)
2017-03-25 21:12:29.142683: step 10920, loss = 0.83 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:30.875146: step 10930, loss = 0.94 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:32.615681: step 10940, loss = 0.92 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:12:34.359980: step 10950, loss = 0.63 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:12:36.088825: step 10960, loss = 0.81 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:37.819595: step 10970, loss = 1.02 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:39.554751: step 10980, loss = 0.84 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:12:41.286715: step 10990, loss = 0.88 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:43.174894: step 11000, loss = 0.92 (678.1 examples/sec; 0.189 sec/batch)
2017-03-25 21:12:44.784570: step 11010, loss = 1.02 (794.8 examples/sec; 0.161 sec/batch)
2017-03-25 21:12:46.518404: step 11020, loss = 0.98 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:48.255044: step 11030, loss = 0.88 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:12:49.989814: step 11040, loss = 0.94 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:51.722923: step 11050, loss = 0.90 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:53.452727: step 11060, loss = 0.89 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:55.178119: step 11070, loss = 0.82 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:12:56.913462: step 11080, loss = 0.93 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:12:58.640836: step 11090, loss = 0.85 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:13:00.533502: step 11100, loss = 0.77 (676.7 examples/sec; 0.189 sec/batch)
2017-03-25 21:13:02.148734: step 11110, loss = 0.93 (791.9 examples/sec; 0.162 sec/batch)
2017-03-25 21:13:03.885639: step 11120, loss = 0.85 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:13:05.626813: step 11130, loss = 0.87 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:13:07.364882: step 11140, loss = 0.97 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:13:09.109158: step 11150, loss = 0.94 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:13:10.837092: step 11160, loss = 0.77 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:13:12.573705: step 11170, loss = 0.75 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:13:14.295059: step 11180, loss = 1.05 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:13:16.037112: step 11190, loss = 0.90 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:13:18.011059: step 11200, loss = 1.00 (648.7 examples/sec; 0.197 sec/batch)
2017-03-25 21:13:19.531298: step 11210, loss = 0.78 (841.5 examples/sec; 0.152 sec/batch)
2017-03-25 21:13:21.261095: step 11220, loss = 0.98 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:13:22.982130: step 11230, loss = 0.84 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:13:24.721833: step 11240, loss = 0.74 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:13:26.450543: step 11250, loss = 0.97 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:13:28.184610: step 11260, loss = 0.99 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:13:29.921435: step 11270, loss = 0.83 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:13:31.651490: step 11280, loss = 0.70 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:13:33.390486: step 11290, loss = 0.87 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:13:35.276226: step 11300, loss = 0.70 (678.8 examples/sec; 0.189 sec/batch)
2017-03-25 21:13:36.890505: step 11310, loss = 1.03 (792.9 examples/sec; 0.161 sec/batch)
2017-03-25 21:13:38.642370: step 11320, loss = 0.89 (730.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:13:40.366368: step 11330, loss = 0.87 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:13:42.101836: step 11340, loss = 0.85 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:13:43.829858: step 11350, loss = 1.14 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:13:45.570185: step 11360, loss = 0.79 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:13:47.311382: step 11370, loss = 0.84 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:13:49.055902: step 11380, loss = 0.79 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:13:50.792538: step 11390, loss = 1.05 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:13:52.722948: step 11400, loss = 0.77 (663.1 examples/sec; 0.193 sec/batch)
2017-03-25 21:13:54.273916: step 11410, loss = 0.90 (825.3 examples/sec; 0.155 sec/batch)
2017-03-25 21:13:56.000689: step 11420, loss = 0.78 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:13:57.735383: step 11430, loss = 0.92 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:13:59.468867: step 11440, loss = 0.74 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:14:01.213915: step 11450, loss = 0.86 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:14:02.956608: step 11460, loss = 0.91 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:14:04.688019: step 11470, loss = 0.82 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:14:06.433689: step 11480, loss = 0.94 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 21:14:08.158171: step 11490, loss = 0.85 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:14:10.055733: step 11500, loss = 1.09 (674.6 examples/sec; 0.190 sec/batch)
2017-03-25 21:14:11.660915: step 11510, loss = 0.78 (797.4 examples/sec; 0.161 sec/batch)
2017-03-25 21:14:13.398392: step 11520, loss = 0.97 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:14:15.133436: step 11530, loss = 0.85 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:14:16.875103: step 11540, loss = 0.84 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:14:18.605338: step 11550, loss = 0.84 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:14:20.328498: step 11560, loss = 1.22 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 21:14:22.061912: step 11570, loss = 1.11 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:14:23.795253: step 11580, loss = 0.90 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:14:25.531312: step 11590, loss = 0.90 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:14:27.443551: step 11600, loss = 1.07 (669.4 examples/sec; 0.191 sec/batch)
2017-03-25 21:14:29.036231: step 11610, loss = 0.83 (803.7 examples/sec; 0.159 sec/batch)
2017-03-25 21:14:30.773649: step 11620, loss = 0.98 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:14:32.510798: step 11630, loss = 0.89 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:14:34.246501: step 11640, loss = 0.83 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:14:35.980851: step 11650, loss = 0.74 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:14:37.705924: step 11660, loss = 0.77 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:14:39.443932: step 11670, loss = 0.84 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:14:41.184221: step 11680, loss = 0.89 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:14:42.913819: step 11690, loss = 0.90 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:14:44.811925: step 11700, loss = 1.09 (674.4 examples/sec; 0.190 sec/batch)
2017-03-25 21:14:46.416516: step 11710, loss = 0.91 (797.7 examples/sec; 0.160 sec/batch)
2017-03-25 21:14:48.141263: step 11720, loss = 0.88 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:14:49.881490: step 11730, loss = 0.79 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:14:51.614990: step 11740, loss = 0.79 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:14:53.366943: step 11750, loss = 0.91 (730.6 examples/sec; 0.175 sec/batch)
2017-03-25 21:14:55.088332: step 11760, loss = 0.76 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:14:56.824332: step 11770, loss = 0.80 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:14:58.573152: step 11780, loss = 0.82 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 21:15:00.297779: step 11790, loss = 0.99 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:15:02.213498: step 11800, loss = 0.75 (668.3 examples/sec; 0.192 sec/batch)
2017-03-25 21:15:03.834137: step 11810, loss = 0.91 (789.4 examples/sec; 0.162 sec/batch)
2017-03-25 21:15:05.568936: step 11820, loss = 0.84 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:15:07.315023: step 11830, loss = 0.86 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:15:09.033635: step 11840, loss = 0.82 (744.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:15:10.766637: step 11850, loss = 1.09 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:15:12.508011: step 11860, loss = 1.03 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:15:14.247341: step 11870, loss = 0.85 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:15:15.982567: step 11880, loss = 0.95 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:15:17.705357: step 11890, loss = 0.71 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:15:19.679629: step 11900, loss = 0.88 (649.0 examples/sec; 0.197 sec/batch)
2017-03-25 21:15:21.166411: step 11910, loss = 0.78 (859.7 examples/sec; 0.149 sec/batch)
2017-03-25 21:15:22.861043: step 11920, loss = 0.76 (755.5 examples/sec; 0.169 sec/batch)
2017-03-25 21:15:24.587718: step 11930, loss = 0.93 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:15:26.320318: step 11940, loss = 0.90 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:15:28.061535: step 11950, loss = 0.73 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:15:29.804394: step 11960, loss = 0.72 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:15:31.540725: step 11970, loss = 0.76 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:15:33.264997: step 11980, loss = 0.95 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:15:35.002967: step 11990, loss = 0.88 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:15:36.918467: step 12000, loss = 0.88 (668.2 examples/sec; 0.192 sec/batch)
2017-03-25 21:15:38.544902: step 12010, loss = 0.92 (787.3 examples/sec; 0.163 sec/batch)
2017-03-25 21:15:40.287796: step 12020, loss = 0.92 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:15:42.043012: step 12030, loss = 0.86 (729.3 examples/sec; 0.176 sec/batch)
2017-03-25 21:15:43.779618: step 12040, loss = 0.81 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:15:45.522794: step 12050, loss = 0.77 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:15:47.269753: step 12060, loss = 0.82 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:15:49.015233: step 12070, loss = 0.85 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:15:50.757379: step 12080, loss = 1.02 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:15:52.498598: step 12090, loss = 0.86 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:15:54.386835: step 12100, loss = 0.70 (678.2 examples/sec; 0.189 sec/batch)
2017-03-25 21:15:56.004893: step 12110, loss = 0.85 (790.7 examples/sec; 0.162 sec/batch)
2017-03-25 21:15:57.751215: step 12120, loss = 0.62 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 21:15:59.483208: step 12130, loss = 0.93 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:16:01.232000: step 12140, loss = 0.68 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 21:16:02.983077: step 12150, loss = 0.80 (731.0 examples/sec; 0.175 sec/batch)
2017-03-25 21:16:04.713072: step 12160, loss = 0.99 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:16:06.469145: step 12170, loss = 0.70 (728.9 examples/sec; 0.176 sec/batch)
2017-03-25 21:16:08.207774: step 12180, loss = 0.91 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:16:09.952880: step 12190, loss = 0.80 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:16:11.922514: step 12200, loss = 0.79 (650.1 examples/sec; 0.197 sec/batch)
2017-03-25 21:16:13.439322: step 12210, loss = 0.84 (843.6 examples/sec; 0.152 sec/batch)
2017-03-25 21:16:15.146344: step 12220, loss = 0.91 (749.8 examples/sec; 0.171 sec/batch)
2017-03-25 21:16:16.896833: step 12230, loss = 0.78 (731.2 examples/sec; 0.175 sec/batch)
2017-03-25 21:16:18.628235: step 12240, loss = 0.90 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:16:20.364016: step 12250, loss = 0.94 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:16:22.103409: step 12260, loss = 0.94 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:16:23.825607: step 12270, loss = 0.83 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:16:25.559346: step 12280, loss = 0.86 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:16:27.301812: step 12290, loss = 1.05 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:16:29.213792: step 12300, loss = 0.88 (669.7 examples/sec; 0.191 sec/batch)
2017-03-25 21:16:30.828290: step 12310, loss = 0.86 (792.5 examples/sec; 0.162 sec/batch)
2017-03-25 21:16:32.556038: step 12320, loss = 0.82 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:16:34.297425: step 12330, loss = 0.86 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:16:36.036759: step 12340, loss = 0.94 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:16:37.770003: step 12350, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:16:39.499835: step 12360, loss = 0.91 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:16:41.237738: step 12370, loss = 0.98 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:16:42.968187: step 12380, loss = 0.96 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:16:44.692775: step 12390, loss = 0.83 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:16:46.592661: step 12400, loss = 0.92 (673.7 examples/sec; 0.190 sec/batch)
2017-03-25 21:16:48.200387: step 12410, loss = 0.94 (796.1 examples/sec; 0.161 sec/batch)
2017-03-25 21:16:49.931439: step 12420, loss = 0.78 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:16:51.673807: step 12430, loss = 0.75 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:16:53.402897: step 12440, loss = 0.84 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:16:55.140540: step 12450, loss = 0.90 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:16:56.876661: step 12460, loss = 0.75 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:16:58.602872: step 12470, loss = 0.92 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:17:00.340332: step 12480, loss = 0.79 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:17:02.066786: step 12490, loss = 0.87 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:17:03.974782: step 12500, loss = 0.92 (670.9 examples/sec; 0.191 sec/batch)
2017-03-25 21:17:05.589667: step 12510, loss = 0.86 (792.6 examples/sec; 0.161 sec/batch)
2017-03-25 21:17:07.321331: step 12520, loss = 0.75 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:17:09.037105: step 12530, loss = 0.66 (746.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:17:10.764533: step 12540, loss = 0.86 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:17:12.496016: step 12550, loss = 0.80 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:17:14.228654: step 12560, loss = 1.03 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:17:15.946317: step 12570, loss = 0.81 (745.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:17:17.683595: step 12580, loss = 0.82 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:17:19.423041: step 12590, loss = 0.77 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:17:21.328676: step 12600, loss = 0.88 (671.7 examples/sec; 0.191 sec/batch)
2017-03-25 21:17:22.930966: step 12610, loss = 0.86 (798.9 examples/sec; 0.160 sec/batch)
2017-03-25 21:17:24.660885: step 12620, loss = 0.83 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:17:26.392906: step 12630, loss = 0.99 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:17:28.133014: step 12640, loss = 0.80 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:17:29.860181: step 12650, loss = 0.96 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:17:31.599253: step 12660, loss = 0.96 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:17:33.332080: step 12670, loss = 0.78 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:17:35.058100: step 12680, loss = 0.86 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:17:36.789396: step 12690, loss = 0.78 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:17:38.683363: step 12700, loss = 1.05 (675.8 examples/sec; 0.189 sec/batch)
2017-03-25 21:17:40.294418: step 12710, loss = 0.83 (794.5 examples/sec; 0.161 sec/batch)
2017-03-25 21:17:42.023365: step 12720, loss = 0.96 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:17:43.774096: step 12730, loss = 0.84 (731.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:17:45.495215: step 12740, loss = 0.85 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:17:47.219992: step 12750, loss = 0.99 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:17:48.946548: step 12760, loss = 0.91 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:17:50.682771: step 12770, loss = 0.86 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:17:52.399378: step 12780, loss = 0.80 (745.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:17:54.135027: step 12790, loss = 0.87 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:17:56.037139: step 12800, loss = 0.78 (673.3 examples/sec; 0.190 sec/batch)
2017-03-25 21:17:57.653135: step 12810, loss = 0.84 (791.6 examples/sec; 0.162 sec/batch)
2017-03-25 21:17:59.391876: step 12820, loss = 0.84 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:18:01.133502: step 12830, loss = 1.05 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:18:02.869153: step 12840, loss = 0.82 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:18:04.607721: step 12850, loss = 0.84 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:18:06.333404: step 12860, loss = 0.77 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:18:08.063099: step 12870, loss = 0.78 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:18:09.797007: step 12880, loss = 0.99 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:18:11.520279: step 12890, loss = 0.76 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:18:13.412020: step 12900, loss = 0.96 (676.6 examples/sec; 0.189 sec/batch)
2017-03-25 21:18:15.022924: step 12910, loss = 0.74 (794.6 examples/sec; 0.161 sec/batch)
2017-03-25 21:18:16.768634: step 12920, loss = 0.79 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 21:18:18.484741: step 12930, loss = 0.96 (745.9 examples/sec; 0.172 sec/batch)
2017-03-25 21:18:20.224575: step 12940, loss = 0.88 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:18:21.963755: step 12950, loss = 0.98 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:18:23.693317: step 12960, loss = 0.78 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:18:25.417444: step 12970, loss = 0.89 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:18:27.159176: step 12980, loss = 0.81 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:18:28.904234: step 12990, loss = 0.83 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:18:30.804508: step 13000, loss = 0.95 (673.9 examples/sec; 0.190 sec/batch)
2017-03-25 21:18:32.398334: step 13010, loss = 0.93 (802.7 examples/sec; 0.159 sec/batch)
2017-03-25 21:18:34.129177: step 13020, loss = 0.93 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:18:35.869799: step 13030, loss = 0.94 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:18:37.603589: step 13040, loss = 0.92 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:18:39.337950: step 13050, loss = 0.80 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:18:41.063312: step 13060, loss = 0.80 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:18:42.792977: step 13070, loss = 0.84 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:18:44.520147: step 13080, loss = 0.74 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:18:46.252020: step 13090, loss = 0.83 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:18:48.210931: step 13100, loss = 0.96 (653.5 examples/sec; 0.196 sec/batch)
2017-03-25 21:18:49.713656: step 13110, loss = 0.84 (851.7 examples/sec; 0.150 sec/batch)
2017-03-25 21:18:51.431601: step 13120, loss = 0.86 (745.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:18:53.164162: step 13130, loss = 0.91 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:18:54.898318: step 13140, loss = 0.92 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:18:56.656792: step 13150, loss = 0.70 (727.9 examples/sec; 0.176 sec/batch)
2017-03-25 21:18:58.384669: step 13160, loss = 0.90 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:19:00.125455: step 13170, loss = 0.95 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:19:01.857417: step 13180, loss = 0.82 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:19:03.595144: step 13190, loss = 0.91 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:19:05.486204: step 13200, loss = 0.82 (676.9 examples/sec; 0.189 sec/batch)
2017-03-25 21:19:07.130077: step 13210, loss = 0.91 (778.6 examples/sec; 0.164 sec/batch)
2017-03-25 21:19:08.858122: step 13220, loss = 0.95 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:19:10.577228: step 13230, loss = 0.91 (744.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:19:12.304808: step 13240, loss = 0.86 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:19:14.040588: step 13250, loss = 0.77 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:19:15.772434: step 13260, loss = 0.76 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:19:17.506286: step 13270, loss = 0.83 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:19:19.234034: step 13280, loss = 0.86 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:19:20.956438: step 13290, loss = 0.91 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:19:22.863761: step 13300, loss = 0.83 (671.2 examples/sec; 0.191 sec/batch)
2017-03-25 21:19:24.453022: step 13310, loss = 0.78 (805.3 examples/sec; 0.159 sec/batch)
2017-03-25 21:19:26.171011: step 13320, loss = 1.08 (745.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:19:27.908029: step 13330, loss = 0.96 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:19:29.644378: step 13340, loss = 0.84 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:19:31.379516: step 13350, loss = 0.88 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:19:33.109028: step 13360, loss = 0.94 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:19:34.832705: step 13370, loss = 0.74 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:19:36.574626: step 13380, loss = 0.82 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:19:38.300679: step 13390, loss = 0.78 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:19:40.259551: step 13400, loss = 1.07 (653.4 examples/sec; 0.196 sec/batch)
2017-03-25 21:19:41.776440: step 13410, loss = 0.72 (843.8 examples/sec; 0.152 sec/batch)
2017-03-25 21:19:43.476535: step 13420, loss = 0.76 (752.9 examples/sec; 0.170 sec/batch)
2017-03-25 21:19:45.210603: step 13430, loss = 0.96 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:19:46.946993: step 13440, loss = 0.83 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:19:48.680012: step 13450, loss = 0.80 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:19:50.415808: step 13460, loss = 0.62 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:19:52.147660: step 13470, loss = 0.76 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:19:53.892559: step 13480, loss = 0.97 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:19:55.634692: step 13490, loss = 0.84 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:19:57.527800: step 13500, loss = 0.82 (676.1 examples/sec; 0.189 sec/batch)
2017-03-25 21:19:59.148981: step 13510, loss = 0.97 (789.5 examples/sec; 0.162 sec/batch)
2017-03-25 21:20:00.887578: step 13520, loss = 0.72 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:20:02.613732: step 13530, loss = 0.83 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:20:04.351285: step 13540, loss = 0.82 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:20:06.073530: step 13550, loss = 0.87 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:20:07.808826: step 13560, loss = 0.81 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:20:09.554773: step 13570, loss = 1.02 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:20:11.294128: step 13580, loss = 0.95 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:20:13.028453: step 13590, loss = 0.87 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:20:14.914666: step 13600, loss = 0.89 (678.6 examples/sec; 0.189 sec/batch)
2017-03-25 21:20:16.536645: step 13610, loss = 0.87 (789.3 examples/sec; 0.162 sec/batch)
2017-03-25 21:20:18.287149: step 13620, loss = 0.89 (731.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:20:20.008638: step 13630, loss = 1.15 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:20:21.743257: step 13640, loss = 0.82 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:20:23.482911: step 13650, loss = 0.86 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:20:25.207659: step 13660, loss = 0.78 (742.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:20:26.936874: step 13670, loss = 0.86 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:20:28.666644: step 13680, loss = 0.97 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:20:30.400778: step 13690, loss = 0.93 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:20:32.305261: step 13700, loss = 0.67 (672.1 examples/sec; 0.190 sec/batch)
2017-03-25 21:20:33.893147: step 13710, loss = 0.75 (806.1 examples/sec; 0.159 sec/batch)
2017-03-25 21:20:35.634278: step 13720, loss = 0.82 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:20:37.372301: step 13730, loss = 0.72 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:20:39.096113: step 13740, loss = 0.80 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:20:40.823972: step 13750, loss = 0.94 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:20:42.548030: step 13760, loss = 0.66 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:20:44.281587: step 13770, loss = 0.88 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:20:46.007443: step 13780, loss = 0.93 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:20:47.743517: step 13790, loss = 0.79 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:20:49.682104: step 13800, loss = 1.04 (660.5 examples/sec; 0.194 sec/batch)
2017-03-25 21:20:51.236849: step 13810, loss = 0.72 (823.0 examples/sec; 0.156 sec/batch)
2017-03-25 21:20:53.000838: step 13820, loss = 0.88 (725.5 examples/sec; 0.176 sec/batch)
2017-03-25 21:20:54.742168: step 13830, loss = 0.86 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:20:56.457657: step 13840, loss = 1.12 (746.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:20:58.188924: step 13850, loss = 0.89 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:20:59.913628: step 13860, loss = 0.92 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:21:01.658130: step 13870, loss = 0.91 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:21:03.397283: step 13880, loss = 0.86 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:21:05.121213: step 13890, loss = 0.72 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:21:07.009779: step 13900, loss = 0.96 (678.3 examples/sec; 0.189 sec/batch)
2017-03-25 21:21:08.625043: step 13910, loss = 0.96 (791.7 examples/sec; 0.162 sec/batch)
2017-03-25 21:21:10.366613: step 13920, loss = 0.88 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:21:12.088082: step 13930, loss = 0.76 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:21:13.807518: step 13940, loss = 0.85 (744.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:21:15.553928: step 13950, loss = 0.79 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 21:21:17.299657: step 13960, loss = 0.72 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 21:21:19.032727: step 13970, loss = 0.75 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:21:20.763765: step 13980, loss = 0.78 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:21:22.506657: step 13990, loss = 0.92 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:21:24.410366: step 14000, loss = 0.90 (672.5 examples/sec; 0.190 sec/batch)
2017-03-25 21:21:26.023253: step 14010, loss = 1.04 (793.5 examples/sec; 0.161 sec/batch)
2017-03-25 21:21:27.752100: step 14020, loss = 0.75 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:21:29.484116: step 14030, loss = 0.90 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:21:31.218722: step 14040, loss = 0.76 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:21:32.945210: step 14050, loss = 0.90 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:21:34.680271: step 14060, loss = 0.78 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:21:36.416141: step 14070, loss = 0.90 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:21:38.151587: step 14080, loss = 0.79 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:21:39.878602: step 14090, loss = 1.12 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:21:41.778622: step 14100, loss = 0.83 (673.9 examples/sec; 0.190 sec/batch)
2017-03-25 21:21:43.399267: step 14110, loss = 0.68 (789.5 examples/sec; 0.162 sec/batch)
2017-03-25 21:21:45.142791: step 14120, loss = 0.95 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:21:46.874329: step 14130, loss = 0.89 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:21:48.607829: step 14140, loss = 0.71 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:21:50.345585: step 14150, loss = 0.89 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:21:52.090280: step 14160, loss = 0.91 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:21:53.812362: step 14170, loss = 0.88 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:21:55.545083: step 14180, loss = 0.94 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:21:57.277950: step 14190, loss = 0.84 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:21:59.163354: step 14200, loss = 0.84 (678.9 examples/sec; 0.189 sec/batch)
2017-03-25 21:22:00.796007: step 14210, loss = 0.72 (784.0 examples/sec; 0.163 sec/batch)
2017-03-25 21:22:02.538962: step 14220, loss = 0.84 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:04.278931: step 14230, loss = 0.85 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:06.019020: step 14240, loss = 0.74 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:07.758988: step 14250, loss = 0.94 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:09.497512: step 14260, loss = 0.77 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:11.220543: step 14270, loss = 0.82 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 21:22:12.956534: step 14280, loss = 1.04 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:14.687571: step 14290, loss = 0.70 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:22:16.662814: step 14300, loss = 0.77 (648.3 examples/sec; 0.197 sec/batch)
2017-03-25 21:22:18.205531: step 14310, loss = 0.82 (829.2 examples/sec; 0.154 sec/batch)
2017-03-25 21:22:19.953517: step 14320, loss = 0.86 (732.3 examples/sec; 0.175 sec/batch)
2017-03-25 21:22:21.675741: step 14330, loss = 0.78 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:22:23.415587: step 14340, loss = 0.94 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:25.143668: step 14350, loss = 0.73 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:22:26.887385: step 14360, loss = 0.99 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:28.628291: step 14370, loss = 0.84 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:30.365403: step 14380, loss = 0.82 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:32.106749: step 14390, loss = 0.91 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:34.002699: step 14400, loss = 1.00 (675.4 examples/sec; 0.190 sec/batch)
2017-03-25 21:22:35.622812: step 14410, loss = 1.05 (789.7 examples/sec; 0.162 sec/batch)
2017-03-25 21:22:37.360936: step 14420, loss = 0.99 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:39.096686: step 14430, loss = 0.73 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:40.842599: step 14440, loss = 0.74 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:22:42.571285: step 14450, loss = 0.93 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:22:44.296809: step 14460, loss = 0.84 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:22:46.034438: step 14470, loss = 0.83 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:47.757607: step 14480, loss = 0.84 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:22:49.504349: step 14490, loss = 0.93 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 21:22:51.398666: step 14500, loss = 0.99 (675.9 examples/sec; 0.189 sec/batch)
2017-03-25 21:22:53.024690: step 14510, loss = 0.82 (786.9 examples/sec; 0.163 sec/batch)
2017-03-25 21:22:54.760995: step 14520, loss = 0.92 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:22:56.487441: step 14530, loss = 0.93 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:22:58.221411: step 14540, loss = 0.95 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:22:59.952171: step 14550, loss = 0.93 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:23:01.689495: step 14560, loss = 0.87 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:23:03.428252: step 14570, loss = 0.80 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:23:05.167074: step 14580, loss = 0.73 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:23:06.901886: step 14590, loss = 0.79 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:23:08.868595: step 14600, loss = 0.92 (650.8 examples/sec; 0.197 sec/batch)
2017-03-25 21:23:10.426577: step 14610, loss = 0.86 (821.7 examples/sec; 0.156 sec/batch)
2017-03-25 21:23:12.165899: step 14620, loss = 0.95 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:23:13.886511: step 14630, loss = 0.94 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 21:23:15.612759: step 14640, loss = 0.91 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:23:17.353959: step 14650, loss = 0.85 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:23:19.095478: step 14660, loss = 0.90 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:23:20.828110: step 14670, loss = 0.76 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:23:22.541587: step 14680, loss = 0.87 (746.9 examples/sec; 0.171 sec/batch)
2017-03-25 21:23:24.285154: step 14690, loss = 0.92 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:23:26.161667: step 14700, loss = 0.94 (682.1 examples/sec; 0.188 sec/batch)
2017-03-25 21:23:27.791969: step 14710, loss = 0.84 (785.1 examples/sec; 0.163 sec/batch)
2017-03-25 21:23:29.535469: step 14720, loss = 0.98 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:23:31.258796: step 14730, loss = 0.93 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:23:32.984736: step 14740, loss = 0.77 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:23:34.714701: step 14750, loss = 0.85 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:23:36.446583: step 14760, loss = 0.87 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:23:38.175815: step 14770, loss = 0.83 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:23:39.896697: step 14780, loss = 0.88 (743.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:23:41.644929: step 14790, loss = 0.74 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 21:23:43.567438: step 14800, loss = 0.80 (665.8 examples/sec; 0.192 sec/batch)
2017-03-25 21:23:45.138059: step 14810, loss = 0.79 (814.9 examples/sec; 0.157 sec/batch)
2017-03-25 21:23:46.881794: step 14820, loss = 0.97 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:23:48.629978: step 14830, loss = 0.83 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 21:23:50.367119: step 14840, loss = 0.85 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:23:52.130444: step 14850, loss = 0.85 (725.9 examples/sec; 0.176 sec/batch)
2017-03-25 21:23:53.872895: step 14860, loss = 0.88 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:23:55.610939: step 14870, loss = 0.93 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:23:57.349104: step 14880, loss = 0.83 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:23:59.084987: step 14890, loss = 0.63 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:24:00.973355: step 14900, loss = 0.71 (678.1 examples/sec; 0.189 sec/batch)
2017-03-25 21:24:02.592747: step 14910, loss = 0.81 (790.1 examples/sec; 0.162 sec/batch)
2017-03-25 21:24:04.331401: step 14920, loss = 0.85 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:24:06.059619: step 14930, loss = 0.82 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:24:07.790378: step 14940, loss = 0.88 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:24:09.533178: step 14950, loss = 1.00 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:24:11.262820: step 14960, loss = 0.89 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:24:12.981119: step 14970, loss = 0.81 (744.9 examples/sec; 0.172 sec/batch)
2017-03-25 21:24:14.709468: step 14980, loss = 0.76 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:24:16.456407: step 14990, loss = 0.75 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:24:18.318991: step 15000, loss = 0.90 (687.2 examples/sec; 0.186 sec/batch)
2017-03-25 21:24:19.942467: step 15010, loss = 0.71 (788.4 examples/sec; 0.162 sec/batch)
2017-03-25 21:24:21.671791: step 15020, loss = 1.04 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:24:23.412348: step 15030, loss = 0.84 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:24:25.148506: step 15040, loss = 0.84 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:24:26.880834: step 15050, loss = 0.87 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:24:28.624614: step 15060, loss = 0.74 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:24:30.351119: step 15070, loss = 0.82 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:24:32.078455: step 15080, loss = 0.70 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:24:33.805974: step 15090, loss = 0.80 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:24:35.690348: step 15100, loss = 0.81 (679.7 examples/sec; 0.188 sec/batch)
2017-03-25 21:24:37.294417: step 15110, loss = 0.98 (797.4 examples/sec; 0.161 sec/batch)
2017-03-25 21:24:39.021629: step 15120, loss = 0.94 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:24:40.751444: step 15130, loss = 0.89 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:24:42.481779: step 15140, loss = 0.80 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:24:44.232111: step 15150, loss = 0.68 (731.3 examples/sec; 0.175 sec/batch)
2017-03-25 21:24:45.971183: step 15160, loss = 0.90 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:24:47.695405: step 15170, loss = 0.82 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:24:49.430919: step 15180, loss = 0.84 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:24:51.171643: step 15190, loss = 0.73 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:24:53.064932: step 15200, loss = 0.97 (676.1 examples/sec; 0.189 sec/batch)
2017-03-25 21:24:54.689223: step 15210, loss = 0.75 (788.0 examples/sec; 0.162 sec/batch)
2017-03-25 21:24:56.422984: step 15220, loss = 0.93 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:24:58.161559: step 15230, loss = 0.69 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:24:59.899919: step 15240, loss = 0.76 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:25:01.629122: step 15250, loss = 0.93 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:25:03.391254: step 15260, loss = 0.88 (726.4 examples/sec; 0.176 sec/batch)
2017-03-25 21:25:05.124267: step 15270, loss = 0.88 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:25:06.879706: step 15280, loss = 0.93 (729.2 examples/sec; 0.176 sec/batch)
2017-03-25 21:25:08.612217: step 15290, loss = 0.84 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:25:10.583155: step 15300, loss = 0.84 (649.6 examples/sec; 0.197 sec/batch)
2017-03-25 21:25:12.073191: step 15310, loss = 0.79 (858.8 examples/sec; 0.149 sec/batch)
2017-03-25 21:25:13.761965: step 15320, loss = 0.96 (758.0 examples/sec; 0.169 sec/batch)
2017-03-25 21:25:15.489200: step 15330, loss = 0.80 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:25:17.231583: step 15340, loss = 1.00 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:25:18.973293: step 15350, loss = 0.85 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:25:20.707397: step 15360, loss = 0.83 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:25:22.447635: step 15370, loss = 0.78 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:25:24.176538: step 15380, loss = 1.08 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:25:25.932403: step 15390, loss = 0.85 (729.0 examples/sec; 0.176 sec/batch)
2017-03-25 21:25:27.877002: step 15400, loss = 0.97 (658.5 examples/sec; 0.194 sec/batch)
2017-03-25 21:25:29.434486: step 15410, loss = 1.08 (821.5 examples/sec; 0.156 sec/batch)
2017-03-25 21:25:31.165957: step 15420, loss = 0.77 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:25:32.907347: step 15430, loss = 0.91 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:25:34.643930: step 15440, loss = 0.99 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:25:36.379981: step 15450, loss = 0.89 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:25:38.111707: step 15460, loss = 0.91 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:25:39.847671: step 15470, loss = 0.85 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:25:41.586784: step 15480, loss = 1.02 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:25:43.319579: step 15490, loss = 0.74 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:25:45.227089: step 15500, loss = 0.77 (671.0 examples/sec; 0.191 sec/batch)
2017-03-25 21:25:46.841008: step 15510, loss = 0.74 (793.1 examples/sec; 0.161 sec/batch)
2017-03-25 21:25:48.589533: step 15520, loss = 0.77 (732.0 examples/sec; 0.175 sec/batch)
2017-03-25 21:25:50.318837: step 15530, loss = 0.68 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:25:52.051529: step 15540, loss = 0.76 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:25:53.788290: step 15550, loss = 0.93 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:25:55.533683: step 15560, loss = 0.79 (733.4 examples/sec; 0.175 sec/batch)
2017-03-25 21:25:57.270607: step 15570, loss = 0.85 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:25:59.004656: step 15580, loss = 0.95 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:00.747321: step 15590, loss = 0.80 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:26:02.649918: step 15600, loss = 0.87 (672.8 examples/sec; 0.190 sec/batch)
2017-03-25 21:26:04.278670: step 15610, loss = 0.95 (785.9 examples/sec; 0.163 sec/batch)
2017-03-25 21:26:06.023236: step 15620, loss = 0.76 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:26:07.762977: step 15630, loss = 0.96 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:26:09.496297: step 15640, loss = 0.84 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:11.251481: step 15650, loss = 0.88 (729.3 examples/sec; 0.176 sec/batch)
2017-03-25 21:26:12.995774: step 15660, loss = 0.79 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:26:14.722960: step 15670, loss = 1.04 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:16.454684: step 15680, loss = 0.85 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:18.202901: step 15690, loss = 0.76 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 21:26:20.098647: step 15700, loss = 0.94 (675.1 examples/sec; 0.190 sec/batch)
2017-03-25 21:26:21.717901: step 15710, loss = 0.77 (790.5 examples/sec; 0.162 sec/batch)
2017-03-25 21:26:23.451728: step 15720, loss = 0.82 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:25.177283: step 15730, loss = 0.79 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:26.903848: step 15740, loss = 0.66 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:28.630474: step 15750, loss = 0.93 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:30.373222: step 15760, loss = 0.83 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:26:32.093111: step 15770, loss = 0.83 (744.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:26:33.827936: step 15780, loss = 0.73 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:35.555551: step 15790, loss = 0.80 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:37.460146: step 15800, loss = 0.87 (672.0 examples/sec; 0.190 sec/batch)
2017-03-25 21:26:39.075574: step 15810, loss = 0.86 (792.4 examples/sec; 0.162 sec/batch)
2017-03-25 21:26:40.807276: step 15820, loss = 0.78 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:42.540693: step 15830, loss = 0.79 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:44.268972: step 15840, loss = 0.78 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:45.994867: step 15850, loss = 0.74 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:47.737835: step 15860, loss = 1.14 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:26:49.486903: step 15870, loss = 0.85 (731.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:26:51.221907: step 15880, loss = 0.80 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:52.951127: step 15890, loss = 0.85 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:54.887870: step 15900, loss = 0.74 (660.9 examples/sec; 0.194 sec/batch)
2017-03-25 21:26:56.430369: step 15910, loss = 0.88 (829.8 examples/sec; 0.154 sec/batch)
2017-03-25 21:26:58.160195: step 15920, loss = 0.91 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:26:59.884948: step 15930, loss = 0.88 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:27:01.623400: step 15940, loss = 0.96 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:27:03.352893: step 15950, loss = 0.98 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:27:05.082131: step 15960, loss = 0.83 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:27:06.819926: step 15970, loss = 0.80 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:27:08.554643: step 15980, loss = 0.82 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:27:10.293165: step 15990, loss = 0.75 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:27:12.257409: step 16000, loss = 0.82 (651.6 examples/sec; 0.196 sec/batch)
2017-03-25 21:27:13.803432: step 16010, loss = 1.00 (827.9 examples/sec; 0.155 sec/batch)
2017-03-25 21:27:15.534010: step 16020, loss = 1.01 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:27:17.259732: step 16030, loss = 0.80 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:27:18.983672: step 16040, loss = 0.76 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:27:20.730765: step 16050, loss = 0.81 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:27:22.457672: step 16060, loss = 1.03 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:27:24.175442: step 16070, loss = 0.76 (745.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:27:25.911298: step 16080, loss = 0.85 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:27:27.635294: step 16090, loss = 0.70 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:27:29.529297: step 16100, loss = 0.92 (675.8 examples/sec; 0.189 sec/batch)
2017-03-25 21:27:31.152571: step 16110, loss = 0.75 (788.5 examples/sec; 0.162 sec/batch)
2017-03-25 21:27:32.878575: step 16120, loss = 0.71 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:27:34.591372: step 16130, loss = 0.65 (746.9 examples/sec; 0.171 sec/batch)
2017-03-25 21:27:36.333136: step 16140, loss = 0.77 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:27:38.067305: step 16150, loss = 0.83 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:27:39.792262: step 16160, loss = 0.80 (742.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:27:41.520120: step 16170, loss = 0.89 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:27:43.239194: step 16180, loss = 0.80 (744.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:27:44.970615: step 16190, loss = 0.88 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:27:46.856901: step 16200, loss = 0.92 (678.9 examples/sec; 0.189 sec/batch)
2017-03-25 21:27:48.454225: step 16210, loss = 0.71 (800.9 examples/sec; 0.160 sec/batch)
2017-03-25 21:27:50.196863: step 16220, loss = 0.82 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:27:51.932078: step 16230, loss = 0.97 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:27:53.652671: step 16240, loss = 0.98 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 21:27:55.384067: step 16250, loss = 0.82 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:27:57.123501: step 16260, loss = 0.73 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:27:58.851019: step 16270, loss = 0.85 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:28:00.587591: step 16280, loss = 0.78 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:28:02.333775: step 16290, loss = 0.85 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 21:28:04.318243: step 16300, loss = 1.00 (645.0 examples/sec; 0.198 sec/batch)
2017-03-25 21:28:05.857416: step 16310, loss = 0.93 (831.6 examples/sec; 0.154 sec/batch)
2017-03-25 21:28:07.586378: step 16320, loss = 0.85 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:28:09.323805: step 16330, loss = 0.76 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:28:11.052956: step 16340, loss = 0.82 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:28:12.785660: step 16350, loss = 0.77 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:28:14.508395: step 16360, loss = 0.93 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:28:16.241333: step 16370, loss = 0.73 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:28:17.976238: step 16380, loss = 0.99 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:28:19.698456: step 16390, loss = 0.88 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:28:21.608383: step 16400, loss = 0.89 (670.8 examples/sec; 0.191 sec/batch)
2017-03-25 21:28:23.211359: step 16410, loss = 0.91 (797.7 examples/sec; 0.160 sec/batch)
2017-03-25 21:28:24.933524: step 16420, loss = 0.78 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:28:26.664994: step 16430, loss = 0.97 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:28:28.402510: step 16440, loss = 0.78 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:28:30.141822: step 16450, loss = 0.98 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:28:31.887779: step 16460, loss = 0.69 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:28:33.608276: step 16470, loss = 0.78 (744.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:28:35.359674: step 16480, loss = 0.63 (730.8 examples/sec; 0.175 sec/batch)
2017-03-25 21:28:37.093849: step 16490, loss = 0.67 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:28:39.001283: step 16500, loss = 0.91 (671.3 examples/sec; 0.191 sec/batch)
2017-03-25 21:28:40.609697: step 16510, loss = 0.78 (795.5 examples/sec; 0.161 sec/batch)
2017-03-25 21:28:42.333680: step 16520, loss = 0.88 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:28:44.052058: step 16530, loss = 0.93 (744.9 examples/sec; 0.172 sec/batch)
2017-03-25 21:28:45.790523: step 16540, loss = 0.82 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:28:47.538957: step 16550, loss = 0.84 (732.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:28:49.262129: step 16560, loss = 0.95 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:28:50.983276: step 16570, loss = 0.79 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:28:52.707264: step 16580, loss = 0.73 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:28:54.431449: step 16590, loss = 0.96 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:28:56.327012: step 16600, loss = 0.69 (675.6 examples/sec; 0.189 sec/batch)
2017-03-25 21:28:57.921060: step 16610, loss = 1.04 (802.4 examples/sec; 0.160 sec/batch)
2017-03-25 21:28:59.650970: step 16620, loss = 0.83 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:01.389703: step 16630, loss = 0.82 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:29:03.120700: step 16640, loss = 0.77 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:04.852509: step 16650, loss = 0.87 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:06.581139: step 16660, loss = 0.76 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:08.312184: step 16670, loss = 0.74 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:10.056558: step 16680, loss = 0.76 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:29:11.792165: step 16690, loss = 0.84 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:29:13.671773: step 16700, loss = 0.76 (681.0 examples/sec; 0.188 sec/batch)
2017-03-25 21:29:15.293479: step 16710, loss = 0.76 (789.3 examples/sec; 0.162 sec/batch)
2017-03-25 21:29:17.021122: step 16720, loss = 0.79 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:18.756045: step 16730, loss = 0.94 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:29:20.502100: step 16740, loss = 0.81 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:29:22.224425: step 16750, loss = 0.70 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:29:23.958114: step 16760, loss = 0.83 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:25.696139: step 16770, loss = 0.77 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:29:27.430056: step 16780, loss = 0.78 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:29.161914: step 16790, loss = 0.92 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:31.114016: step 16800, loss = 0.92 (655.9 examples/sec; 0.195 sec/batch)
2017-03-25 21:29:32.674914: step 16810, loss = 0.87 (819.8 examples/sec; 0.156 sec/batch)
2017-03-25 21:29:34.412798: step 16820, loss = 0.90 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:29:36.139187: step 16830, loss = 0.89 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:37.871269: step 16840, loss = 0.87 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:39.600790: step 16850, loss = 0.79 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:41.334334: step 16860, loss = 0.87 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:43.070203: step 16870, loss = 0.77 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:29:44.793461: step 16880, loss = 0.80 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:29:46.530210: step 16890, loss = 0.70 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:29:48.433768: step 16900, loss = 0.78 (672.4 examples/sec; 0.190 sec/batch)
2017-03-25 21:29:50.038678: step 16910, loss = 0.82 (797.5 examples/sec; 0.160 sec/batch)
2017-03-25 21:29:51.771882: step 16920, loss = 0.90 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:53.505362: step 16930, loss = 0.86 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:55.240417: step 16940, loss = 0.71 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:29:56.968830: step 16950, loss = 0.87 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:29:58.696689: step 16960, loss = 0.66 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:30:00.431712: step 16970, loss = 0.72 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:30:02.159770: step 16980, loss = 0.86 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:30:03.917114: step 16990, loss = 0.91 (728.4 examples/sec; 0.176 sec/batch)
2017-03-25 21:30:05.818390: step 17000, loss = 0.92 (673.8 examples/sec; 0.190 sec/batch)
2017-03-25 21:30:07.412019: step 17010, loss = 0.75 (802.5 examples/sec; 0.160 sec/batch)
2017-03-25 21:30:09.134089: step 17020, loss = 0.94 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:30:10.863211: step 17030, loss = 0.76 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:30:12.610605: step 17040, loss = 0.85 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:30:14.348085: step 17050, loss = 0.88 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:30:16.075377: step 17060, loss = 0.95 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:30:17.822550: step 17070, loss = 0.97 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 21:30:19.554118: step 17080, loss = 0.55 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:30:21.292871: step 17090, loss = 0.82 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:30:23.214286: step 17100, loss = 0.97 (666.2 examples/sec; 0.192 sec/batch)
2017-03-25 21:30:24.782582: step 17110, loss = 0.83 (816.2 examples/sec; 0.157 sec/batch)
2017-03-25 21:30:26.512927: step 17120, loss = 0.72 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:30:28.251583: step 17130, loss = 0.86 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:30:29.984352: step 17140, loss = 0.78 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:30:31.733864: step 17150, loss = 0.76 (731.6 examples/sec; 0.175 sec/batch)
2017-03-25 21:30:33.456684: step 17160, loss = 0.84 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:30:35.190909: step 17170, loss = 0.71 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:30:36.922213: step 17180, loss = 0.79 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:30:38.664568: step 17190, loss = 0.79 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:30:40.562783: step 17200, loss = 0.96 (674.4 examples/sec; 0.190 sec/batch)
2017-03-25 21:30:42.156203: step 17210, loss = 0.75 (803.1 examples/sec; 0.159 sec/batch)
2017-03-25 21:30:43.899435: step 17220, loss = 0.81 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:30:45.627234: step 17230, loss = 0.98 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:30:47.356488: step 17240, loss = 0.99 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:30:49.086502: step 17250, loss = 0.95 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:30:50.825816: step 17260, loss = 0.80 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:30:52.578912: step 17270, loss = 0.78 (730.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:30:54.304738: step 17280, loss = 0.80 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:30:56.041823: step 17290, loss = 0.68 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:30:57.943559: step 17300, loss = 0.77 (672.9 examples/sec; 0.190 sec/batch)
2017-03-25 21:30:59.556005: step 17310, loss = 0.97 (793.8 examples/sec; 0.161 sec/batch)
2017-03-25 21:31:01.296802: step 17320, loss = 0.79 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:31:03.041313: step 17330, loss = 0.86 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:31:04.767253: step 17340, loss = 0.80 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:31:06.514076: step 17350, loss = 0.76 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 21:31:08.251295: step 17360, loss = 0.78 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:31:09.991337: step 17370, loss = 0.86 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:31:11.729684: step 17380, loss = 0.95 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:31:13.460104: step 17390, loss = 0.83 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:31:15.442724: step 17400, loss = 0.80 (645.7 examples/sec; 0.198 sec/batch)
2017-03-25 21:31:16.904862: step 17410, loss = 0.81 (875.3 examples/sec; 0.146 sec/batch)
2017-03-25 21:31:18.600472: step 17420, loss = 0.93 (754.9 examples/sec; 0.170 sec/batch)
2017-03-25 21:31:20.340349: step 17430, loss = 0.85 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:31:22.079062: step 17440, loss = 0.90 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:31:23.809774: step 17450, loss = 0.73 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:31:25.532115: step 17460, loss = 0.80 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:31:27.270561: step 17470, loss = 0.90 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:31:28.994314: step 17480, loss = 0.74 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:31:30.723685: step 17490, loss = 0.78 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:31:32.637364: step 17500, loss = 0.88 (668.9 examples/sec; 0.191 sec/batch)
2017-03-25 21:31:34.243879: step 17510, loss = 0.94 (796.7 examples/sec; 0.161 sec/batch)
2017-03-25 21:31:35.978689: step 17520, loss = 0.83 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:31:37.714765: step 17530, loss = 0.87 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:31:39.431594: step 17540, loss = 0.75 (745.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:31:41.165109: step 17550, loss = 0.87 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:31:42.900752: step 17560, loss = 0.89 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:31:44.621943: step 17570, loss = 0.79 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:31:46.354804: step 17580, loss = 0.71 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:31:48.084022: step 17590, loss = 0.85 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:31:49.983643: step 17600, loss = 0.77 (673.8 examples/sec; 0.190 sec/batch)
2017-03-25 21:31:51.590592: step 17610, loss = 0.91 (796.5 examples/sec; 0.161 sec/batch)
2017-03-25 21:31:53.324093: step 17620, loss = 0.97 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:31:55.058752: step 17630, loss = 1.03 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:31:56.782672: step 17640, loss = 0.94 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:31:58.525683: step 17650, loss = 0.74 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:32:00.256756: step 17660, loss = 0.85 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:01.997112: step 17670, loss = 0.88 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:32:03.725325: step 17680, loss = 0.79 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:05.451141: step 17690, loss = 0.86 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:07.339371: step 17700, loss = 0.67 (677.9 examples/sec; 0.189 sec/batch)
2017-03-25 21:32:08.969804: step 17710, loss = 0.67 (785.1 examples/sec; 0.163 sec/batch)
2017-03-25 21:32:10.702463: step 17720, loss = 0.73 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:12.437074: step 17730, loss = 0.85 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:14.163040: step 17740, loss = 0.77 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:15.892163: step 17750, loss = 0.82 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:17.620269: step 17760, loss = 1.01 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:19.347152: step 17770, loss = 0.86 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:21.070910: step 17780, loss = 0.78 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:32:22.802680: step 17790, loss = 0.91 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:24.694057: step 17800, loss = 0.73 (677.0 examples/sec; 0.189 sec/batch)
2017-03-25 21:32:26.310655: step 17810, loss = 0.88 (791.5 examples/sec; 0.162 sec/batch)
2017-03-25 21:32:28.035307: step 17820, loss = 0.88 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:32:29.781240: step 17830, loss = 0.91 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:32:31.522128: step 17840, loss = 0.84 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:32:33.242148: step 17850, loss = 0.91 (744.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:32:34.972157: step 17860, loss = 0.89 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:36.701942: step 17870, loss = 0.75 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:38.432172: step 17880, loss = 0.76 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:40.163730: step 17890, loss = 0.76 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:42.152210: step 17900, loss = 0.79 (643.7 examples/sec; 0.199 sec/batch)
2017-03-25 21:32:43.601904: step 17910, loss = 0.78 (882.9 examples/sec; 0.145 sec/batch)
2017-03-25 21:32:45.299929: step 17920, loss = 0.78 (753.8 examples/sec; 0.170 sec/batch)
2017-03-25 21:32:47.038000: step 17930, loss = 0.68 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:32:48.756712: step 17940, loss = 0.84 (744.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:32:50.497657: step 17950, loss = 0.73 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:32:52.223698: step 17960, loss = 0.84 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:32:53.960488: step 17970, loss = 0.75 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:32:55.695782: step 17980, loss = 0.95 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:32:57.431887: step 17990, loss = 0.72 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:32:59.343704: step 18000, loss = 0.87 (669.9 examples/sec; 0.191 sec/batch)
2017-03-25 21:33:00.960973: step 18010, loss = 0.69 (790.9 examples/sec; 0.162 sec/batch)
2017-03-25 21:33:02.713002: step 18020, loss = 0.79 (730.6 examples/sec; 0.175 sec/batch)
2017-03-25 21:33:04.457559: step 18030, loss = 0.79 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:33:06.201728: step 18040, loss = 0.82 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:33:07.939501: step 18050, loss = 0.82 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:33:09.690378: step 18060, loss = 0.83 (731.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:33:11.420590: step 18070, loss = 0.77 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:33:13.145828: step 18080, loss = 0.85 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:33:14.878651: step 18090, loss = 0.70 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:33:16.765602: step 18100, loss = 0.83 (678.8 examples/sec; 0.189 sec/batch)
2017-03-25 21:33:18.372163: step 18110, loss = 1.09 (796.0 examples/sec; 0.161 sec/batch)
2017-03-25 21:33:20.113017: step 18120, loss = 0.66 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:33:21.854307: step 18130, loss = 0.88 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:33:23.599721: step 18140, loss = 0.80 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 21:33:25.334334: step 18150, loss = 0.91 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:33:27.095127: step 18160, loss = 0.96 (727.0 examples/sec; 0.176 sec/batch)
2017-03-25 21:33:28.823707: step 18170, loss = 0.92 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:33:30.566032: step 18180, loss = 0.75 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:33:32.304890: step 18190, loss = 0.82 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:33:34.196699: step 18200, loss = 0.98 (676.8 examples/sec; 0.189 sec/batch)
2017-03-25 21:33:35.820328: step 18210, loss = 0.87 (788.0 examples/sec; 0.162 sec/batch)
2017-03-25 21:33:37.563774: step 18220, loss = 0.89 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:33:39.300845: step 18230, loss = 0.77 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:33:41.033392: step 18240, loss = 0.74 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:33:42.770356: step 18250, loss = 0.69 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:33:44.513395: step 18260, loss = 0.80 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:33:46.255047: step 18270, loss = 0.76 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:33:47.995796: step 18280, loss = 0.77 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:33:49.726106: step 18290, loss = 0.98 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:33:51.619816: step 18300, loss = 0.88 (675.9 examples/sec; 0.189 sec/batch)
2017-03-25 21:33:53.239548: step 18310, loss = 0.81 (790.3 examples/sec; 0.162 sec/batch)
2017-03-25 21:33:54.983646: step 18320, loss = 0.86 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:33:56.700489: step 18330, loss = 0.82 (745.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:33:58.430688: step 18340, loss = 0.62 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:00.165206: step 18350, loss = 0.75 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:01.916844: step 18360, loss = 0.79 (730.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:34:03.642511: step 18370, loss = 0.73 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:05.384661: step 18380, loss = 0.76 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:34:07.110714: step 18390, loss = 0.95 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:09.008068: step 18400, loss = 0.88 (674.6 examples/sec; 0.190 sec/batch)
2017-03-25 21:34:10.616929: step 18410, loss = 0.79 (795.6 examples/sec; 0.161 sec/batch)
2017-03-25 21:34:12.352372: step 18420, loss = 0.92 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:34:14.090392: step 18430, loss = 0.78 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:34:15.820480: step 18440, loss = 0.70 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:17.555656: step 18450, loss = 0.83 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:34:19.282923: step 18460, loss = 0.81 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:21.029424: step 18470, loss = 0.97 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:34:22.756022: step 18480, loss = 0.86 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:24.494925: step 18490, loss = 0.88 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:34:26.404051: step 18500, loss = 0.73 (670.5 examples/sec; 0.191 sec/batch)
2017-03-25 21:34:28.003672: step 18510, loss = 0.79 (800.2 examples/sec; 0.160 sec/batch)
2017-03-25 21:34:29.743521: step 18520, loss = 1.01 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:34:31.474998: step 18530, loss = 0.83 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:33.204030: step 18540, loss = 0.89 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:34.934447: step 18550, loss = 0.93 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:36.669494: step 18560, loss = 0.78 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:34:38.396272: step 18570, loss = 0.86 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:40.123925: step 18580, loss = 0.72 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:41.865162: step 18590, loss = 0.93 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:34:43.762275: step 18600, loss = 0.83 (674.7 examples/sec; 0.190 sec/batch)
2017-03-25 21:34:45.383600: step 18610, loss = 0.82 (789.5 examples/sec; 0.162 sec/batch)
2017-03-25 21:34:47.116027: step 18620, loss = 0.85 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:48.867902: step 18630, loss = 0.73 (730.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:34:50.602538: step 18640, loss = 0.74 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:52.335892: step 18650, loss = 0.82 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:54.061763: step 18660, loss = 0.94 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:34:55.782635: step 18670, loss = 0.90 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:34:57.527880: step 18680, loss = 0.93 (733.4 examples/sec; 0.175 sec/batch)
2017-03-25 21:34:59.278089: step 18690, loss = 0.59 (731.3 examples/sec; 0.175 sec/batch)
2017-03-25 21:35:01.165329: step 18700, loss = 0.87 (678.5 examples/sec; 0.189 sec/batch)
2017-03-25 21:35:02.774116: step 18710, loss = 0.69 (795.3 examples/sec; 0.161 sec/batch)
2017-03-25 21:35:04.508913: step 18720, loss = 0.73 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:35:06.238904: step 18730, loss = 1.04 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:35:07.966709: step 18740, loss = 0.78 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:35:09.702531: step 18750, loss = 0.76 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:35:11.428108: step 18760, loss = 1.10 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:35:13.171074: step 18770, loss = 0.82 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:35:14.908151: step 18780, loss = 0.84 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:35:16.637705: step 18790, loss = 0.79 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:35:18.532423: step 18800, loss = 0.70 (675.6 examples/sec; 0.189 sec/batch)
2017-03-25 21:35:20.156854: step 18810, loss = 0.57 (788.0 examples/sec; 0.162 sec/batch)
2017-03-25 21:35:21.895105: step 18820, loss = 0.69 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:35:23.611422: step 18830, loss = 0.83 (745.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:35:25.339188: step 18840, loss = 0.82 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:35:27.076050: step 18850, loss = 0.97 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:35:28.810053: step 18860, loss = 0.80 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:35:30.557171: step 18870, loss = 0.68 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:35:32.293500: step 18880, loss = 0.92 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:35:34.025158: step 18890, loss = 0.67 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:35:36.023301: step 18900, loss = 0.80 (640.6 examples/sec; 0.200 sec/batch)
2017-03-25 21:35:37.516754: step 18910, loss = 0.65 (857.1 examples/sec; 0.149 sec/batch)
2017-03-25 21:35:39.224395: step 18920, loss = 0.88 (749.6 examples/sec; 0.171 sec/batch)
2017-03-25 21:35:40.959384: step 18930, loss = 0.76 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:35:42.687486: step 18940, loss = 0.75 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:35:44.424180: step 18950, loss = 0.75 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:35:46.153323: step 18960, loss = 0.76 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:35:47.882901: step 18970, loss = 0.90 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:35:49.608948: step 18980, loss = 0.85 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:35:51.339797: step 18990, loss = 0.73 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:35:53.227856: step 19000, loss = 0.77 (678.3 examples/sec; 0.189 sec/batch)
2017-03-25 21:35:54.845519: step 19010, loss = 0.70 (790.7 examples/sec; 0.162 sec/batch)
2017-03-25 21:35:56.583717: step 19020, loss = 0.77 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:35:58.319160: step 19030, loss = 0.78 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:36:00.049940: step 19040, loss = 0.80 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:01.789833: step 19050, loss = 0.80 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:36:03.518672: step 19060, loss = 0.83 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:05.260845: step 19070, loss = 0.82 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:36:06.985980: step 19080, loss = 0.75 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:08.716689: step 19090, loss = 0.85 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:10.620011: step 19100, loss = 0.88 (672.5 examples/sec; 0.190 sec/batch)
2017-03-25 21:36:12.211958: step 19110, loss = 0.84 (804.0 examples/sec; 0.159 sec/batch)
2017-03-25 21:36:13.945165: step 19120, loss = 0.80 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:15.675269: step 19130, loss = 0.78 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:17.415156: step 19140, loss = 0.78 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:36:19.149585: step 19150, loss = 0.93 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:20.879653: step 19160, loss = 0.77 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:22.615520: step 19170, loss = 0.95 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:36:24.353350: step 19180, loss = 0.78 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:36:26.085965: step 19190, loss = 0.87 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:28.044266: step 19200, loss = 0.76 (654.0 examples/sec; 0.196 sec/batch)
2017-03-25 21:36:29.607243: step 19210, loss = 0.82 (818.4 examples/sec; 0.156 sec/batch)
2017-03-25 21:36:31.332883: step 19220, loss = 0.81 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:33.064438: step 19230, loss = 0.78 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:34.800758: step 19240, loss = 0.72 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:36:36.522962: step 19250, loss = 0.93 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:36:38.251073: step 19260, loss = 0.83 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:39.979530: step 19270, loss = 0.77 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:41.714222: step 19280, loss = 0.88 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:43.450784: step 19290, loss = 0.80 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:36:45.451064: step 19300, loss = 0.84 (639.9 examples/sec; 0.200 sec/batch)
2017-03-25 21:36:46.955756: step 19310, loss = 1.16 (850.8 examples/sec; 0.150 sec/batch)
2017-03-25 21:36:48.671573: step 19320, loss = 0.88 (745.9 examples/sec; 0.172 sec/batch)
2017-03-25 21:36:50.420859: step 19330, loss = 0.72 (731.8 examples/sec; 0.175 sec/batch)
2017-03-25 21:36:52.153201: step 19340, loss = 0.87 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:53.878595: step 19350, loss = 0.86 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:55.614972: step 19360, loss = 1.07 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:36:57.345573: step 19370, loss = 0.81 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:36:59.083655: step 19380, loss = 0.84 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:37:00.809396: step 19390, loss = 0.96 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:37:02.757149: step 19400, loss = 0.76 (657.9 examples/sec; 0.195 sec/batch)
2017-03-25 21:37:04.294987: step 19410, loss = 0.83 (831.2 examples/sec; 0.154 sec/batch)
2017-03-25 21:37:06.008234: step 19420, loss = 0.84 (747.1 examples/sec; 0.171 sec/batch)
2017-03-25 21:37:07.756596: step 19430, loss = 0.79 (732.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:37:09.483488: step 19440, loss = 0.82 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:37:11.219228: step 19450, loss = 0.69 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:37:12.952841: step 19460, loss = 0.85 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:37:14.675210: step 19470, loss = 0.87 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:37:16.407855: step 19480, loss = 0.87 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:37:18.143021: step 19490, loss = 0.85 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:37:20.068263: step 19500, loss = 0.85 (665.1 examples/sec; 0.192 sec/batch)
2017-03-25 21:37:21.643328: step 19510, loss = 0.90 (812.3 examples/sec; 0.158 sec/batch)
2017-03-25 21:37:23.378841: step 19520, loss = 0.81 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:37:25.114723: step 19530, loss = 0.92 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:37:26.837386: step 19540, loss = 0.86 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:37:28.571161: step 19550, loss = 1.04 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:37:30.311638: step 19560, loss = 0.69 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:37:32.034336: step 19570, loss = 0.75 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:37:33.764858: step 19580, loss = 0.77 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:37:35.510967: step 19590, loss = 0.73 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:37:37.408010: step 19600, loss = 0.68 (674.8 examples/sec; 0.190 sec/batch)
2017-03-25 21:37:39.036591: step 19610, loss = 0.83 (785.9 examples/sec; 0.163 sec/batch)
2017-03-25 21:37:40.758970: step 19620, loss = 0.83 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:37:42.487486: step 19630, loss = 0.72 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:37:44.222253: step 19640, loss = 0.87 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:37:45.953787: step 19650, loss = 0.86 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:37:47.687786: step 19660, loss = 0.82 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:37:49.421279: step 19670, loss = 0.73 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:37:51.152019: step 19680, loss = 0.90 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:37:52.880394: step 19690, loss = 0.95 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:37:54.803608: step 19700, loss = 0.93 (665.7 examples/sec; 0.192 sec/batch)
2017-03-25 21:37:56.380541: step 19710, loss = 0.80 (811.5 examples/sec; 0.158 sec/batch)
2017-03-25 21:37:58.092834: step 19720, loss = 0.82 (747.5 examples/sec; 0.171 sec/batch)
2017-03-25 21:37:59.820416: step 19730, loss = 0.63 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:38:01.555588: step 19740, loss = 0.74 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:38:03.277589: step 19750, loss = 0.83 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:38:05.015392: step 19760, loss = 0.79 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:38:06.746673: step 19770, loss = 0.82 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:38:08.479751: step 19780, loss = 0.98 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:38:10.220562: step 19790, loss = 0.87 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:38:12.170349: step 19800, loss = 0.72 (656.5 examples/sec; 0.195 sec/batch)
2017-03-25 21:38:13.714828: step 19810, loss = 0.78 (828.8 examples/sec; 0.154 sec/batch)
2017-03-25 21:38:15.451103: step 19820, loss = 0.89 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:38:17.181958: step 19830, loss = 0.95 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:38:18.914942: step 19840, loss = 0.93 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:38:20.641974: step 19850, loss = 0.82 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:38:22.366166: step 19860, loss = 1.08 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:38:24.094559: step 19870, loss = 0.83 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:38:25.822858: step 19880, loss = 0.68 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:38:27.544001: step 19890, loss = 0.97 (743.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:38:29.429834: step 19900, loss = 0.87 (678.7 examples/sec; 0.189 sec/batch)
2017-03-25 21:38:31.049233: step 19910, loss = 0.72 (790.4 examples/sec; 0.162 sec/batch)
2017-03-25 21:38:32.767037: step 19920, loss = 0.86 (745.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:38:34.502327: step 19930, loss = 0.67 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:38:36.221834: step 19940, loss = 0.76 (744.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:38:37.955756: step 19950, loss = 0.89 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:38:39.679789: step 19960, loss = 0.79 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:38:41.409398: step 19970, loss = 0.86 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:38:43.144499: step 19980, loss = 0.81 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:38:44.879796: step 19990, loss = 0.84 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:38:46.789615: step 20000, loss = 0.75 (670.2 examples/sec; 0.191 sec/batch)
2017-03-25 21:38:48.419774: step 20010, loss = 0.75 (785.2 examples/sec; 0.163 sec/batch)
2017-03-25 21:38:50.150078: step 20020, loss = 0.76 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:38:51.883951: step 20030, loss = 0.77 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:38:53.606733: step 20040, loss = 0.80 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:38:55.343792: step 20050, loss = 1.06 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:38:57.055434: step 20060, loss = 0.71 (747.8 examples/sec; 0.171 sec/batch)
2017-03-25 21:38:58.800149: step 20070, loss = 0.76 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:39:00.529313: step 20080, loss = 0.84 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:39:02.265582: step 20090, loss = 0.89 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:39:04.164566: step 20100, loss = 0.83 (674.0 examples/sec; 0.190 sec/batch)
2017-03-25 21:39:05.776415: step 20110, loss = 0.96 (794.1 examples/sec; 0.161 sec/batch)
2017-03-25 21:39:07.505328: step 20120, loss = 0.86 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:39:09.228377: step 20130, loss = 0.96 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:39:10.955822: step 20140, loss = 0.78 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:39:12.702730: step 20150, loss = 0.90 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:39:14.441851: step 20160, loss = 0.91 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:39:16.176379: step 20170, loss = 0.75 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:39:17.908230: step 20180, loss = 0.87 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:39:19.637738: step 20190, loss = 0.59 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:39:21.538049: step 20200, loss = 0.92 (673.9 examples/sec; 0.190 sec/batch)
2017-03-25 21:39:23.138556: step 20210, loss = 0.95 (799.3 examples/sec; 0.160 sec/batch)
2017-03-25 21:39:24.890255: step 20220, loss = 0.67 (730.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:39:26.614725: step 20230, loss = 0.82 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:39:28.368086: step 20240, loss = 0.70 (730.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:39:30.092505: step 20250, loss = 0.79 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:39:31.805346: step 20260, loss = 0.78 (747.3 examples/sec; 0.171 sec/batch)
2017-03-25 21:39:33.544554: step 20270, loss = 0.78 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:39:35.272501: step 20280, loss = 0.82 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:39:37.008704: step 20290, loss = 0.78 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:39:38.983253: step 20300, loss = 0.67 (648.2 examples/sec; 0.197 sec/batch)
2017-03-25 21:39:40.524547: step 20310, loss = 0.71 (830.5 examples/sec; 0.154 sec/batch)
2017-03-25 21:39:42.264898: step 20320, loss = 0.72 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:39:43.993378: step 20330, loss = 0.93 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:39:45.715062: step 20340, loss = 0.72 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:39:47.449783: step 20350, loss = 0.70 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:39:49.169380: step 20360, loss = 0.75 (744.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:39:50.918728: step 20370, loss = 0.78 (731.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:39:52.654814: step 20380, loss = 0.97 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:39:54.388780: step 20390, loss = 0.84 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:39:56.280942: step 20400, loss = 0.94 (676.5 examples/sec; 0.189 sec/batch)
2017-03-25 21:39:57.896940: step 20410, loss = 0.77 (792.1 examples/sec; 0.162 sec/batch)
2017-03-25 21:39:59.629936: step 20420, loss = 0.62 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:40:01.364230: step 20430, loss = 0.97 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:40:03.105066: step 20440, loss = 0.61 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:40:04.854627: step 20450, loss = 0.81 (731.6 examples/sec; 0.175 sec/batch)
2017-03-25 21:40:06.590166: step 20460, loss = 0.80 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:40:08.326989: step 20470, loss = 0.81 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:40:10.079087: step 20480, loss = 0.78 (730.6 examples/sec; 0.175 sec/batch)
2017-03-25 21:40:11.813954: step 20490, loss = 0.72 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:40:13.722281: step 20500, loss = 0.80 (670.8 examples/sec; 0.191 sec/batch)
2017-03-25 21:40:15.322823: step 20510, loss = 0.89 (799.7 examples/sec; 0.160 sec/batch)
2017-03-25 21:40:17.058832: step 20520, loss = 0.87 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:40:18.809014: step 20530, loss = 0.92 (731.4 examples/sec; 0.175 sec/batch)
2017-03-25 21:40:20.537156: step 20540, loss = 0.73 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:40:22.253696: step 20550, loss = 0.77 (745.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:40:23.980979: step 20560, loss = 0.91 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:40:25.717188: step 20570, loss = 0.71 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:40:27.447388: step 20580, loss = 0.78 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:40:29.175481: step 20590, loss = 0.81 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:40:31.072093: step 20600, loss = 0.87 (674.7 examples/sec; 0.190 sec/batch)
2017-03-25 21:40:32.692242: step 20610, loss = 0.92 (790.1 examples/sec; 0.162 sec/batch)
2017-03-25 21:40:34.434723: step 20620, loss = 0.84 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:40:36.172574: step 20630, loss = 0.75 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:40:37.889134: step 20640, loss = 0.70 (745.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:40:39.626620: step 20650, loss = 0.77 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:40:41.360166: step 20660, loss = 0.66 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:40:43.088690: step 20670, loss = 0.85 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:40:44.820119: step 20680, loss = 0.65 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:40:46.561485: step 20690, loss = 0.79 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:40:48.522554: step 20700, loss = 0.83 (653.7 examples/sec; 0.196 sec/batch)
2017-03-25 21:40:50.072348: step 20710, loss = 0.73 (824.3 examples/sec; 0.155 sec/batch)
2017-03-25 21:40:51.797702: step 20720, loss = 0.84 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:40:53.546422: step 20730, loss = 0.81 (732.0 examples/sec; 0.175 sec/batch)
2017-03-25 21:40:55.285261: step 20740, loss = 0.66 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:40:57.007865: step 20750, loss = 0.87 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:40:58.758759: step 20760, loss = 0.94 (731.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:41:00.489289: step 20770, loss = 0.77 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:41:02.231000: step 20780, loss = 0.70 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:41:03.972875: step 20790, loss = 0.75 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:41:05.946001: step 20800, loss = 0.82 (649.0 examples/sec; 0.197 sec/batch)
2017-03-25 21:41:07.469052: step 20810, loss = 0.89 (840.1 examples/sec; 0.152 sec/batch)
2017-03-25 21:41:09.207451: step 20820, loss = 0.84 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:41:10.935085: step 20830, loss = 0.85 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:41:12.684939: step 20840, loss = 0.74 (731.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:41:14.411202: step 20850, loss = 0.78 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:41:16.147311: step 20860, loss = 0.91 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:41:17.880061: step 20870, loss = 0.72 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:41:19.604187: step 20880, loss = 0.71 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:41:21.331787: step 20890, loss = 0.74 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:41:23.241163: step 20900, loss = 0.82 (670.4 examples/sec; 0.191 sec/batch)
2017-03-25 21:41:24.820544: step 20910, loss = 0.75 (810.4 examples/sec; 0.158 sec/batch)
2017-03-25 21:41:26.550872: step 20920, loss = 0.92 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:41:28.282807: step 20930, loss = 0.85 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:41:30.024412: step 20940, loss = 0.83 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:41:31.760612: step 20950, loss = 0.84 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:41:33.512394: step 20960, loss = 0.86 (730.6 examples/sec; 0.175 sec/batch)
2017-03-25 21:41:35.256418: step 20970, loss = 1.19 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:41:36.986960: step 20980, loss = 0.69 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:41:38.721343: step 20990, loss = 0.96 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:41:40.644720: step 21000, loss = 0.72 (665.7 examples/sec; 0.192 sec/batch)
2017-03-25 21:41:42.251876: step 21010, loss = 0.70 (796.1 examples/sec; 0.161 sec/batch)
2017-03-25 21:41:43.998625: step 21020, loss = 0.83 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 21:41:45.728907: step 21030, loss = 0.84 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:41:47.467029: step 21040, loss = 0.62 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:41:49.204243: step 21050, loss = 0.69 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:41:50.939342: step 21060, loss = 0.69 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:41:52.682096: step 21070, loss = 0.89 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:41:54.410931: step 21080, loss = 0.72 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:41:56.143196: step 21090, loss = 0.77 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:41:58.021706: step 21100, loss = 0.85 (681.4 examples/sec; 0.188 sec/batch)
2017-03-25 21:41:59.647042: step 21110, loss = 0.86 (787.5 examples/sec; 0.163 sec/batch)
2017-03-25 21:42:01.392723: step 21120, loss = 0.77 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 21:42:03.114682: step 21130, loss = 0.77 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:42:04.847877: step 21140, loss = 0.74 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:42:06.570409: step 21150, loss = 0.83 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:42:08.302366: step 21160, loss = 0.92 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:42:10.036663: step 21170, loss = 0.62 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:42:11.777159: step 21180, loss = 0.71 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:42:13.503314: step 21190, loss = 0.74 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:42:15.388876: step 21200, loss = 0.69 (678.8 examples/sec; 0.189 sec/batch)
2017-03-25 21:42:16.998068: step 21210, loss = 0.71 (795.4 examples/sec; 0.161 sec/batch)
2017-03-25 21:42:18.743577: step 21220, loss = 0.72 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 21:42:20.494607: step 21230, loss = 0.83 (731.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:42:22.216903: step 21240, loss = 0.77 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:42:23.949760: step 21250, loss = 0.81 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:42:25.678226: step 21260, loss = 0.82 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:42:27.403319: step 21270, loss = 0.69 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:42:29.153970: step 21280, loss = 0.74 (731.2 examples/sec; 0.175 sec/batch)
2017-03-25 21:42:30.893669: step 21290, loss = 0.92 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:42:32.803440: step 21300, loss = 0.86 (670.1 examples/sec; 0.191 sec/batch)
2017-03-25 21:42:34.376732: step 21310, loss = 0.81 (813.7 examples/sec; 0.157 sec/batch)
2017-03-25 21:42:36.110944: step 21320, loss = 0.84 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:42:37.837027: step 21330, loss = 0.89 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:42:39.572257: step 21340, loss = 0.79 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:42:41.300925: step 21350, loss = 0.89 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:42:43.037880: step 21360, loss = 0.72 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:42:44.785532: step 21370, loss = 0.91 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:42:46.525066: step 21380, loss = 0.77 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:42:48.257136: step 21390, loss = 0.76 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:42:50.220803: step 21400, loss = 0.80 (652.1 examples/sec; 0.196 sec/batch)
2017-03-25 21:42:51.738436: step 21410, loss = 0.72 (842.9 examples/sec; 0.152 sec/batch)
2017-03-25 21:42:53.468403: step 21420, loss = 0.88 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:42:55.204181: step 21430, loss = 0.75 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:42:56.940664: step 21440, loss = 0.86 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:42:58.664394: step 21450, loss = 0.84 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:43:00.392705: step 21460, loss = 0.81 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:02.122240: step 21470, loss = 0.81 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:03.845304: step 21480, loss = 0.79 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:43:05.579553: step 21490, loss = 0.95 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:07.484002: step 21500, loss = 0.71 (672.4 examples/sec; 0.190 sec/batch)
2017-03-25 21:43:09.111992: step 21510, loss = 0.97 (785.9 examples/sec; 0.163 sec/batch)
2017-03-25 21:43:10.841242: step 21520, loss = 0.86 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:12.571834: step 21530, loss = 0.92 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:14.318972: step 21540, loss = 0.78 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 21:43:16.054980: step 21550, loss = 0.79 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:43:17.788463: step 21560, loss = 0.82 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:19.518318: step 21570, loss = 0.76 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:21.249976: step 21580, loss = 0.93 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:22.978555: step 21590, loss = 0.88 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:24.904818: step 21600, loss = 0.70 (664.5 examples/sec; 0.193 sec/batch)
2017-03-25 21:43:26.466569: step 21610, loss = 0.72 (819.6 examples/sec; 0.156 sec/batch)
2017-03-25 21:43:28.199084: step 21620, loss = 0.74 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:29.933561: step 21630, loss = 0.88 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:31.676722: step 21640, loss = 0.80 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:43:33.410644: step 21650, loss = 0.71 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:35.137955: step 21660, loss = 0.68 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:36.885927: step 21670, loss = 0.83 (732.3 examples/sec; 0.175 sec/batch)
2017-03-25 21:43:38.602478: step 21680, loss = 0.71 (745.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:43:40.347947: step 21690, loss = 0.82 (733.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:43:42.237540: step 21700, loss = 0.83 (677.3 examples/sec; 0.189 sec/batch)
2017-03-25 21:43:43.838250: step 21710, loss = 0.71 (799.6 examples/sec; 0.160 sec/batch)
2017-03-25 21:43:45.562563: step 21720, loss = 0.84 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:43:47.285015: step 21730, loss = 0.83 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:43:49.016071: step 21740, loss = 0.70 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:50.734275: step 21750, loss = 0.78 (745.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:43:52.460738: step 21760, loss = 0.84 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:54.199703: step 21770, loss = 0.87 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:43:55.916466: step 21780, loss = 0.74 (745.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:43:57.650591: step 21790, loss = 0.80 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:43:59.548790: step 21800, loss = 0.72 (674.7 examples/sec; 0.190 sec/batch)
2017-03-25 21:44:01.159636: step 21810, loss = 0.85 (794.2 examples/sec; 0.161 sec/batch)
2017-03-25 21:44:02.886342: step 21820, loss = 0.67 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:44:04.629895: step 21830, loss = 0.86 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:44:06.359451: step 21840, loss = 0.86 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:44:08.099384: step 21850, loss = 0.78 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:44:09.837399: step 21860, loss = 0.73 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:44:11.570226: step 21870, loss = 0.85 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:44:13.302132: step 21880, loss = 0.78 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:44:15.042228: step 21890, loss = 0.71 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:44:16.941273: step 21900, loss = 0.79 (674.4 examples/sec; 0.190 sec/batch)
2017-03-25 21:44:18.550875: step 21910, loss = 0.62 (794.7 examples/sec; 0.161 sec/batch)
2017-03-25 21:44:20.291666: step 21920, loss = 0.76 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:44:22.020210: step 21930, loss = 0.79 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:44:23.755211: step 21940, loss = 0.73 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:44:25.485754: step 21950, loss = 0.68 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:44:27.217253: step 21960, loss = 0.87 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:44:28.960097: step 21970, loss = 0.86 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:44:30.693707: step 21980, loss = 0.85 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:44:32.420221: step 21990, loss = 0.75 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:44:34.328713: step 22000, loss = 0.81 (670.7 examples/sec; 0.191 sec/batch)
2017-03-25 21:44:35.939356: step 22010, loss = 0.91 (794.7 examples/sec; 0.161 sec/batch)
2017-03-25 21:44:37.662115: step 22020, loss = 0.82 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:44:39.412829: step 22030, loss = 0.76 (731.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:44:41.145081: step 22040, loss = 0.78 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:44:42.872975: step 22050, loss = 0.96 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:44:44.613900: step 22060, loss = 0.66 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:44:46.356978: step 22070, loss = 1.02 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:44:48.088199: step 22080, loss = 0.67 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:44:49.812160: step 22090, loss = 0.98 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:44:51.707492: step 22100, loss = 0.70 (675.6 examples/sec; 0.189 sec/batch)
2017-03-25 21:44:53.332025: step 22110, loss = 1.00 (787.6 examples/sec; 0.163 sec/batch)
2017-03-25 21:44:55.065127: step 22120, loss = 0.79 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:44:56.804091: step 22130, loss = 0.65 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:44:58.534253: step 22140, loss = 0.88 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:45:00.273663: step 22150, loss = 0.78 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:45:02.014877: step 22160, loss = 0.90 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:45:03.748315: step 22170, loss = 0.81 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:45:05.496090: step 22180, loss = 0.79 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 21:45:07.225674: step 22190, loss = 0.87 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:45:09.121027: step 22200, loss = 0.78 (675.3 examples/sec; 0.190 sec/batch)
2017-03-25 21:45:10.734966: step 22210, loss = 0.82 (793.1 examples/sec; 0.161 sec/batch)
2017-03-25 21:45:12.473588: step 22220, loss = 0.75 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:45:14.215367: step 22230, loss = 0.82 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:45:15.940705: step 22240, loss = 0.68 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:45:17.679448: step 22250, loss = 0.87 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:45:19.399524: step 22260, loss = 0.82 (744.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:45:21.138606: step 22270, loss = 0.83 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:45:22.874822: step 22280, loss = 0.82 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:45:24.604207: step 22290, loss = 0.89 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:45:26.531437: step 22300, loss = 0.79 (664.1 examples/sec; 0.193 sec/batch)
2017-03-25 21:45:28.111934: step 22310, loss = 0.83 (809.9 examples/sec; 0.158 sec/batch)
2017-03-25 21:45:29.849405: step 22320, loss = 0.83 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:45:31.590603: step 22330, loss = 0.77 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:45:33.318204: step 22340, loss = 0.81 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:45:35.062672: step 22350, loss = 0.91 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:45:36.791096: step 22360, loss = 0.72 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:45:38.524389: step 22370, loss = 0.74 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:45:40.259164: step 22380, loss = 0.88 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:45:41.982226: step 22390, loss = 0.69 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 21:45:43.890411: step 22400, loss = 0.89 (671.1 examples/sec; 0.191 sec/batch)
2017-03-25 21:45:45.490687: step 22410, loss = 0.94 (799.5 examples/sec; 0.160 sec/batch)
2017-03-25 21:45:47.225283: step 22420, loss = 0.71 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:45:48.960944: step 22430, loss = 0.69 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:45:50.683198: step 22440, loss = 0.68 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:45:52.422790: step 22450, loss = 0.70 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:45:54.159208: step 22460, loss = 0.88 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:45:55.887330: step 22470, loss = 0.83 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:45:57.619052: step 22480, loss = 0.93 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:45:59.356833: step 22490, loss = 0.82 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:46:01.248959: step 22500, loss = 0.79 (676.6 examples/sec; 0.189 sec/batch)
2017-03-25 21:46:02.873324: step 22510, loss = 1.22 (787.8 examples/sec; 0.162 sec/batch)
2017-03-25 21:46:04.613531: step 22520, loss = 0.77 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:46:06.350266: step 22530, loss = 0.89 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:46:08.085741: step 22540, loss = 0.82 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:46:09.800490: step 22550, loss = 0.83 (746.5 examples/sec; 0.171 sec/batch)
2017-03-25 21:46:11.543861: step 22560, loss = 0.75 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:46:13.273210: step 22570, loss = 1.04 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:46:15.018094: step 22580, loss = 0.79 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:46:16.748545: step 22590, loss = 0.88 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:46:18.630691: step 22600, loss = 0.75 (680.1 examples/sec; 0.188 sec/batch)
2017-03-25 21:46:20.243093: step 22610, loss = 0.83 (793.9 examples/sec; 0.161 sec/batch)
2017-03-25 21:46:21.984350: step 22620, loss = 0.67 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:46:23.727313: step 22630, loss = 0.84 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:46:25.462688: step 22640, loss = 0.75 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:46:27.193460: step 22650, loss = 0.84 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:46:28.929812: step 22660, loss = 0.96 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:46:30.676531: step 22670, loss = 0.82 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:46:32.417753: step 22680, loss = 0.74 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:46:34.147312: step 22690, loss = 0.88 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:46:36.114435: step 22700, loss = 0.72 (651.0 examples/sec; 0.197 sec/batch)
2017-03-25 21:46:37.624454: step 22710, loss = 0.77 (847.8 examples/sec; 0.151 sec/batch)
2017-03-25 21:46:39.314769: step 22720, loss = 0.68 (756.8 examples/sec; 0.169 sec/batch)
2017-03-25 21:46:41.061885: step 22730, loss = 0.75 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 21:46:42.796232: step 22740, loss = 0.80 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:46:44.550559: step 22750, loss = 0.72 (729.6 examples/sec; 0.175 sec/batch)
2017-03-25 21:46:46.271577: step 22760, loss = 0.82 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:46:47.999491: step 22770, loss = 0.79 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:46:49.730102: step 22780, loss = 0.84 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:46:51.471062: step 22790, loss = 0.71 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:46:53.381483: step 22800, loss = 0.93 (670.0 examples/sec; 0.191 sec/batch)
2017-03-25 21:46:54.974588: step 22810, loss = 0.79 (803.5 examples/sec; 0.159 sec/batch)
2017-03-25 21:46:56.707288: step 22820, loss = 0.86 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:46:58.448175: step 22830, loss = 0.89 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:47:00.163755: step 22840, loss = 0.74 (746.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:47:01.906461: step 22850, loss = 0.83 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:47:03.641949: step 22860, loss = 0.86 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:47:05.369035: step 22870, loss = 0.70 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:47:07.103393: step 22880, loss = 0.95 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:47:08.835679: step 22890, loss = 0.86 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:47:10.741906: step 22900, loss = 0.90 (672.0 examples/sec; 0.190 sec/batch)
2017-03-25 21:47:12.341851: step 22910, loss = 0.86 (799.3 examples/sec; 0.160 sec/batch)
2017-03-25 21:47:14.074287: step 22920, loss = 0.81 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:47:15.799840: step 22930, loss = 0.86 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:47:17.535911: step 22940, loss = 0.68 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:47:19.275258: step 22950, loss = 0.76 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:47:21.008020: step 22960, loss = 0.67 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:47:22.738162: step 22970, loss = 0.78 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:47:24.482640: step 22980, loss = 0.64 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:47:26.215106: step 22990, loss = 0.81 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:47:28.103680: step 23000, loss = 0.89 (678.1 examples/sec; 0.189 sec/batch)
2017-03-25 21:47:29.720850: step 23010, loss = 0.70 (791.0 examples/sec; 0.162 sec/batch)
2017-03-25 21:47:31.450664: step 23020, loss = 0.90 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:47:33.187579: step 23030, loss = 0.85 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:47:34.922572: step 23040, loss = 0.88 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:47:36.661184: step 23050, loss = 0.66 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:47:38.416969: step 23060, loss = 0.79 (728.9 examples/sec; 0.176 sec/batch)
2017-03-25 21:47:40.155177: step 23070, loss = 0.72 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:47:41.908283: step 23080, loss = 0.91 (730.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:47:43.637857: step 23090, loss = 0.93 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:47:45.618728: step 23100, loss = 0.73 (646.2 examples/sec; 0.198 sec/batch)
2017-03-25 21:47:47.117629: step 23110, loss = 0.76 (853.9 examples/sec; 0.150 sec/batch)
2017-03-25 21:47:48.820105: step 23120, loss = 1.07 (751.9 examples/sec; 0.170 sec/batch)
2017-03-25 21:47:50.552236: step 23130, loss = 0.88 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:47:52.282705: step 23140, loss = 0.87 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:47:54.028928: step 23150, loss = 0.88 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 21:47:55.783581: step 23160, loss = 0.82 (729.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:47:57.516206: step 23170, loss = 0.78 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:47:59.239919: step 23180, loss = 0.73 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:48:00.975511: step 23190, loss = 0.84 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:48:02.889777: step 23200, loss = 0.70 (668.6 examples/sec; 0.191 sec/batch)
2017-03-25 21:48:04.487913: step 23210, loss = 0.70 (800.9 examples/sec; 0.160 sec/batch)
2017-03-25 21:48:06.228092: step 23220, loss = 0.76 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:48:07.957526: step 23230, loss = 0.83 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:48:09.691755: step 23240, loss = 0.89 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:48:11.430817: step 23250, loss = 0.89 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:48:13.159052: step 23260, loss = 0.73 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:48:14.885190: step 23270, loss = 0.75 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:48:16.609912: step 23280, loss = 0.76 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:48:18.338982: step 23290, loss = 0.83 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:48:20.243722: step 23300, loss = 0.78 (672.2 examples/sec; 0.190 sec/batch)
2017-03-25 21:48:21.853893: step 23310, loss = 0.96 (794.7 examples/sec; 0.161 sec/batch)
2017-03-25 21:48:23.579386: step 23320, loss = 0.82 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:48:25.310719: step 23330, loss = 0.85 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:48:27.042146: step 23340, loss = 0.73 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:48:28.761817: step 23350, loss = 0.81 (744.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:48:30.488978: step 23360, loss = 0.82 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:48:32.233673: step 23370, loss = 0.96 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:48:33.970075: step 23380, loss = 0.74 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:48:35.700080: step 23390, loss = 0.74 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:48:37.612717: step 23400, loss = 0.72 (669.4 examples/sec; 0.191 sec/batch)
2017-03-25 21:48:39.186014: step 23410, loss = 0.73 (813.3 examples/sec; 0.157 sec/batch)
2017-03-25 21:48:40.925575: step 23420, loss = 0.79 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:48:42.653226: step 23430, loss = 0.86 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:48:44.396487: step 23440, loss = 0.77 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:48:46.133964: step 23450, loss = 0.66 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:48:47.863255: step 23460, loss = 0.80 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:48:49.602958: step 23470, loss = 0.68 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:48:51.334004: step 23480, loss = 0.99 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:48:53.061933: step 23490, loss = 0.83 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:48:54.943394: step 23500, loss = 0.81 (680.6 examples/sec; 0.188 sec/batch)
2017-03-25 21:48:56.570318: step 23510, loss = 0.91 (786.4 examples/sec; 0.163 sec/batch)
2017-03-25 21:48:58.299135: step 23520, loss = 0.81 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:49:00.044662: step 23530, loss = 0.79 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 21:49:01.766761: step 23540, loss = 0.91 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:49:03.516758: step 23550, loss = 0.68 (731.4 examples/sec; 0.175 sec/batch)
2017-03-25 21:49:05.240009: step 23560, loss = 0.86 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:49:06.965681: step 23570, loss = 0.75 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:49:08.695630: step 23580, loss = 0.92 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:49:10.421937: step 23590, loss = 0.76 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:49:12.369227: step 23600, loss = 0.79 (657.3 examples/sec; 0.195 sec/batch)
2017-03-25 21:49:13.923262: step 23610, loss = 0.76 (823.7 examples/sec; 0.155 sec/batch)
2017-03-25 21:49:15.639051: step 23620, loss = 0.81 (746.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:49:17.362536: step 23630, loss = 0.69 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:49:19.100002: step 23640, loss = 0.71 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:49:20.846316: step 23650, loss = 0.89 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 21:49:22.578681: step 23660, loss = 0.73 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:49:24.309447: step 23670, loss = 0.78 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:49:26.052057: step 23680, loss = 0.78 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:49:27.790604: step 23690, loss = 0.80 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:49:29.787351: step 23700, loss = 0.65 (641.4 examples/sec; 0.200 sec/batch)
2017-03-25 21:49:31.317474: step 23710, loss = 0.74 (836.0 examples/sec; 0.153 sec/batch)
2017-03-25 21:49:33.042661: step 23720, loss = 0.79 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:49:34.780412: step 23730, loss = 0.91 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:49:36.508239: step 23740, loss = 0.83 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:49:38.247807: step 23750, loss = 0.94 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:49:39.962970: step 23760, loss = 0.90 (746.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:49:41.692017: step 23770, loss = 0.86 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:49:43.408327: step 23780, loss = 0.88 (745.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:49:45.129376: step 23790, loss = 0.74 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:49:47.024661: step 23800, loss = 0.75 (675.4 examples/sec; 0.190 sec/batch)
2017-03-25 21:49:48.622118: step 23810, loss = 0.74 (801.3 examples/sec; 0.160 sec/batch)
2017-03-25 21:49:50.337027: step 23820, loss = 0.82 (746.4 examples/sec; 0.171 sec/batch)
2017-03-25 21:49:52.060560: step 23830, loss = 0.83 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:49:53.788178: step 23840, loss = 0.72 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:49:55.500398: step 23850, loss = 0.79 (747.6 examples/sec; 0.171 sec/batch)
2017-03-25 21:49:57.217855: step 23860, loss = 0.80 (745.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:49:58.942049: step 23870, loss = 0.91 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:50:00.678512: step 23880, loss = 0.69 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:02.407428: step 23890, loss = 0.65 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:50:04.297384: step 23900, loss = 0.70 (677.6 examples/sec; 0.189 sec/batch)
2017-03-25 21:50:05.923137: step 23910, loss = 0.75 (786.9 examples/sec; 0.163 sec/batch)
2017-03-25 21:50:07.650700: step 23920, loss = 0.78 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:50:09.394782: step 23930, loss = 0.71 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:11.133744: step 23940, loss = 0.77 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:12.869317: step 23950, loss = 0.94 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:14.607723: step 23960, loss = 0.71 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:16.339659: step 23970, loss = 0.90 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:50:18.068757: step 23980, loss = 0.89 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:50:19.805434: step 23990, loss = 0.94 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:21.698325: step 24000, loss = 0.93 (676.2 examples/sec; 0.189 sec/batch)
2017-03-25 21:50:23.309155: step 24010, loss = 0.83 (794.6 examples/sec; 0.161 sec/batch)
2017-03-25 21:50:25.056036: step 24020, loss = 0.75 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:50:26.791563: step 24030, loss = 0.87 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:28.534269: step 24040, loss = 0.72 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:30.272437: step 24050, loss = 0.85 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:32.015145: step 24060, loss = 0.75 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:33.753819: step 24070, loss = 0.84 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:35.502697: step 24080, loss = 0.96 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 21:50:37.249278: step 24090, loss = 1.00 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 21:50:39.140931: step 24100, loss = 0.76 (676.7 examples/sec; 0.189 sec/batch)
2017-03-25 21:50:40.761931: step 24110, loss = 0.75 (789.6 examples/sec; 0.162 sec/batch)
2017-03-25 21:50:42.499311: step 24120, loss = 0.86 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:44.224855: step 24130, loss = 0.77 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:50:45.965806: step 24140, loss = 0.87 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:47.691383: step 24150, loss = 0.86 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:50:49.426869: step 24160, loss = 0.96 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:51.158318: step 24170, loss = 0.79 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:50:52.919371: step 24180, loss = 0.79 (726.9 examples/sec; 0.176 sec/batch)
2017-03-25 21:50:54.659284: step 24190, loss = 0.71 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:50:56.568334: step 24200, loss = 0.73 (670.7 examples/sec; 0.191 sec/batch)
2017-03-25 21:50:58.178807: step 24210, loss = 0.91 (794.4 examples/sec; 0.161 sec/batch)
2017-03-25 21:50:59.908216: step 24220, loss = 0.86 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:51:01.656934: step 24230, loss = 0.73 (732.0 examples/sec; 0.175 sec/batch)
2017-03-25 21:51:03.386614: step 24240, loss = 0.79 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:51:05.127071: step 24250, loss = 0.88 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:51:06.863114: step 24260, loss = 0.89 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:51:08.585071: step 24270, loss = 0.72 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:51:10.321651: step 24280, loss = 0.83 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:51:12.066772: step 24290, loss = 0.78 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:51:14.026454: step 24300, loss = 0.97 (653.2 examples/sec; 0.196 sec/batch)
2017-03-25 21:51:15.580501: step 24310, loss = 0.79 (823.7 examples/sec; 0.155 sec/batch)
2017-03-25 21:51:17.316585: step 24320, loss = 0.85 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:51:19.041679: step 24330, loss = 0.72 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:51:20.778350: step 24340, loss = 0.89 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:51:22.513233: step 24350, loss = 1.12 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:51:24.239220: step 24360, loss = 0.98 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:51:25.980155: step 24370, loss = 0.78 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:51:27.723812: step 24380, loss = 0.76 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:51:29.459011: step 24390, loss = 0.86 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:51:31.349399: step 24400, loss = 0.88 (677.5 examples/sec; 0.189 sec/batch)
2017-03-25 21:51:32.957996: step 24410, loss = 0.65 (795.2 examples/sec; 0.161 sec/batch)
2017-03-25 21:51:34.704042: step 24420, loss = 0.87 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:51:36.442437: step 24430, loss = 0.78 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:51:38.180707: step 24440, loss = 0.82 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:51:39.906904: step 24450, loss = 0.85 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:51:41.645156: step 24460, loss = 0.79 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:51:43.382586: step 24470, loss = 0.72 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:51:45.113385: step 24480, loss = 0.78 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:51:46.845188: step 24490, loss = 0.78 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:51:48.822126: step 24500, loss = 0.68 (647.5 examples/sec; 0.198 sec/batch)
2017-03-25 21:51:50.349283: step 24510, loss = 0.82 (838.1 examples/sec; 0.153 sec/batch)
2017-03-25 21:51:52.053423: step 24520, loss = 0.86 (751.1 examples/sec; 0.170 sec/batch)
2017-03-25 21:51:53.788583: step 24530, loss = 1.00 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:51:55.530847: step 24540, loss = 0.81 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:51:57.275873: step 24550, loss = 0.64 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:51:59.001984: step 24560, loss = 0.64 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:52:00.745451: step 24570, loss = 0.76 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:52:02.491802: step 24580, loss = 0.92 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 21:52:04.222668: step 24590, loss = 0.75 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:52:06.194016: step 24600, loss = 0.71 (649.3 examples/sec; 0.197 sec/batch)
2017-03-25 21:52:07.740707: step 24610, loss = 0.71 (827.7 examples/sec; 0.155 sec/batch)
2017-03-25 21:52:09.471223: step 24620, loss = 1.02 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:52:11.224163: step 24630, loss = 0.90 (730.2 examples/sec; 0.175 sec/batch)
2017-03-25 21:52:12.945433: step 24640, loss = 0.75 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:52:14.687947: step 24650, loss = 0.89 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:52:16.424060: step 24660, loss = 0.83 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:52:18.158401: step 24670, loss = 0.91 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:52:19.881383: step 24680, loss = 0.80 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 21:52:21.611218: step 24690, loss = 0.93 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:52:23.541518: step 24700, loss = 0.66 (663.4 examples/sec; 0.193 sec/batch)
2017-03-25 21:52:25.117698: step 24710, loss = 0.66 (811.6 examples/sec; 0.158 sec/batch)
2017-03-25 21:52:26.854564: step 24720, loss = 0.66 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:52:28.573143: step 24730, loss = 0.66 (744.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:52:30.333450: step 24740, loss = 0.88 (727.1 examples/sec; 0.176 sec/batch)
2017-03-25 21:52:32.041222: step 24750, loss = 0.92 (749.7 examples/sec; 0.171 sec/batch)
2017-03-25 21:52:33.777809: step 24760, loss = 0.76 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:52:35.512984: step 24770, loss = 0.67 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:52:37.234177: step 24780, loss = 0.91 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:52:38.980016: step 24790, loss = 0.78 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 21:52:40.901711: step 24800, loss = 0.70 (666.3 examples/sec; 0.192 sec/batch)
2017-03-25 21:52:42.468661: step 24810, loss = 0.92 (816.6 examples/sec; 0.157 sec/batch)
2017-03-25 21:52:44.216083: step 24820, loss = 0.66 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:52:45.941753: step 24830, loss = 0.96 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:52:47.678822: step 24840, loss = 0.95 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:52:49.425283: step 24850, loss = 0.77 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 21:52:51.156136: step 24860, loss = 0.83 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:52:52.885831: step 24870, loss = 0.69 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:52:54.608415: step 24880, loss = 0.78 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:52:56.346641: step 24890, loss = 0.70 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:52:58.242377: step 24900, loss = 0.92 (675.3 examples/sec; 0.190 sec/batch)
2017-03-25 21:52:59.849612: step 24910, loss = 0.73 (796.3 examples/sec; 0.161 sec/batch)
2017-03-25 21:53:01.583834: step 24920, loss = 0.70 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:53:03.308728: step 24930, loss = 0.70 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:53:05.050611: step 24940, loss = 0.76 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:53:06.774251: step 24950, loss = 0.65 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:53:08.504216: step 24960, loss = 0.72 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:53:10.232524: step 24970, loss = 0.79 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:53:11.965790: step 24980, loss = 0.84 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:53:13.686868: step 24990, loss = 0.66 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:53:15.586117: step 25000, loss = 0.77 (674.0 examples/sec; 0.190 sec/batch)
2017-03-25 21:53:17.181125: step 25010, loss = 0.77 (802.5 examples/sec; 0.159 sec/batch)
2017-03-25 21:53:18.919913: step 25020, loss = 0.99 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:53:20.655921: step 25030, loss = 0.84 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:53:22.388064: step 25040, loss = 1.04 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:53:24.117370: step 25050, loss = 0.69 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:53:25.856825: step 25060, loss = 0.83 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:53:27.585443: step 25070, loss = 0.72 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:53:29.325044: step 25080, loss = 0.75 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:53:31.060786: step 25090, loss = 0.95 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:53:32.981223: step 25100, loss = 0.88 (666.8 examples/sec; 0.192 sec/batch)
2017-03-25 21:53:34.571347: step 25110, loss = 0.77 (804.5 examples/sec; 0.159 sec/batch)
2017-03-25 21:53:36.310368: step 25120, loss = 0.76 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:53:38.035018: step 25130, loss = 0.88 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:53:39.783530: step 25140, loss = 0.78 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 21:53:41.521525: step 25150, loss = 0.88 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:53:43.270226: step 25160, loss = 0.84 (732.0 examples/sec; 0.175 sec/batch)
2017-03-25 21:53:45.003926: step 25170, loss = 1.00 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:53:46.744719: step 25180, loss = 0.82 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:53:48.474218: step 25190, loss = 0.76 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:53:50.361275: step 25200, loss = 0.82 (678.3 examples/sec; 0.189 sec/batch)
2017-03-25 21:53:51.972673: step 25210, loss = 0.65 (794.3 examples/sec; 0.161 sec/batch)
2017-03-25 21:53:53.703342: step 25220, loss = 0.72 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:53:55.429205: step 25230, loss = 0.90 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:53:57.164561: step 25240, loss = 0.82 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:53:58.914140: step 25250, loss = 0.70 (731.6 examples/sec; 0.175 sec/batch)
2017-03-25 21:54:00.649254: step 25260, loss = 0.87 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:02.399280: step 25270, loss = 0.83 (731.4 examples/sec; 0.175 sec/batch)
2017-03-25 21:54:04.135512: step 25280, loss = 0.63 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:05.857604: step 25290, loss = 0.86 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:54:07.782112: step 25300, loss = 0.77 (665.4 examples/sec; 0.192 sec/batch)
2017-03-25 21:54:09.364129: step 25310, loss = 0.73 (808.7 examples/sec; 0.158 sec/batch)
2017-03-25 21:54:11.093678: step 25320, loss = 0.63 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 21:54:12.818910: step 25330, loss = 0.70 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:54:14.555287: step 25340, loss = 0.80 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:16.295689: step 25350, loss = 0.97 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:18.025813: step 25360, loss = 0.95 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:54:19.768677: step 25370, loss = 0.74 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:21.498831: step 25380, loss = 0.85 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:54:23.227666: step 25390, loss = 0.79 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:54:25.154976: step 25400, loss = 0.74 (664.1 examples/sec; 0.193 sec/batch)
2017-03-25 21:54:26.748872: step 25410, loss = 0.82 (803.1 examples/sec; 0.159 sec/batch)
2017-03-25 21:54:28.489386: step 25420, loss = 0.91 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:30.226656: step 25430, loss = 0.94 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:31.963782: step 25440, loss = 0.84 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:33.704883: step 25450, loss = 0.72 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:35.443564: step 25460, loss = 0.64 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:37.179178: step 25470, loss = 0.77 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:38.922103: step 25480, loss = 0.86 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:40.649482: step 25490, loss = 0.89 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:54:42.552898: step 25500, loss = 0.82 (672.5 examples/sec; 0.190 sec/batch)
2017-03-25 21:54:44.167973: step 25510, loss = 0.87 (792.5 examples/sec; 0.162 sec/batch)
2017-03-25 21:54:45.904991: step 25520, loss = 0.82 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:47.646465: step 25530, loss = 0.86 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:49.372055: step 25540, loss = 0.82 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:54:51.104200: step 25550, loss = 0.95 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:54:52.841718: step 25560, loss = 0.76 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:54.564681: step 25570, loss = 0.81 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 21:54:56.301563: step 25580, loss = 0.75 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:54:58.028299: step 25590, loss = 0.82 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:55:00.027307: step 25600, loss = 0.75 (640.3 examples/sec; 0.200 sec/batch)
2017-03-25 21:55:01.502146: step 25610, loss = 0.69 (867.9 examples/sec; 0.147 sec/batch)
2017-03-25 21:55:03.218820: step 25620, loss = 0.77 (745.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:55:04.947558: step 25630, loss = 0.79 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:55:06.690255: step 25640, loss = 0.84 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:55:08.432976: step 25650, loss = 0.81 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:55:10.182676: step 25660, loss = 0.75 (731.6 examples/sec; 0.175 sec/batch)
2017-03-25 21:55:11.928233: step 25670, loss = 0.86 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 21:55:13.675042: step 25680, loss = 0.69 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 21:55:15.406314: step 25690, loss = 0.72 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:55:17.376529: step 25700, loss = 0.71 (649.9 examples/sec; 0.197 sec/batch)
2017-03-25 21:55:18.865765: step 25710, loss = 0.83 (859.2 examples/sec; 0.149 sec/batch)
2017-03-25 21:55:20.568272: step 25720, loss = 0.67 (751.8 examples/sec; 0.170 sec/batch)
2017-03-25 21:55:22.246170: step 25730, loss = 0.99 (763.0 examples/sec; 0.168 sec/batch)
2017-03-25 21:55:23.941388: step 25740, loss = 0.71 (755.0 examples/sec; 0.170 sec/batch)
2017-03-25 21:55:25.631686: step 25750, loss = 0.68 (757.4 examples/sec; 0.169 sec/batch)
2017-03-25 21:55:27.307939: step 25760, loss = 0.75 (763.5 examples/sec; 0.168 sec/batch)
2017-03-25 21:55:28.994740: step 25770, loss = 0.60 (758.8 examples/sec; 0.169 sec/batch)
2017-03-25 21:55:30.684262: step 25780, loss = 0.83 (757.6 examples/sec; 0.169 sec/batch)
2017-03-25 21:55:32.372212: step 25790, loss = 0.81 (758.3 examples/sec; 0.169 sec/batch)
2017-03-25 21:55:34.228638: step 25800, loss = 0.80 (689.5 examples/sec; 0.186 sec/batch)
2017-03-25 21:55:35.782199: step 25810, loss = 0.74 (823.9 examples/sec; 0.155 sec/batch)
2017-03-25 21:55:37.468293: step 25820, loss = 0.74 (759.1 examples/sec; 0.169 sec/batch)
2017-03-25 21:55:39.147498: step 25830, loss = 0.88 (762.3 examples/sec; 0.168 sec/batch)
2017-03-25 21:55:40.828752: step 25840, loss = 0.86 (761.3 examples/sec; 0.168 sec/batch)
2017-03-25 21:55:42.508191: step 25850, loss = 0.85 (762.2 examples/sec; 0.168 sec/batch)
2017-03-25 21:55:44.187881: step 25860, loss = 0.84 (762.0 examples/sec; 0.168 sec/batch)
2017-03-25 21:55:45.877623: step 25870, loss = 0.72 (757.5 examples/sec; 0.169 sec/batch)
2017-03-25 21:55:47.559114: step 25880, loss = 0.63 (761.2 examples/sec; 0.168 sec/batch)
2017-03-25 21:55:49.238681: step 25890, loss = 0.87 (762.4 examples/sec; 0.168 sec/batch)
2017-03-25 21:55:51.122659: step 25900, loss = 0.93 (679.2 examples/sec; 0.188 sec/batch)
2017-03-25 21:55:52.730107: step 25910, loss = 0.77 (796.3 examples/sec; 0.161 sec/batch)
2017-03-25 21:55:54.465543: step 25920, loss = 0.79 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:55:56.205312: step 25930, loss = 0.78 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:55:57.940657: step 25940, loss = 0.72 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:55:59.682243: step 25950, loss = 0.71 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:56:01.428828: step 25960, loss = 0.77 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 21:56:03.183490: step 25970, loss = 0.75 (729.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:56:04.931185: step 25980, loss = 0.69 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 21:56:06.667588: step 25990, loss = 0.60 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:56:08.561538: step 26000, loss = 0.96 (675.8 examples/sec; 0.189 sec/batch)
2017-03-25 21:56:10.204262: step 26010, loss = 0.95 (779.2 examples/sec; 0.164 sec/batch)
2017-03-25 21:56:11.937878: step 26020, loss = 0.70 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:56:13.689434: step 26030, loss = 0.87 (730.9 examples/sec; 0.175 sec/batch)
2017-03-25 21:56:15.409187: step 26040, loss = 0.62 (744.3 examples/sec; 0.172 sec/batch)
2017-03-25 21:56:17.146866: step 26050, loss = 0.79 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:56:18.892025: step 26060, loss = 0.85 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:56:20.622910: step 26070, loss = 0.73 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:56:22.359988: step 26080, loss = 0.76 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:56:24.085893: step 26090, loss = 0.75 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:56:25.961985: step 26100, loss = 0.87 (682.3 examples/sec; 0.188 sec/batch)
2017-03-25 21:56:27.604324: step 26110, loss = 0.71 (779.4 examples/sec; 0.164 sec/batch)
2017-03-25 21:56:29.349084: step 26120, loss = 0.82 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:56:31.084757: step 26130, loss = 0.60 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:56:32.818361: step 26140, loss = 0.87 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:56:34.535007: step 26150, loss = 0.71 (745.6 examples/sec; 0.172 sec/batch)
2017-03-25 21:56:36.275923: step 26160, loss = 0.61 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:56:38.017646: step 26170, loss = 0.69 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 21:56:39.744355: step 26180, loss = 0.83 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:56:41.478675: step 26190, loss = 0.82 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:56:43.432467: step 26200, loss = 0.74 (655.1 examples/sec; 0.195 sec/batch)
2017-03-25 21:56:44.974244: step 26210, loss = 0.67 (830.2 examples/sec; 0.154 sec/batch)
2017-03-25 21:56:46.709114: step 26220, loss = 0.78 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:56:48.440650: step 26230, loss = 0.70 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:56:50.167129: step 26240, loss = 0.77 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:56:51.895561: step 26250, loss = 0.77 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:56:53.632164: step 26260, loss = 0.63 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:56:55.364925: step 26270, loss = 0.81 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:56:57.106924: step 26280, loss = 0.90 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:56:58.855870: step 26290, loss = 0.88 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 21:57:00.763757: step 26300, loss = 0.78 (670.9 examples/sec; 0.191 sec/batch)
2017-03-25 21:57:02.368417: step 26310, loss = 0.84 (797.7 examples/sec; 0.160 sec/batch)
2017-03-25 21:57:04.099178: step 26320, loss = 0.73 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:57:05.816747: step 26330, loss = 0.78 (745.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:57:07.544673: step 26340, loss = 0.76 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:57:09.271025: step 26350, loss = 0.88 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:57:10.999369: step 26360, loss = 0.78 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:57:12.739515: step 26370, loss = 0.86 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:57:14.467635: step 26380, loss = 0.82 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:57:16.208758: step 26390, loss = 0.74 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 21:57:18.102420: step 26400, loss = 0.82 (676.2 examples/sec; 0.189 sec/batch)
2017-03-25 21:57:19.709634: step 26410, loss = 0.80 (796.2 examples/sec; 0.161 sec/batch)
2017-03-25 21:57:21.441975: step 26420, loss = 0.79 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:57:23.164318: step 26430, loss = 0.86 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 21:57:24.887607: step 26440, loss = 0.74 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:57:26.617883: step 26450, loss = 0.92 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:57:28.355905: step 26460, loss = 0.94 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:57:30.089290: step 26470, loss = 0.77 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:57:31.817483: step 26480, loss = 0.79 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:57:33.558989: step 26490, loss = 0.64 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:57:35.469612: step 26500, loss = 0.80 (669.9 examples/sec; 0.191 sec/batch)
2017-03-25 21:57:37.071823: step 26510, loss = 0.86 (798.9 examples/sec; 0.160 sec/batch)
2017-03-25 21:57:38.806234: step 26520, loss = 0.77 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:57:40.544140: step 26530, loss = 0.85 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:57:42.272973: step 26540, loss = 0.68 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:57:44.009377: step 26550, loss = 0.77 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:57:45.736280: step 26560, loss = 0.78 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:57:47.469602: step 26570, loss = 0.83 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:57:49.205840: step 26580, loss = 0.64 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:57:50.948976: step 26590, loss = 0.78 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:57:52.836110: step 26600, loss = 0.86 (678.3 examples/sec; 0.189 sec/batch)
2017-03-25 21:57:54.458598: step 26610, loss = 0.81 (789.1 examples/sec; 0.162 sec/batch)
2017-03-25 21:57:56.172791: step 26620, loss = 0.77 (746.5 examples/sec; 0.171 sec/batch)
2017-03-25 21:57:57.907694: step 26630, loss = 0.81 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:57:59.643141: step 26640, loss = 0.82 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:58:01.381696: step 26650, loss = 0.85 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:58:03.120289: step 26660, loss = 0.73 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:58:04.844668: step 26670, loss = 0.82 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:58:06.577657: step 26680, loss = 0.87 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 21:58:08.305409: step 26690, loss = 0.74 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 21:58:10.276326: step 26700, loss = 0.83 (649.7 examples/sec; 0.197 sec/batch)
2017-03-25 21:58:11.810501: step 26710, loss = 0.69 (833.8 examples/sec; 0.154 sec/batch)
2017-03-25 21:58:13.539794: step 26720, loss = 0.77 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:58:15.277745: step 26730, loss = 0.76 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:58:17.013755: step 26740, loss = 0.91 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 21:58:18.738551: step 26750, loss = 0.77 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:58:20.463520: step 26760, loss = 0.81 (742.0 examples/sec; 0.172 sec/batch)
2017-03-25 21:58:22.192605: step 26770, loss = 0.83 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:58:23.915829: step 26780, loss = 0.96 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:58:25.651564: step 26790, loss = 0.80 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 21:58:27.549833: step 26800, loss = 0.84 (674.3 examples/sec; 0.190 sec/batch)
2017-03-25 21:58:29.161185: step 26810, loss = 0.69 (794.4 examples/sec; 0.161 sec/batch)
2017-03-25 21:58:30.898774: step 26820, loss = 0.99 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:58:32.620502: step 26830, loss = 0.78 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 21:58:34.350608: step 26840, loss = 0.95 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:58:36.095196: step 26850, loss = 0.81 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:58:37.826758: step 26860, loss = 0.66 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 21:58:39.560680: step 26870, loss = 0.80 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:58:41.308910: step 26880, loss = 0.68 (732.1 examples/sec; 0.175 sec/batch)
2017-03-25 21:58:43.031949: step 26890, loss = 0.76 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 21:58:44.929823: step 26900, loss = 0.70 (674.4 examples/sec; 0.190 sec/batch)
2017-03-25 21:58:46.545628: step 26910, loss = 0.81 (792.3 examples/sec; 0.162 sec/batch)
2017-03-25 21:58:48.283084: step 26920, loss = 0.68 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:58:50.017445: step 26930, loss = 0.89 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:58:51.745606: step 26940, loss = 0.79 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:58:53.481190: step 26950, loss = 0.70 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:58:55.221326: step 26960, loss = 0.79 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:58:56.956656: step 26970, loss = 0.69 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 21:58:58.684818: step 26980, loss = 0.75 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:59:00.418336: step 26990, loss = 0.86 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:59:02.324114: step 27000, loss = 0.84 (671.9 examples/sec; 0.191 sec/batch)
2017-03-25 21:59:03.920161: step 27010, loss = 0.74 (801.6 examples/sec; 0.160 sec/batch)
2017-03-25 21:59:05.645474: step 27020, loss = 0.98 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 21:59:07.384030: step 27030, loss = 0.81 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:59:09.105065: step 27040, loss = 0.76 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 21:59:10.841796: step 27050, loss = 0.83 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 21:59:12.573024: step 27060, loss = 0.86 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:59:14.293900: step 27070, loss = 0.75 (743.8 examples/sec; 0.172 sec/batch)
2017-03-25 21:59:16.033135: step 27080, loss = 0.86 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:59:17.776489: step 27090, loss = 0.77 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:59:19.666239: step 27100, loss = 0.95 (677.1 examples/sec; 0.189 sec/batch)
2017-03-25 21:59:21.292425: step 27110, loss = 0.65 (787.1 examples/sec; 0.163 sec/batch)
2017-03-25 21:59:23.030475: step 27120, loss = 0.88 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 21:59:24.765173: step 27130, loss = 0.71 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:59:26.498894: step 27140, loss = 0.95 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 21:59:28.245885: step 27150, loss = 0.74 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 21:59:29.969744: step 27160, loss = 0.77 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 21:59:31.707311: step 27170, loss = 0.74 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 21:59:33.438201: step 27180, loss = 0.71 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 21:59:35.163510: step 27190, loss = 0.74 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:59:37.078080: step 27200, loss = 0.66 (668.6 examples/sec; 0.191 sec/batch)
2017-03-25 21:59:38.676916: step 27210, loss = 0.90 (800.6 examples/sec; 0.160 sec/batch)
2017-03-25 21:59:40.407346: step 27220, loss = 0.78 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 21:59:42.148375: step 27230, loss = 0.71 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 21:59:43.877347: step 27240, loss = 0.80 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 21:59:45.623666: step 27250, loss = 0.71 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 21:59:47.351234: step 27260, loss = 0.77 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:59:49.073699: step 27270, loss = 0.83 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 21:59:50.808534: step 27280, loss = 0.84 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 21:59:52.552604: step 27290, loss = 0.80 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 21:59:54.447786: step 27300, loss = 0.80 (675.6 examples/sec; 0.189 sec/batch)
2017-03-25 21:59:56.069967: step 27310, loss = 0.74 (788.7 examples/sec; 0.162 sec/batch)
2017-03-25 21:59:57.822353: step 27320, loss = 0.79 (730.5 examples/sec; 0.175 sec/batch)
2017-03-25 21:59:59.554668: step 27330, loss = 0.71 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:00:01.307850: step 27340, loss = 0.72 (730.1 examples/sec; 0.175 sec/batch)
2017-03-25 22:00:03.044557: step 27350, loss = 0.86 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:00:04.784267: step 27360, loss = 0.75 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:00:06.525197: step 27370, loss = 0.72 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:00:08.255595: step 27380, loss = 0.80 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:00:09.993699: step 27390, loss = 0.62 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:00:11.965792: step 27400, loss = 0.69 (649.1 examples/sec; 0.197 sec/batch)
2017-03-25 22:00:13.490603: step 27410, loss = 0.82 (839.4 examples/sec; 0.152 sec/batch)
2017-03-25 22:00:15.217574: step 27420, loss = 0.60 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:00:16.946797: step 27430, loss = 0.82 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:00:18.687574: step 27440, loss = 0.82 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:00:20.417936: step 27450, loss = 0.84 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:00:22.144661: step 27460, loss = 0.85 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:00:23.892325: step 27470, loss = 0.84 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:00:25.619287: step 27480, loss = 0.74 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:00:27.365335: step 27490, loss = 0.74 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 22:00:29.272489: step 27500, loss = 0.75 (671.2 examples/sec; 0.191 sec/batch)
2017-03-25 22:00:30.872702: step 27510, loss = 0.72 (799.9 examples/sec; 0.160 sec/batch)
2017-03-25 22:00:32.606089: step 27520, loss = 0.81 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:00:34.330935: step 27530, loss = 0.76 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:00:36.076349: step 27540, loss = 0.72 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:00:37.830642: step 27550, loss = 0.82 (729.6 examples/sec; 0.175 sec/batch)
2017-03-25 22:00:39.551004: step 27560, loss = 0.68 (744.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:00:41.267897: step 27570, loss = 0.86 (745.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:00:43.008721: step 27580, loss = 0.80 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:00:44.752962: step 27590, loss = 0.78 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:00:46.670842: step 27600, loss = 0.62 (667.7 examples/sec; 0.192 sec/batch)
2017-03-25 22:00:48.261851: step 27610, loss = 0.68 (804.0 examples/sec; 0.159 sec/batch)
2017-03-25 22:00:49.998862: step 27620, loss = 0.96 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:00:51.727787: step 27630, loss = 0.76 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:00:53.487119: step 27640, loss = 0.72 (727.6 examples/sec; 0.176 sec/batch)
2017-03-25 22:00:55.228851: step 27650, loss = 0.84 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:00:56.946533: step 27660, loss = 0.82 (745.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:00:58.691303: step 27670, loss = 0.66 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:01:00.441390: step 27680, loss = 0.65 (731.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:01:02.181669: step 27690, loss = 0.73 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:01:04.075856: step 27700, loss = 0.75 (675.8 examples/sec; 0.189 sec/batch)
2017-03-25 22:01:05.683503: step 27710, loss = 0.78 (796.2 examples/sec; 0.161 sec/batch)
2017-03-25 22:01:07.419835: step 27720, loss = 0.80 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:01:09.156496: step 27730, loss = 0.76 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:01:10.895348: step 27740, loss = 0.74 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:01:12.618711: step 27750, loss = 0.64 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:01:14.348947: step 27760, loss = 0.76 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:01:16.095951: step 27770, loss = 0.75 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 22:01:17.832761: step 27780, loss = 0.92 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:01:19.578833: step 27790, loss = 0.67 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 22:01:21.473182: step 27800, loss = 1.05 (676.1 examples/sec; 0.189 sec/batch)
2017-03-25 22:01:23.081454: step 27810, loss = 0.85 (795.2 examples/sec; 0.161 sec/batch)
2017-03-25 22:01:24.809035: step 27820, loss = 0.59 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:01:26.538307: step 27830, loss = 0.70 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:01:28.271437: step 27840, loss = 0.99 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:01:30.001536: step 27850, loss = 0.83 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:01:31.734952: step 27860, loss = 0.70 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:01:33.466796: step 27870, loss = 0.88 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:01:35.201255: step 27880, loss = 0.90 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:01:36.940464: step 27890, loss = 0.71 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:01:38.903936: step 27900, loss = 0.70 (651.9 examples/sec; 0.196 sec/batch)
2017-03-25 22:01:40.413905: step 27910, loss = 0.73 (847.7 examples/sec; 0.151 sec/batch)
2017-03-25 22:01:42.124735: step 27920, loss = 0.84 (748.2 examples/sec; 0.171 sec/batch)
2017-03-25 22:01:43.858241: step 27930, loss = 0.91 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:01:45.591474: step 27940, loss = 0.76 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:01:47.333423: step 27950, loss = 0.70 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:01:49.062455: step 27960, loss = 0.82 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:01:50.799976: step 27970, loss = 0.79 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:01:52.529803: step 27980, loss = 0.75 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:01:54.264825: step 27990, loss = 1.00 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:01:56.147116: step 28000, loss = 0.68 (680.0 examples/sec; 0.188 sec/batch)
2017-03-25 22:01:57.767946: step 28010, loss = 0.88 (789.7 examples/sec; 0.162 sec/batch)
2017-03-25 22:01:59.514702: step 28020, loss = 0.78 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 22:02:01.251981: step 28030, loss = 0.87 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:02:02.987866: step 28040, loss = 1.02 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:02:04.729086: step 28050, loss = 0.79 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:02:06.466769: step 28060, loss = 0.73 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:02:08.204189: step 28070, loss = 0.81 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:02:09.932882: step 28080, loss = 0.73 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:02:11.651760: step 28090, loss = 0.70 (744.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:02:13.646449: step 28100, loss = 0.62 (641.7 examples/sec; 0.199 sec/batch)
2017-03-25 22:02:15.170739: step 28110, loss = 0.64 (839.7 examples/sec; 0.152 sec/batch)
2017-03-25 22:02:16.896410: step 28120, loss = 0.79 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:02:18.649474: step 28130, loss = 0.80 (730.2 examples/sec; 0.175 sec/batch)
2017-03-25 22:02:20.367284: step 28140, loss = 0.87 (745.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:02:22.101175: step 28150, loss = 0.78 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:02:23.819391: step 28160, loss = 0.88 (745.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:02:25.559760: step 28170, loss = 0.73 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:02:27.281087: step 28180, loss = 0.82 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:02:29.011433: step 28190, loss = 0.95 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:02:30.899442: step 28200, loss = 0.77 (678.0 examples/sec; 0.189 sec/batch)
2017-03-25 22:02:32.518686: step 28210, loss = 0.72 (790.5 examples/sec; 0.162 sec/batch)
2017-03-25 22:02:34.277192: step 28220, loss = 0.78 (727.9 examples/sec; 0.176 sec/batch)
2017-03-25 22:02:35.990691: step 28230, loss = 0.75 (747.0 examples/sec; 0.171 sec/batch)
2017-03-25 22:02:37.722453: step 28240, loss = 0.78 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:02:39.442645: step 28250, loss = 0.79 (744.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:02:41.186790: step 28260, loss = 0.78 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:02:42.909574: step 28270, loss = 0.66 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:02:44.646545: step 28280, loss = 0.94 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:02:46.379760: step 28290, loss = 0.92 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:02:48.321340: step 28300, loss = 0.82 (659.3 examples/sec; 0.194 sec/batch)
2017-03-25 22:02:49.876892: step 28310, loss = 0.81 (822.9 examples/sec; 0.156 sec/batch)
2017-03-25 22:02:51.614058: step 28320, loss = 0.86 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:02:53.352965: step 28330, loss = 0.81 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:02:55.086228: step 28340, loss = 0.80 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:02:56.813942: step 28350, loss = 0.84 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:02:58.550609: step 28360, loss = 0.73 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:03:00.296641: step 28370, loss = 0.73 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 22:03:02.039573: step 28380, loss = 0.92 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:03:03.775874: step 28390, loss = 0.73 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:03:05.789090: step 28400, loss = 0.76 (635.8 examples/sec; 0.201 sec/batch)
2017-03-25 22:03:07.303335: step 28410, loss = 0.74 (845.3 examples/sec; 0.151 sec/batch)
2017-03-25 22:03:09.028770: step 28420, loss = 0.87 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:03:10.764978: step 28430, loss = 0.83 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:03:12.504589: step 28440, loss = 0.71 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:03:14.231993: step 28450, loss = 0.89 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:03:15.964150: step 28460, loss = 0.88 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:03:17.701666: step 28470, loss = 0.77 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:03:19.434942: step 28480, loss = 0.92 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:03:21.178462: step 28490, loss = 0.66 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:03:23.087313: step 28500, loss = 0.72 (670.7 examples/sec; 0.191 sec/batch)
2017-03-25 22:03:24.684117: step 28510, loss = 0.76 (801.3 examples/sec; 0.160 sec/batch)
2017-03-25 22:03:26.419584: step 28520, loss = 0.75 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:03:28.138941: step 28530, loss = 0.89 (744.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:03:29.872816: step 28540, loss = 0.72 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:03:31.617087: step 28550, loss = 0.88 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:03:33.353229: step 28560, loss = 0.65 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:03:35.077179: step 28570, loss = 0.71 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:03:36.815290: step 28580, loss = 0.98 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:03:38.544707: step 28590, loss = 0.81 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:03:40.506413: step 28600, loss = 0.89 (652.5 examples/sec; 0.196 sec/batch)
2017-03-25 22:03:41.999002: step 28610, loss = 0.80 (857.8 examples/sec; 0.149 sec/batch)
2017-03-25 22:03:43.703095: step 28620, loss = 0.67 (751.1 examples/sec; 0.170 sec/batch)
2017-03-25 22:03:45.427221: step 28630, loss = 0.76 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 22:03:47.158660: step 28640, loss = 0.81 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:03:48.876054: step 28650, loss = 0.83 (745.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:03:50.619015: step 28660, loss = 0.70 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:03:52.352024: step 28670, loss = 0.80 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:03:54.091476: step 28680, loss = 0.81 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:03:55.825451: step 28690, loss = 0.66 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:03:57.717759: step 28700, loss = 0.81 (677.0 examples/sec; 0.189 sec/batch)
2017-03-25 22:03:59.345789: step 28710, loss = 0.77 (785.4 examples/sec; 0.163 sec/batch)
2017-03-25 22:04:01.076020: step 28720, loss = 0.73 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:04:02.806517: step 28730, loss = 0.65 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:04:04.543867: step 28740, loss = 0.75 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:04:06.284222: step 28750, loss = 0.83 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:04:08.021282: step 28760, loss = 0.79 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:04:09.739533: step 28770, loss = 0.73 (744.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:04:11.479165: step 28780, loss = 0.72 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:04:13.206807: step 28790, loss = 0.95 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:04:15.106827: step 28800, loss = 0.90 (673.9 examples/sec; 0.190 sec/batch)
2017-03-25 22:04:16.714722: step 28810, loss = 0.91 (795.7 examples/sec; 0.161 sec/batch)
2017-03-25 22:04:18.440467: step 28820, loss = 0.81 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:04:20.179223: step 28830, loss = 0.76 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:04:21.895882: step 28840, loss = 0.69 (745.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:04:23.625359: step 28850, loss = 0.86 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:04:25.360813: step 28860, loss = 0.75 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:04:27.093552: step 28870, loss = 0.73 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:04:28.837054: step 28880, loss = 0.64 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:04:30.548897: step 28890, loss = 0.70 (747.7 examples/sec; 0.171 sec/batch)
2017-03-25 22:04:32.483129: step 28900, loss = 0.76 (662.1 examples/sec; 0.193 sec/batch)
2017-03-25 22:04:34.070635: step 28910, loss = 0.78 (805.7 examples/sec; 0.159 sec/batch)
2017-03-25 22:04:35.810033: step 28920, loss = 0.80 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:04:37.531818: step 28930, loss = 0.84 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:04:39.275319: step 28940, loss = 0.64 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:04:41.006741: step 28950, loss = 0.72 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:04:42.742006: step 28960, loss = 0.60 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:04:44.470847: step 28970, loss = 0.88 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:04:46.198033: step 28980, loss = 0.82 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:04:47.930903: step 28990, loss = 0.77 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:04:49.822385: step 29000, loss = 0.71 (677.0 examples/sec; 0.189 sec/batch)
2017-03-25 22:04:51.434926: step 29010, loss = 0.79 (793.3 examples/sec; 0.161 sec/batch)
2017-03-25 22:04:53.171417: step 29020, loss = 0.87 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:04:54.901679: step 29030, loss = 0.72 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:04:56.641347: step 29040, loss = 0.81 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:04:58.385025: step 29050, loss = 0.74 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:05:00.133363: step 29060, loss = 0.94 (732.1 examples/sec; 0.175 sec/batch)
2017-03-25 22:05:01.879294: step 29070, loss = 0.67 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 22:05:03.626040: step 29080, loss = 0.82 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 22:05:05.354480: step 29090, loss = 0.85 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:05:07.306719: step 29100, loss = 0.87 (655.7 examples/sec; 0.195 sec/batch)
2017-03-25 22:05:08.887785: step 29110, loss = 0.84 (809.6 examples/sec; 0.158 sec/batch)
2017-03-25 22:05:10.632003: step 29120, loss = 0.79 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:05:12.374854: step 29130, loss = 0.81 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:05:14.123452: step 29140, loss = 0.76 (732.0 examples/sec; 0.175 sec/batch)
2017-03-25 22:05:15.871384: step 29150, loss = 0.73 (732.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:05:17.602825: step 29160, loss = 0.90 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:05:19.328225: step 29170, loss = 0.74 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:05:21.062973: step 29180, loss = 0.75 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:05:22.788367: step 29190, loss = 0.69 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:05:24.698605: step 29200, loss = 0.76 (670.1 examples/sec; 0.191 sec/batch)
2017-03-25 22:05:26.284159: step 29210, loss = 0.86 (807.3 examples/sec; 0.159 sec/batch)
2017-03-25 22:05:28.028118: step 29220, loss = 0.76 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:05:29.755411: step 29230, loss = 0.82 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:05:31.492049: step 29240, loss = 0.82 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:05:33.236255: step 29250, loss = 0.72 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:05:34.969700: step 29260, loss = 0.79 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:05:36.702018: step 29270, loss = 0.92 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:05:38.448964: step 29280, loss = 0.73 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 22:05:40.177229: step 29290, loss = 0.81 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:05:42.072702: step 29300, loss = 1.08 (675.2 examples/sec; 0.190 sec/batch)
2017-03-25 22:05:43.681490: step 29310, loss = 0.76 (795.6 examples/sec; 0.161 sec/batch)
2017-03-25 22:05:45.409030: step 29320, loss = 0.84 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:05:47.140040: step 29330, loss = 0.67 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:05:48.878347: step 29340, loss = 0.74 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:05:50.601851: step 29350, loss = 0.72 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:05:52.321123: step 29360, loss = 0.79 (744.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:05:54.057023: step 29370, loss = 0.84 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:05:55.790089: step 29380, loss = 0.85 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:05:57.524669: step 29390, loss = 0.66 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:05:59.427146: step 29400, loss = 0.77 (672.8 examples/sec; 0.190 sec/batch)
2017-03-25 22:06:01.048604: step 29410, loss = 0.63 (789.4 examples/sec; 0.162 sec/batch)
2017-03-25 22:06:02.781680: step 29420, loss = 0.85 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:04.520347: step 29430, loss = 1.02 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:06:06.255145: step 29440, loss = 0.82 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:07.988886: step 29450, loss = 0.81 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:09.717209: step 29460, loss = 0.78 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:11.443329: step 29470, loss = 0.91 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:13.187828: step 29480, loss = 0.71 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:06:14.920790: step 29490, loss = 0.76 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:16.819872: step 29500, loss = 0.88 (674.0 examples/sec; 0.190 sec/batch)
2017-03-25 22:06:18.445276: step 29510, loss = 0.90 (787.5 examples/sec; 0.163 sec/batch)
2017-03-25 22:06:20.170591: step 29520, loss = 0.83 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:21.899395: step 29530, loss = 0.71 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:23.639181: step 29540, loss = 0.62 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:06:25.368485: step 29550, loss = 0.74 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:27.094361: step 29560, loss = 0.83 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:28.832523: step 29570, loss = 0.92 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:06:30.563659: step 29580, loss = 0.72 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:32.296403: step 29590, loss = 0.74 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:34.210437: step 29600, loss = 0.76 (669.1 examples/sec; 0.191 sec/batch)
2017-03-25 22:06:35.799627: step 29610, loss = 0.98 (804.9 examples/sec; 0.159 sec/batch)
2017-03-25 22:06:37.530893: step 29620, loss = 0.90 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:39.267878: step 29630, loss = 0.64 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:06:41.001963: step 29640, loss = 0.78 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:42.732876: step 29650, loss = 0.87 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:44.475583: step 29660, loss = 0.88 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:06:46.213055: step 29670, loss = 0.77 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:06:47.957350: step 29680, loss = 0.67 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:06:49.684226: step 29690, loss = 0.72 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:06:51.634747: step 29700, loss = 0.75 (656.5 examples/sec; 0.195 sec/batch)
2017-03-25 22:06:53.162576: step 29710, loss = 0.74 (837.4 examples/sec; 0.153 sec/batch)
2017-03-25 22:06:54.865031: step 29720, loss = 0.74 (751.9 examples/sec; 0.170 sec/batch)
2017-03-25 22:06:56.608293: step 29730, loss = 0.76 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:06:58.338500: step 29740, loss = 0.87 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:07:00.072795: step 29750, loss = 0.79 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:07:01.817116: step 29760, loss = 0.75 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:07:03.565767: step 29770, loss = 0.74 (732.0 examples/sec; 0.175 sec/batch)
2017-03-25 22:07:05.278871: step 29780, loss = 0.86 (747.2 examples/sec; 0.171 sec/batch)
2017-03-25 22:07:07.010316: step 29790, loss = 0.89 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:07:08.905971: step 29800, loss = 0.84 (675.2 examples/sec; 0.190 sec/batch)
2017-03-25 22:07:10.526563: step 29810, loss = 0.67 (789.8 examples/sec; 0.162 sec/batch)
2017-03-25 22:07:12.254600: step 29820, loss = 0.81 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:07:13.984955: step 29830, loss = 0.93 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:07:15.712149: step 29840, loss = 0.74 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:07:17.442307: step 29850, loss = 0.78 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:07:19.187887: step 29860, loss = 0.73 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 22:07:20.918234: step 29870, loss = 0.82 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:07:22.653395: step 29880, loss = 0.68 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:07:24.381303: step 29890, loss = 0.85 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:07:26.271508: step 29900, loss = 0.89 (677.2 examples/sec; 0.189 sec/batch)
2017-03-25 22:07:27.890829: step 29910, loss = 0.66 (790.5 examples/sec; 0.162 sec/batch)
2017-03-25 22:07:29.621140: step 29920, loss = 0.70 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:07:31.360254: step 29930, loss = 0.75 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:07:33.093813: step 29940, loss = 0.69 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:07:34.832630: step 29950, loss = 0.74 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:07:36.554035: step 29960, loss = 0.78 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:07:38.280181: step 29970, loss = 0.75 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:07:40.017587: step 29980, loss = 0.75 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:07:41.741113: step 29990, loss = 0.73 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:07:43.696320: step 30000, loss = 0.64 (655.1 examples/sec; 0.195 sec/batch)
2017-03-25 22:07:45.230218: step 30010, loss = 0.70 (833.7 examples/sec; 0.154 sec/batch)
2017-03-25 22:07:46.950616: step 30020, loss = 0.83 (744.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:07:48.696729: step 30030, loss = 0.64 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 22:07:50.431791: step 30040, loss = 0.97 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:07:52.170066: step 30050, loss = 0.71 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:07:53.906067: step 30060, loss = 0.83 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:07:55.634468: step 30070, loss = 0.72 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:07:57.372827: step 30080, loss = 0.87 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:07:59.103502: step 30090, loss = 0.75 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:08:01.006116: step 30100, loss = 0.71 (672.8 examples/sec; 0.190 sec/batch)
2017-03-25 22:08:02.605103: step 30110, loss = 0.78 (800.5 examples/sec; 0.160 sec/batch)
2017-03-25 22:08:04.336076: step 30120, loss = 0.74 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:08:06.069766: step 30130, loss = 0.79 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:08:07.805964: step 30140, loss = 0.81 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:08:09.542188: step 30150, loss = 0.68 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:08:11.275618: step 30160, loss = 0.79 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:08:13.019498: step 30170, loss = 0.75 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:08:14.742027: step 30180, loss = 0.68 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:08:16.468133: step 30190, loss = 0.65 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:08:18.360476: step 30200, loss = 0.77 (676.4 examples/sec; 0.189 sec/batch)
2017-03-25 22:08:19.980616: step 30210, loss = 0.58 (790.1 examples/sec; 0.162 sec/batch)
2017-03-25 22:08:21.714074: step 30220, loss = 1.03 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:08:23.439232: step 30230, loss = 0.70 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:08:25.171139: step 30240, loss = 0.90 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:08:26.903933: step 30250, loss = 0.70 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:08:28.640215: step 30260, loss = 0.66 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:08:30.364388: step 30270, loss = 0.81 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:08:32.107602: step 30280, loss = 0.78 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:08:33.825675: step 30290, loss = 0.78 (745.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:08:35.703086: step 30300, loss = 0.60 (681.8 examples/sec; 0.188 sec/batch)
2017-03-25 22:08:37.327107: step 30310, loss = 0.72 (788.2 examples/sec; 0.162 sec/batch)
2017-03-25 22:08:39.052251: step 30320, loss = 0.80 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:08:40.781050: step 30330, loss = 0.79 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:08:42.511281: step 30340, loss = 0.82 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:08:44.249861: step 30350, loss = 0.83 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:08:45.985047: step 30360, loss = 0.67 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:08:47.722227: step 30370, loss = 0.79 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:08:49.461004: step 30380, loss = 0.80 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:08:51.196137: step 30390, loss = 0.99 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:08:53.165262: step 30400, loss = 0.68 (650.0 examples/sec; 0.197 sec/batch)
2017-03-25 22:08:54.705145: step 30410, loss = 0.84 (831.2 examples/sec; 0.154 sec/batch)
2017-03-25 22:08:56.435402: step 30420, loss = 0.81 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:08:58.166490: step 30430, loss = 0.82 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:08:59.904843: step 30440, loss = 0.81 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:09:01.636315: step 30450, loss = 0.63 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:09:03.376125: step 30460, loss = 0.74 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:09:05.100971: step 30470, loss = 0.89 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:09:06.830128: step 30480, loss = 0.70 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:09:08.566236: step 30490, loss = 0.93 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:09:10.462424: step 30500, loss = 0.90 (675.1 examples/sec; 0.190 sec/batch)
2017-03-25 22:09:12.082284: step 30510, loss = 0.86 (790.1 examples/sec; 0.162 sec/batch)
2017-03-25 22:09:13.796736: step 30520, loss = 0.83 (746.6 examples/sec; 0.171 sec/batch)
2017-03-25 22:09:15.533147: step 30530, loss = 0.71 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:09:17.257672: step 30540, loss = 0.75 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:09:18.991476: step 30550, loss = 0.70 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:09:20.730084: step 30560, loss = 0.64 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:09:22.457440: step 30570, loss = 0.66 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:09:24.183526: step 30580, loss = 0.73 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:09:25.909925: step 30590, loss = 0.81 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:09:27.819849: step 30600, loss = 0.70 (670.2 examples/sec; 0.191 sec/batch)
2017-03-25 22:09:29.404729: step 30610, loss = 0.63 (807.9 examples/sec; 0.158 sec/batch)
2017-03-25 22:09:31.147688: step 30620, loss = 0.77 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:09:32.882104: step 30630, loss = 0.87 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:09:34.611024: step 30640, loss = 0.79 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:09:36.341703: step 30650, loss = 0.80 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:09:38.078978: step 30660, loss = 0.73 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:09:39.817500: step 30670, loss = 0.83 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:09:41.556595: step 30680, loss = 0.92 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:09:43.282854: step 30690, loss = 0.91 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:09:45.168639: step 30700, loss = 0.80 (679.1 examples/sec; 0.188 sec/batch)
2017-03-25 22:09:46.769995: step 30710, loss = 0.77 (798.8 examples/sec; 0.160 sec/batch)
2017-03-25 22:09:48.505804: step 30720, loss = 0.91 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:09:50.231616: step 30730, loss = 0.75 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:09:51.963214: step 30740, loss = 0.80 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:09:53.691744: step 30750, loss = 0.90 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:09:55.427065: step 30760, loss = 0.71 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:09:57.148400: step 30770, loss = 0.76 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:09:58.873346: step 30780, loss = 0.69 (742.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:10:00.609647: step 30790, loss = 0.79 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:10:02.528080: step 30800, loss = 0.81 (667.4 examples/sec; 0.192 sec/batch)
2017-03-25 22:10:04.111008: step 30810, loss = 0.76 (808.3 examples/sec; 0.158 sec/batch)
2017-03-25 22:10:05.858772: step 30820, loss = 0.71 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:10:07.586582: step 30830, loss = 0.60 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:10:09.317338: step 30840, loss = 0.75 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:10:11.053949: step 30850, loss = 0.77 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:10:12.774703: step 30860, loss = 0.77 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:10:14.516169: step 30870, loss = 0.83 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:10:16.242415: step 30880, loss = 0.85 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:10:17.985087: step 30890, loss = 0.73 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:10:19.886007: step 30900, loss = 0.94 (673.4 examples/sec; 0.190 sec/batch)
2017-03-25 22:10:21.482928: step 30910, loss = 0.68 (801.5 examples/sec; 0.160 sec/batch)
2017-03-25 22:10:23.225824: step 30920, loss = 0.75 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:10:24.957960: step 30930, loss = 0.68 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:10:26.690504: step 30940, loss = 0.75 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:10:28.411996: step 30950, loss = 0.61 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:10:30.136824: step 30960, loss = 0.81 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:10:31.882390: step 30970, loss = 0.82 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:10:33.600039: step 30980, loss = 0.97 (745.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:10:35.343293: step 30990, loss = 0.80 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:10:37.234666: step 31000, loss = 0.83 (677.0 examples/sec; 0.189 sec/batch)
2017-03-25 22:10:38.860019: step 31010, loss = 0.84 (787.2 examples/sec; 0.163 sec/batch)
2017-03-25 22:10:40.596332: step 31020, loss = 0.69 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:10:42.332509: step 31030, loss = 0.69 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:10:44.077606: step 31040, loss = 0.85 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:10:45.819684: step 31050, loss = 0.87 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:10:47.568424: step 31060, loss = 0.64 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:10:49.308879: step 31070, loss = 0.69 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:10:51.056330: step 31080, loss = 0.69 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:10:52.887679: step 31090, loss = 0.76 (699.0 examples/sec; 0.183 sec/batch)
2017-03-25 22:10:54.778957: step 31100, loss = 0.82 (677.0 examples/sec; 0.189 sec/batch)
2017-03-25 22:10:56.426356: step 31110, loss = 0.78 (776.7 examples/sec; 0.165 sec/batch)
2017-03-25 22:10:58.161214: step 31120, loss = 0.69 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:10:59.902708: step 31130, loss = 0.86 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:11:01.649028: step 31140, loss = 0.70 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:11:03.386395: step 31150, loss = 0.70 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:11:05.120958: step 31160, loss = 0.82 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:11:06.869125: step 31170, loss = 0.82 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 22:11:08.591484: step 31180, loss = 0.72 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 22:11:10.334890: step 31190, loss = 0.91 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:11:12.241159: step 31200, loss = 0.95 (671.7 examples/sec; 0.191 sec/batch)
2017-03-25 22:11:13.864209: step 31210, loss = 0.80 (788.3 examples/sec; 0.162 sec/batch)
2017-03-25 22:11:15.588507: step 31220, loss = 0.78 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 22:11:17.324414: step 31230, loss = 0.85 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:11:19.073932: step 31240, loss = 0.85 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:11:20.816257: step 31250, loss = 0.82 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:11:22.548700: step 31260, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:11:24.293483: step 31270, loss = 0.83 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:11:26.030706: step 31280, loss = 0.75 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:11:27.761635: step 31290, loss = 0.88 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:11:29.664357: step 31300, loss = 0.69 (672.7 examples/sec; 0.190 sec/batch)
2017-03-25 22:11:31.268063: step 31310, loss = 0.85 (798.4 examples/sec; 0.160 sec/batch)
2017-03-25 22:11:33.002435: step 31320, loss = 0.77 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:11:34.727932: step 31330, loss = 0.71 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:11:36.462712: step 31340, loss = 0.81 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:11:38.209097: step 31350, loss = 0.79 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:11:39.927309: step 31360, loss = 0.88 (745.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:11:41.651701: step 31370, loss = 0.89 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 22:11:43.379149: step 31380, loss = 0.81 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:11:45.103818: step 31390, loss = 0.83 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:11:47.073689: step 31400, loss = 0.65 (649.8 examples/sec; 0.197 sec/batch)
2017-03-25 22:11:48.602226: step 31410, loss = 0.78 (837.4 examples/sec; 0.153 sec/batch)
2017-03-25 22:11:50.327701: step 31420, loss = 0.85 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:11:52.068016: step 31430, loss = 0.77 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:11:53.808928: step 31440, loss = 0.82 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:11:55.547822: step 31450, loss = 0.69 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:11:57.275195: step 31460, loss = 0.77 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:11:59.002797: step 31470, loss = 0.74 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:12:00.741181: step 31480, loss = 0.76 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:12:02.479319: step 31490, loss = 0.68 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:12:04.435491: step 31500, loss = 0.75 (654.3 examples/sec; 0.196 sec/batch)
2017-03-25 22:12:06.016965: step 31510, loss = 0.89 (809.4 examples/sec; 0.158 sec/batch)
2017-03-25 22:12:07.742610: step 31520, loss = 0.73 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:12:09.481379: step 31530, loss = 0.74 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:12:11.197480: step 31540, loss = 0.82 (745.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:12:12.925207: step 31550, loss = 0.69 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:12:14.652458: step 31560, loss = 0.79 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:12:16.396353: step 31570, loss = 0.76 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:12:18.122850: step 31580, loss = 0.76 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:12:19.854904: step 31590, loss = 0.88 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:12:21.734532: step 31600, loss = 0.82 (681.0 examples/sec; 0.188 sec/batch)
2017-03-25 22:12:23.358069: step 31610, loss = 0.83 (788.5 examples/sec; 0.162 sec/batch)
2017-03-25 22:12:25.115460: step 31620, loss = 0.84 (728.3 examples/sec; 0.176 sec/batch)
2017-03-25 22:12:26.843536: step 31630, loss = 0.81 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:12:28.578778: step 31640, loss = 0.62 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:12:30.317134: step 31650, loss = 0.70 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:12:32.046568: step 31660, loss = 0.76 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:12:33.786346: step 31670, loss = 0.82 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:12:35.523639: step 31680, loss = 0.71 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:12:37.248177: step 31690, loss = 0.71 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:12:39.163219: step 31700, loss = 0.86 (668.4 examples/sec; 0.192 sec/batch)
2017-03-25 22:12:40.756877: step 31710, loss = 0.75 (803.2 examples/sec; 0.159 sec/batch)
2017-03-25 22:12:42.484665: step 31720, loss = 0.63 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:12:44.229738: step 31730, loss = 0.77 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:12:45.972706: step 31740, loss = 0.80 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:12:47.711363: step 31750, loss = 0.79 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:12:49.444868: step 31760, loss = 0.82 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:12:51.173157: step 31770, loss = 0.74 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:12:52.902982: step 31780, loss = 0.67 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:12:54.630668: step 31790, loss = 0.89 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:12:56.543334: step 31800, loss = 0.69 (669.2 examples/sec; 0.191 sec/batch)
2017-03-25 22:12:58.145682: step 31810, loss = 0.73 (798.8 examples/sec; 0.160 sec/batch)
2017-03-25 22:12:59.882177: step 31820, loss = 0.82 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:13:01.617914: step 31830, loss = 0.79 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:13:03.359004: step 31840, loss = 0.76 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:13:05.085340: step 31850, loss = 0.75 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:13:06.808105: step 31860, loss = 0.86 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:13:08.547461: step 31870, loss = 0.65 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:13:10.289275: step 31880, loss = 0.76 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:13:12.035216: step 31890, loss = 0.67 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 22:13:13.927091: step 31900, loss = 0.59 (676.6 examples/sec; 0.189 sec/batch)
2017-03-25 22:13:15.563352: step 31910, loss = 0.77 (782.4 examples/sec; 0.164 sec/batch)
2017-03-25 22:13:17.293826: step 31920, loss = 0.72 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:13:19.024022: step 31930, loss = 0.63 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:13:20.742795: step 31940, loss = 0.67 (744.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:13:22.474726: step 31950, loss = 0.93 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:13:24.189994: step 31960, loss = 0.72 (746.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:13:25.929752: step 31970, loss = 0.72 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:13:27.660761: step 31980, loss = 1.03 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:13:29.409931: step 31990, loss = 0.67 (731.8 examples/sec; 0.175 sec/batch)
2017-03-25 22:13:31.303745: step 32000, loss = 0.80 (675.9 examples/sec; 0.189 sec/batch)
2017-03-25 22:13:32.919363: step 32010, loss = 0.69 (792.3 examples/sec; 0.162 sec/batch)
2017-03-25 22:13:34.667327: step 32020, loss = 0.82 (732.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:13:36.395086: step 32030, loss = 0.79 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:13:38.134163: step 32040, loss = 0.74 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:13:39.872370: step 32050, loss = 0.77 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:13:41.597243: step 32060, loss = 0.79 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:13:43.326360: step 32070, loss = 0.77 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:13:45.062982: step 32080, loss = 0.72 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:13:46.794627: step 32090, loss = 0.63 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:13:48.717566: step 32100, loss = 0.87 (665.9 examples/sec; 0.192 sec/batch)
2017-03-25 22:13:50.307893: step 32110, loss = 0.63 (804.5 examples/sec; 0.159 sec/batch)
2017-03-25 22:13:52.034869: step 32120, loss = 0.78 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:13:53.772359: step 32130, loss = 0.68 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:13:55.515836: step 32140, loss = 0.85 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:13:57.236797: step 32150, loss = 0.82 (743.8 examples/sec; 0.172 sec/batch)
2017-03-25 22:13:58.969520: step 32160, loss = 0.81 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:14:00.708905: step 32170, loss = 0.76 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:14:02.442539: step 32180, loss = 0.99 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:14:04.192796: step 32190, loss = 0.75 (731.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:14:06.137520: step 32200, loss = 0.92 (658.5 examples/sec; 0.194 sec/batch)
2017-03-25 22:14:07.699448: step 32210, loss = 0.85 (819.2 examples/sec; 0.156 sec/batch)
2017-03-25 22:14:09.439816: step 32220, loss = 0.82 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:14:11.172063: step 32230, loss = 0.80 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:14:12.907301: step 32240, loss = 0.92 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:14:14.625004: step 32250, loss = 0.85 (745.3 examples/sec; 0.172 sec/batch)
2017-03-25 22:14:16.359916: step 32260, loss = 0.67 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:14:18.095338: step 32270, loss = 0.86 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:14:19.832022: step 32280, loss = 0.93 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:14:21.572923: step 32290, loss = 0.86 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:14:23.465462: step 32300, loss = 0.65 (676.3 examples/sec; 0.189 sec/batch)
2017-03-25 22:14:25.056478: step 32310, loss = 0.68 (804.6 examples/sec; 0.159 sec/batch)
2017-03-25 22:14:26.783997: step 32320, loss = 0.79 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:14:28.523019: step 32330, loss = 0.76 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:14:30.266025: step 32340, loss = 0.63 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:14:31.987265: step 32350, loss = 0.67 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:14:33.725553: step 32360, loss = 0.85 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:14:35.476969: step 32370, loss = 0.88 (730.8 examples/sec; 0.175 sec/batch)
2017-03-25 22:14:37.196531: step 32380, loss = 0.75 (744.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:14:38.926323: step 32390, loss = 0.70 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:14:40.806484: step 32400, loss = 0.86 (680.8 examples/sec; 0.188 sec/batch)
2017-03-25 22:14:42.425096: step 32410, loss = 0.84 (790.8 examples/sec; 0.162 sec/batch)
2017-03-25 22:14:44.159866: step 32420, loss = 0.58 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:14:45.881061: step 32430, loss = 0.77 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:14:47.604308: step 32440, loss = 0.76 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 22:14:49.338064: step 32450, loss = 0.78 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:14:51.061082: step 32460, loss = 0.64 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:14:52.800377: step 32470, loss = 0.77 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:14:54.533975: step 32480, loss = 0.88 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:14:56.275855: step 32490, loss = 0.86 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:14:58.264532: step 32500, loss = 0.72 (643.7 examples/sec; 0.199 sec/batch)
2017-03-25 22:14:59.797773: step 32510, loss = 0.65 (834.8 examples/sec; 0.153 sec/batch)
2017-03-25 22:15:01.542864: step 32520, loss = 0.83 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:15:03.277852: step 32530, loss = 0.70 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:15:05.021412: step 32540, loss = 0.87 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:15:06.736872: step 32550, loss = 0.90 (746.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:15:08.473175: step 32560, loss = 0.72 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:15:10.214033: step 32570, loss = 0.78 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:15:11.945625: step 32580, loss = 0.96 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:15:13.673676: step 32590, loss = 0.80 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:15:15.632219: step 32600, loss = 0.77 (654.0 examples/sec; 0.196 sec/batch)
2017-03-25 22:15:17.121898: step 32610, loss = 0.88 (858.4 examples/sec; 0.149 sec/batch)
2017-03-25 22:15:18.815599: step 32620, loss = 0.68 (755.7 examples/sec; 0.169 sec/batch)
2017-03-25 22:15:20.547647: step 32630, loss = 0.75 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:15:22.292921: step 32640, loss = 0.81 (733.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:15:24.031418: step 32650, loss = 0.89 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:15:25.780249: step 32660, loss = 0.82 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:15:27.505517: step 32670, loss = 0.82 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:15:29.237380: step 32680, loss = 0.71 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:15:30.971433: step 32690, loss = 0.80 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:15:32.856906: step 32700, loss = 0.74 (679.2 examples/sec; 0.188 sec/batch)
2017-03-25 22:15:34.476018: step 32710, loss = 0.87 (790.1 examples/sec; 0.162 sec/batch)
2017-03-25 22:15:36.209838: step 32720, loss = 0.76 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:15:37.933394: step 32730, loss = 0.74 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:15:39.671692: step 32740, loss = 0.72 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:15:41.419443: step 32750, loss = 0.79 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:15:43.157293: step 32760, loss = 0.77 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:15:44.891759: step 32770, loss = 0.84 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:15:46.632142: step 32780, loss = 0.88 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:15:48.361812: step 32790, loss = 0.79 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:15:50.245818: step 32800, loss = 0.74 (679.4 examples/sec; 0.188 sec/batch)
2017-03-25 22:15:51.886768: step 32810, loss = 0.64 (780.0 examples/sec; 0.164 sec/batch)
2017-03-25 22:15:53.625542: step 32820, loss = 0.76 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:15:55.350557: step 32830, loss = 0.68 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:15:57.078960: step 32840, loss = 0.77 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:15:58.815571: step 32850, loss = 0.76 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:16:00.541764: step 32860, loss = 0.79 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:16:02.274054: step 32870, loss = 0.80 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:16:04.011523: step 32880, loss = 0.74 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:16:05.741144: step 32890, loss = 0.67 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:16:07.646770: step 32900, loss = 0.79 (671.7 examples/sec; 0.191 sec/batch)
2017-03-25 22:16:09.264435: step 32910, loss = 0.78 (791.3 examples/sec; 0.162 sec/batch)
2017-03-25 22:16:10.993238: step 32920, loss = 0.84 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:16:12.722258: step 32930, loss = 0.74 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:16:14.464472: step 32940, loss = 0.79 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:16:16.199622: step 32950, loss = 0.72 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:16:17.936213: step 32960, loss = 0.90 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:16:19.672303: step 32970, loss = 0.70 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:16:21.404651: step 32980, loss = 0.59 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:16:23.137463: step 32990, loss = 0.74 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:16:25.036230: step 33000, loss = 0.75 (674.1 examples/sec; 0.190 sec/batch)
2017-03-25 22:16:26.636204: step 33010, loss = 0.75 (800.0 examples/sec; 0.160 sec/batch)
2017-03-25 22:16:28.374898: step 33020, loss = 0.84 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:16:30.097388: step 33030, loss = 0.82 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:16:31.836110: step 33040, loss = 0.58 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:16:33.562304: step 33050, loss = 0.76 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:16:35.285710: step 33060, loss = 0.79 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:16:37.015264: step 33070, loss = 0.79 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:16:38.758983: step 33080, loss = 0.86 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:16:40.482743: step 33090, loss = 0.69 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:16:42.381896: step 33100, loss = 0.75 (674.0 examples/sec; 0.190 sec/batch)
2017-03-25 22:16:44.002067: step 33110, loss = 0.79 (790.0 examples/sec; 0.162 sec/batch)
2017-03-25 22:16:45.735847: step 33120, loss = 0.74 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:16:47.464804: step 33130, loss = 0.68 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:16:49.184673: step 33140, loss = 0.86 (744.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:16:50.920589: step 33150, loss = 0.74 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:16:52.661045: step 33160, loss = 0.73 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:16:54.385053: step 33170, loss = 0.71 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:16:56.114571: step 33180, loss = 0.74 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:16:57.849240: step 33190, loss = 0.92 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:16:59.733957: step 33200, loss = 0.81 (679.8 examples/sec; 0.188 sec/batch)
2017-03-25 22:17:01.352401: step 33210, loss = 0.80 (790.0 examples/sec; 0.162 sec/batch)
2017-03-25 22:17:03.093391: step 33220, loss = 0.91 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:17:04.831656: step 33230, loss = 0.72 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:17:06.565167: step 33240, loss = 0.72 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:17:08.306840: step 33250, loss = 0.83 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:17:10.018910: step 33260, loss = 0.78 (747.6 examples/sec; 0.171 sec/batch)
2017-03-25 22:17:11.755257: step 33270, loss = 0.83 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:17:13.486423: step 33280, loss = 0.84 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:17:15.216397: step 33290, loss = 0.75 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:17:17.162853: step 33300, loss = 0.76 (657.8 examples/sec; 0.195 sec/batch)
2017-03-25 22:17:18.706072: step 33310, loss = 1.03 (829.0 examples/sec; 0.154 sec/batch)
2017-03-25 22:17:20.449603: step 33320, loss = 0.57 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:17:22.179676: step 33330, loss = 0.65 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:17:23.907992: step 33340, loss = 0.78 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:17:25.636547: step 33350, loss = 0.74 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:17:27.364942: step 33360, loss = 0.81 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:17:29.098183: step 33370, loss = 0.76 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:17:30.842241: step 33380, loss = 0.70 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:17:32.572111: step 33390, loss = 0.66 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:17:34.471897: step 33400, loss = 0.84 (673.9 examples/sec; 0.190 sec/batch)
2017-03-25 22:17:36.077032: step 33410, loss = 0.77 (797.2 examples/sec; 0.161 sec/batch)
2017-03-25 22:17:37.793852: step 33420, loss = 0.76 (745.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:17:39.532629: step 33430, loss = 0.74 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:17:41.261150: step 33440, loss = 0.78 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:17:42.993146: step 33450, loss = 0.74 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:17:44.746053: step 33460, loss = 0.82 (730.2 examples/sec; 0.175 sec/batch)
2017-03-25 22:17:46.465757: step 33470, loss = 0.81 (744.3 examples/sec; 0.172 sec/batch)
2017-03-25 22:17:48.217814: step 33480, loss = 0.76 (730.6 examples/sec; 0.175 sec/batch)
2017-03-25 22:17:49.950241: step 33490, loss = 0.74 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:17:51.864124: step 33500, loss = 0.91 (668.8 examples/sec; 0.191 sec/batch)
2017-03-25 22:17:53.464504: step 33510, loss = 0.65 (799.8 examples/sec; 0.160 sec/batch)
2017-03-25 22:17:55.191088: step 33520, loss = 0.67 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:17:56.913431: step 33530, loss = 0.74 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:17:58.651707: step 33540, loss = 0.70 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:18:00.394290: step 33550, loss = 0.80 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:18:02.126416: step 33560, loss = 0.74 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:18:03.835808: step 33570, loss = 0.80 (748.8 examples/sec; 0.171 sec/batch)
2017-03-25 22:18:05.567288: step 33580, loss = 0.82 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:18:07.309280: step 33590, loss = 0.74 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:18:09.177866: step 33600, loss = 0.85 (685.4 examples/sec; 0.187 sec/batch)
2017-03-25 22:18:10.816865: step 33610, loss = 0.63 (780.5 examples/sec; 0.164 sec/batch)
2017-03-25 22:18:12.550554: step 33620, loss = 0.81 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:18:14.280733: step 33630, loss = 0.79 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:18:16.001243: step 33640, loss = 0.72 (744.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:18:17.758270: step 33650, loss = 0.74 (728.5 examples/sec; 0.176 sec/batch)
2017-03-25 22:18:19.490490: step 33660, loss = 0.83 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:18:21.227233: step 33670, loss = 0.71 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:18:22.957342: step 33680, loss = 0.76 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:18:24.699072: step 33690, loss = 0.60 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:18:26.599107: step 33700, loss = 0.87 (674.0 examples/sec; 0.190 sec/batch)
2017-03-25 22:18:28.223875: step 33710, loss = 0.73 (787.4 examples/sec; 0.163 sec/batch)
2017-03-25 22:18:29.952984: step 33720, loss = 0.73 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:18:31.675719: step 33730, loss = 0.81 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:18:33.406420: step 33740, loss = 0.68 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:18:35.125025: step 33750, loss = 0.84 (744.8 examples/sec; 0.172 sec/batch)
2017-03-25 22:18:36.865997: step 33760, loss = 0.61 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:18:38.591689: step 33770, loss = 0.73 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:18:40.321162: step 33780, loss = 0.76 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:18:42.050359: step 33790, loss = 0.67 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:18:44.003861: step 33800, loss = 0.81 (655.2 examples/sec; 0.195 sec/batch)
2017-03-25 22:18:45.520414: step 33810, loss = 0.74 (844.0 examples/sec; 0.152 sec/batch)
2017-03-25 22:18:47.212716: step 33820, loss = 0.73 (756.4 examples/sec; 0.169 sec/batch)
2017-03-25 22:18:48.935640: step 33830, loss = 0.90 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:18:50.672929: step 33840, loss = 0.65 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:18:52.405187: step 33850, loss = 0.87 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:18:54.137625: step 33860, loss = 0.64 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:18:55.862221: step 33870, loss = 0.82 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:18:57.593576: step 33880, loss = 0.64 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:18:59.324673: step 33890, loss = 0.77 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:01.283919: step 33900, loss = 0.81 (653.8 examples/sec; 0.196 sec/batch)
2017-03-25 22:19:02.847690: step 33910, loss = 0.84 (817.8 examples/sec; 0.157 sec/batch)
2017-03-25 22:19:04.574095: step 33920, loss = 0.77 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:06.315435: step 33930, loss = 0.84 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:19:08.035914: step 33940, loss = 0.67 (744.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:19:09.767141: step 33950, loss = 0.88 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:11.491370: step 33960, loss = 0.78 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:19:13.229182: step 33970, loss = 0.83 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:19:14.961443: step 33980, loss = 0.79 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:16.696931: step 33990, loss = 0.81 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:19:18.579177: step 34000, loss = 0.86 (680.1 examples/sec; 0.188 sec/batch)
2017-03-25 22:19:20.208764: step 34010, loss = 0.73 (785.4 examples/sec; 0.163 sec/batch)
2017-03-25 22:19:21.940421: step 34020, loss = 0.77 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:23.671484: step 34030, loss = 0.72 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:25.400116: step 34040, loss = 0.79 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:27.131571: step 34050, loss = 0.71 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:28.871312: step 34060, loss = 0.76 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:19:30.598861: step 34070, loss = 0.76 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:32.351558: step 34080, loss = 0.83 (730.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:19:34.078456: step 34090, loss = 0.77 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:35.988622: step 34100, loss = 0.75 (670.5 examples/sec; 0.191 sec/batch)
2017-03-25 22:19:37.584507: step 34110, loss = 0.91 (801.5 examples/sec; 0.160 sec/batch)
2017-03-25 22:19:39.315872: step 34120, loss = 0.63 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:41.066672: step 34130, loss = 0.88 (731.1 examples/sec; 0.175 sec/batch)
2017-03-25 22:19:42.787955: step 34140, loss = 0.83 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:19:44.524175: step 34150, loss = 0.67 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:19:46.255428: step 34160, loss = 0.79 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:47.985346: step 34170, loss = 0.71 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:49.719572: step 34180, loss = 0.78 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:51.447667: step 34190, loss = 0.69 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:53.338570: step 34200, loss = 0.73 (676.9 examples/sec; 0.189 sec/batch)
2017-03-25 22:19:54.944156: step 34210, loss = 0.82 (797.2 examples/sec; 0.161 sec/batch)
2017-03-25 22:19:56.676258: step 34220, loss = 0.77 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:19:58.401316: step 34230, loss = 0.52 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:20:00.137154: step 34240, loss = 0.76 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:20:01.864279: step 34250, loss = 0.82 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:20:03.601045: step 34260, loss = 0.73 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:20:05.338033: step 34270, loss = 0.94 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:20:07.078038: step 34280, loss = 0.88 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:20:08.807777: step 34290, loss = 0.75 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:20:10.766501: step 34300, loss = 0.59 (653.5 examples/sec; 0.196 sec/batch)
2017-03-25 22:20:12.283470: step 34310, loss = 0.86 (843.8 examples/sec; 0.152 sec/batch)
2017-03-25 22:20:13.982712: step 34320, loss = 0.82 (753.3 examples/sec; 0.170 sec/batch)
2017-03-25 22:20:15.725027: step 34330, loss = 0.80 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:20:17.458260: step 34340, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:20:19.186363: step 34350, loss = 0.91 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:20:20.930024: step 34360, loss = 0.87 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:20:22.644153: step 34370, loss = 0.89 (746.7 examples/sec; 0.171 sec/batch)
2017-03-25 22:20:24.388519: step 34380, loss = 0.62 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:20:26.132103: step 34390, loss = 0.74 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:20:28.011000: step 34400, loss = 0.83 (681.3 examples/sec; 0.188 sec/batch)
2017-03-25 22:20:29.633714: step 34410, loss = 0.81 (788.8 examples/sec; 0.162 sec/batch)
2017-03-25 22:20:31.348331: step 34420, loss = 0.72 (746.5 examples/sec; 0.171 sec/batch)
2017-03-25 22:20:33.087952: step 34430, loss = 0.72 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:20:34.838188: step 34440, loss = 0.79 (731.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:20:36.568089: step 34450, loss = 0.59 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:20:38.309163: step 34460, loss = 0.57 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:20:40.036128: step 34470, loss = 0.85 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:20:41.768983: step 34480, loss = 0.82 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:20:43.498174: step 34490, loss = 0.69 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:20:45.424551: step 34500, loss = 0.75 (664.5 examples/sec; 0.193 sec/batch)
2017-03-25 22:20:47.011594: step 34510, loss = 0.89 (806.5 examples/sec; 0.159 sec/batch)
2017-03-25 22:20:48.751706: step 34520, loss = 0.71 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:20:50.485951: step 34530, loss = 0.80 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:20:52.218085: step 34540, loss = 0.73 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:20:53.973121: step 34550, loss = 0.74 (729.3 examples/sec; 0.176 sec/batch)
2017-03-25 22:20:55.696362: step 34560, loss = 0.76 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 22:20:57.433024: step 34570, loss = 0.79 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:20:59.160391: step 34580, loss = 0.82 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:00.890049: step 34590, loss = 0.77 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:02.786886: step 34600, loss = 0.80 (674.8 examples/sec; 0.190 sec/batch)
2017-03-25 22:21:04.395156: step 34610, loss = 0.84 (795.9 examples/sec; 0.161 sec/batch)
2017-03-25 22:21:06.127369: step 34620, loss = 0.77 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:07.854809: step 34630, loss = 0.92 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:09.592281: step 34640, loss = 0.63 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:21:11.314063: step 34650, loss = 1.00 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:21:13.045357: step 34660, loss = 0.72 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:14.775253: step 34670, loss = 0.63 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:16.503203: step 34680, loss = 0.70 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:18.238462: step 34690, loss = 0.91 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:21:20.142567: step 34700, loss = 0.74 (672.5 examples/sec; 0.190 sec/batch)
2017-03-25 22:21:21.738898: step 34710, loss = 0.74 (801.4 examples/sec; 0.160 sec/batch)
2017-03-25 22:21:23.473875: step 34720, loss = 0.71 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:25.214325: step 34730, loss = 0.68 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:21:26.945043: step 34740, loss = 0.73 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:28.679381: step 34750, loss = 0.79 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:30.413978: step 34760, loss = 0.80 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:32.141298: step 34770, loss = 0.82 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:33.870931: step 34780, loss = 0.68 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:35.605694: step 34790, loss = 0.80 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:37.474244: step 34800, loss = 0.78 (685.0 examples/sec; 0.187 sec/batch)
2017-03-25 22:21:39.094562: step 34810, loss = 0.80 (790.0 examples/sec; 0.162 sec/batch)
2017-03-25 22:21:40.838222: step 34820, loss = 0.77 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:21:42.571418: step 34830, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:44.307846: step 34840, loss = 0.69 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:21:46.033693: step 34850, loss = 0.80 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:47.755958: step 34860, loss = 0.77 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:21:49.482939: step 34870, loss = 0.81 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:51.223963: step 34880, loss = 0.71 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:21:52.958903: step 34890, loss = 0.71 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:21:54.904186: step 34900, loss = 0.83 (658.4 examples/sec; 0.194 sec/batch)
2017-03-25 22:21:56.457830: step 34910, loss = 0.68 (823.2 examples/sec; 0.155 sec/batch)
2017-03-25 22:21:58.182825: step 34920, loss = 0.77 (742.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:21:59.916634: step 34930, loss = 0.74 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:01.652514: step 34940, loss = 0.79 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:22:03.391531: step 34950, loss = 0.74 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:22:05.116458: step 34960, loss = 0.76 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:22:06.849005: step 34970, loss = 0.74 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:08.582597: step 34980, loss = 0.73 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:10.310588: step 34990, loss = 0.85 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:12.224329: step 35000, loss = 1.00 (669.2 examples/sec; 0.191 sec/batch)
2017-03-25 22:22:13.817219: step 35010, loss = 0.73 (803.1 examples/sec; 0.159 sec/batch)
2017-03-25 22:22:15.539800: step 35020, loss = 0.72 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:22:17.284222: step 35030, loss = 0.69 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:22:19.008617: step 35040, loss = 0.90 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 22:22:20.739055: step 35050, loss = 0.79 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:22.475252: step 35060, loss = 0.68 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:22:24.208725: step 35070, loss = 0.59 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:25.940728: step 35080, loss = 0.85 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:27.665256: step 35090, loss = 0.85 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:22:29.559633: step 35100, loss = 0.68 (676.0 examples/sec; 0.189 sec/batch)
2017-03-25 22:22:31.183214: step 35110, loss = 0.82 (787.9 examples/sec; 0.162 sec/batch)
2017-03-25 22:22:32.910027: step 35120, loss = 0.83 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:34.651452: step 35130, loss = 0.76 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:22:36.395364: step 35140, loss = 0.79 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:22:38.124984: step 35150, loss = 0.70 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:39.859448: step 35160, loss = 0.71 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:41.586518: step 35170, loss = 0.66 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:43.321505: step 35180, loss = 0.81 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:45.056230: step 35190, loss = 0.88 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:47.010533: step 35200, loss = 0.81 (655.2 examples/sec; 0.195 sec/batch)
2017-03-25 22:22:48.533313: step 35210, loss = 0.71 (840.1 examples/sec; 0.152 sec/batch)
2017-03-25 22:22:50.249347: step 35220, loss = 0.77 (745.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:22:51.993110: step 35230, loss = 0.81 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:22:53.724774: step 35240, loss = 0.87 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:55.449981: step 35250, loss = 0.75 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:57.184767: step 35260, loss = 0.77 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:22:58.920181: step 35270, loss = 0.74 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:23:00.667000: step 35280, loss = 0.78 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 22:23:02.404716: step 35290, loss = 0.77 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:23:04.305976: step 35300, loss = 0.82 (673.4 examples/sec; 0.190 sec/batch)
2017-03-25 22:23:05.914039: step 35310, loss = 0.87 (795.9 examples/sec; 0.161 sec/batch)
2017-03-25 22:23:07.644075: step 35320, loss = 0.61 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:23:09.385744: step 35330, loss = 0.76 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:23:11.109834: step 35340, loss = 0.58 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:23:12.845780: step 35350, loss = 0.70 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:23:14.578080: step 35360, loss = 0.74 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:23:16.310359: step 35370, loss = 0.62 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:23:18.038061: step 35380, loss = 0.88 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:23:19.769952: step 35390, loss = 0.76 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:23:21.766167: step 35400, loss = 0.74 (641.4 examples/sec; 0.200 sec/batch)
2017-03-25 22:23:23.225763: step 35410, loss = 0.71 (876.8 examples/sec; 0.146 sec/batch)
2017-03-25 22:23:24.904045: step 35420, loss = 0.91 (762.5 examples/sec; 0.168 sec/batch)
2017-03-25 22:23:26.583717: step 35430, loss = 0.89 (762.3 examples/sec; 0.168 sec/batch)
2017-03-25 22:23:28.257607: step 35440, loss = 0.77 (764.4 examples/sec; 0.167 sec/batch)
2017-03-25 22:23:29.933308: step 35450, loss = 0.83 (763.9 examples/sec; 0.168 sec/batch)
2017-03-25 22:23:31.599094: step 35460, loss = 0.78 (768.4 examples/sec; 0.167 sec/batch)
2017-03-25 22:23:33.280039: step 35470, loss = 0.94 (761.6 examples/sec; 0.168 sec/batch)
2017-03-25 22:23:34.958419: step 35480, loss = 0.68 (762.5 examples/sec; 0.168 sec/batch)
2017-03-25 22:23:36.680757: step 35490, loss = 0.64 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:23:38.582731: step 35500, loss = 1.02 (673.0 examples/sec; 0.190 sec/batch)
2017-03-25 22:23:40.191379: step 35510, loss = 0.80 (795.7 examples/sec; 0.161 sec/batch)
2017-03-25 22:23:41.926278: step 35520, loss = 0.74 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:23:43.659892: step 35530, loss = 0.74 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:23:45.405021: step 35540, loss = 0.69 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:23:47.142604: step 35550, loss = 0.80 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:23:48.882906: step 35560, loss = 0.71 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:23:50.622849: step 35570, loss = 0.72 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:23:52.350796: step 35580, loss = 0.79 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:23:54.082947: step 35590, loss = 0.63 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:23:55.970882: step 35600, loss = 0.75 (678.0 examples/sec; 0.189 sec/batch)
2017-03-25 22:23:57.581280: step 35610, loss = 0.75 (794.8 examples/sec; 0.161 sec/batch)
2017-03-25 22:23:59.320850: step 35620, loss = 0.73 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:24:01.056843: step 35630, loss = 0.77 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:24:02.788846: step 35640, loss = 0.70 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:04.529859: step 35650, loss = 0.68 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:24:06.264372: step 35660, loss = 0.78 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:08.003089: step 35670, loss = 0.71 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:24:09.746059: step 35680, loss = 0.72 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:24:11.486709: step 35690, loss = 0.73 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:24:13.390814: step 35700, loss = 0.86 (672.5 examples/sec; 0.190 sec/batch)
2017-03-25 22:24:14.998119: step 35710, loss = 0.69 (796.2 examples/sec; 0.161 sec/batch)
2017-03-25 22:24:16.715404: step 35720, loss = 0.68 (745.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:24:18.442228: step 35730, loss = 0.73 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:20.176593: step 35740, loss = 0.78 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:21.906301: step 35750, loss = 0.69 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:23.640825: step 35760, loss = 0.79 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:25.372111: step 35770, loss = 0.77 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:27.106818: step 35780, loss = 0.70 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:28.837563: step 35790, loss = 0.83 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:30.802602: step 35800, loss = 0.75 (651.4 examples/sec; 0.197 sec/batch)
2017-03-25 22:24:32.343305: step 35810, loss = 0.78 (830.9 examples/sec; 0.154 sec/batch)
2017-03-25 22:24:34.072050: step 35820, loss = 0.65 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:35.801673: step 35830, loss = 0.80 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:37.530349: step 35840, loss = 0.80 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:39.247089: step 35850, loss = 0.81 (745.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:24:40.977554: step 35860, loss = 0.66 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:42.721303: step 35870, loss = 0.72 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:24:44.448402: step 35880, loss = 0.74 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:46.172256: step 35890, loss = 0.77 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:24:48.060798: step 35900, loss = 0.67 (677.8 examples/sec; 0.189 sec/batch)
2017-03-25 22:24:49.678272: step 35910, loss = 0.66 (791.4 examples/sec; 0.162 sec/batch)
2017-03-25 22:24:51.417642: step 35920, loss = 0.79 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:24:53.147683: step 35930, loss = 0.80 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:24:54.889966: step 35940, loss = 0.80 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:24:56.610299: step 35950, loss = 0.65 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:24:58.345846: step 35960, loss = 0.98 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:25:00.076474: step 35970, loss = 0.71 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:25:01.820757: step 35980, loss = 0.90 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:25:03.543754: step 35990, loss = 0.87 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:25:05.550218: step 36000, loss = 0.90 (638.2 examples/sec; 0.201 sec/batch)
2017-03-25 22:25:07.018880: step 36010, loss = 0.78 (871.1 examples/sec; 0.147 sec/batch)
2017-03-25 22:25:08.705218: step 36020, loss = 0.81 (759.0 examples/sec; 0.169 sec/batch)
2017-03-25 22:25:10.438760: step 36030, loss = 0.68 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:25:12.179706: step 36040, loss = 0.75 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:25:13.923744: step 36050, loss = 0.72 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:25:15.656260: step 36060, loss = 1.06 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:25:17.385256: step 36070, loss = 0.84 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:25:19.131184: step 36080, loss = 0.86 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 22:25:20.862615: step 36090, loss = 0.71 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:25:22.857531: step 36100, loss = 0.82 (642.0 examples/sec; 0.199 sec/batch)
2017-03-25 22:25:24.399716: step 36110, loss = 0.79 (829.3 examples/sec; 0.154 sec/batch)
2017-03-25 22:25:26.130849: step 36120, loss = 0.82 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:25:27.869262: step 36130, loss = 0.63 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:25:29.596352: step 36140, loss = 0.68 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:25:31.329966: step 36150, loss = 0.67 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:25:33.064243: step 36160, loss = 0.89 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:25:34.786240: step 36170, loss = 0.56 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:25:36.523767: step 36180, loss = 0.62 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:25:38.252417: step 36190, loss = 0.75 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:25:40.138184: step 36200, loss = 0.86 (679.1 examples/sec; 0.188 sec/batch)
2017-03-25 22:25:41.751653: step 36210, loss = 0.73 (792.9 examples/sec; 0.161 sec/batch)
2017-03-25 22:25:43.487208: step 36220, loss = 0.73 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:25:45.228371: step 36230, loss = 0.64 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:25:46.980609: step 36240, loss = 0.91 (730.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:25:48.717202: step 36250, loss = 0.68 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:25:50.463678: step 36260, loss = 0.64 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:25:52.208798: step 36270, loss = 0.88 (733.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:25:53.935151: step 36280, loss = 0.74 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:25:55.678839: step 36290, loss = 0.90 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:25:57.584706: step 36300, loss = 0.71 (671.6 examples/sec; 0.191 sec/batch)
2017-03-25 22:25:59.215911: step 36310, loss = 0.68 (784.8 examples/sec; 0.163 sec/batch)
2017-03-25 22:26:00.957999: step 36320, loss = 0.75 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:02.702291: step 36330, loss = 0.78 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:04.442901: step 36340, loss = 0.72 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:06.186051: step 36350, loss = 0.83 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:07.922204: step 36360, loss = 0.71 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:09.660365: step 36370, loss = 0.76 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:11.400166: step 36380, loss = 0.78 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:13.142144: step 36390, loss = 0.66 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:15.069656: step 36400, loss = 0.62 (664.1 examples/sec; 0.193 sec/batch)
2017-03-25 22:26:16.646663: step 36410, loss = 0.69 (811.7 examples/sec; 0.158 sec/batch)
2017-03-25 22:26:18.393627: step 36420, loss = 0.80 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 22:26:20.127795: step 36430, loss = 0.73 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:26:21.863772: step 36440, loss = 0.84 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:23.597924: step 36450, loss = 0.74 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:26:25.336837: step 36460, loss = 0.82 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:27.073317: step 36470, loss = 0.81 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:28.815876: step 36480, loss = 0.73 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:30.541881: step 36490, loss = 0.75 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:26:32.426798: step 36500, loss = 0.70 (679.6 examples/sec; 0.188 sec/batch)
2017-03-25 22:26:34.039560: step 36510, loss = 0.75 (793.0 examples/sec; 0.161 sec/batch)
2017-03-25 22:26:35.776034: step 36520, loss = 0.79 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:37.512102: step 36530, loss = 0.72 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:39.242832: step 36540, loss = 0.80 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:26:40.989699: step 36550, loss = 0.80 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 22:26:42.702045: step 36560, loss = 0.81 (747.5 examples/sec; 0.171 sec/batch)
2017-03-25 22:26:44.440876: step 36570, loss = 0.92 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:26:46.173562: step 36580, loss = 0.71 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:26:47.906420: step 36590, loss = 0.72 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:26:49.815551: step 36600, loss = 0.65 (670.7 examples/sec; 0.191 sec/batch)
2017-03-25 22:26:51.431521: step 36610, loss = 0.65 (791.8 examples/sec; 0.162 sec/batch)
2017-03-25 22:26:53.155406: step 36620, loss = 0.82 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:26:54.887021: step 36630, loss = 0.68 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:26:56.615669: step 36640, loss = 0.61 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:26:58.365640: step 36650, loss = 0.83 (731.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:27:00.095449: step 36660, loss = 0.81 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:01.849662: step 36670, loss = 0.66 (729.7 examples/sec; 0.175 sec/batch)
2017-03-25 22:27:03.581749: step 36680, loss = 0.81 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:05.316722: step 36690, loss = 0.95 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:07.197221: step 36700, loss = 0.68 (681.0 examples/sec; 0.188 sec/batch)
2017-03-25 22:27:08.820167: step 36710, loss = 0.90 (788.2 examples/sec; 0.162 sec/batch)
2017-03-25 22:27:10.556874: step 36720, loss = 0.83 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:27:12.295781: step 36730, loss = 0.77 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:27:14.018351: step 36740, loss = 0.67 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:27:15.747363: step 36750, loss = 0.65 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:17.461728: step 36760, loss = 0.65 (746.6 examples/sec; 0.171 sec/batch)
2017-03-25 22:27:19.194625: step 36770, loss = 0.80 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:20.927184: step 36780, loss = 1.00 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:22.659934: step 36790, loss = 0.79 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:24.573120: step 36800, loss = 0.77 (669.5 examples/sec; 0.191 sec/batch)
2017-03-25 22:27:26.188767: step 36810, loss = 0.90 (791.6 examples/sec; 0.162 sec/batch)
2017-03-25 22:27:27.912262: step 36820, loss = 0.61 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:27:29.651464: step 36830, loss = 0.83 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:27:31.377389: step 36840, loss = 0.72 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:33.114907: step 36850, loss = 0.74 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:27:34.834779: step 36860, loss = 0.78 (744.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:27:36.573757: step 36870, loss = 0.76 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:27:38.292669: step 36880, loss = 0.84 (744.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:27:40.027381: step 36890, loss = 0.73 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:41.936104: step 36900, loss = 0.78 (670.6 examples/sec; 0.191 sec/batch)
2017-03-25 22:27:43.545126: step 36910, loss = 0.86 (795.5 examples/sec; 0.161 sec/batch)
2017-03-25 22:27:45.275422: step 36920, loss = 0.80 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:47.001250: step 36930, loss = 0.72 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:48.750999: step 36940, loss = 0.76 (731.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:27:50.481272: step 36950, loss = 0.78 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:52.207856: step 36960, loss = 0.79 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:53.947306: step 36970, loss = 0.69 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:27:55.672922: step 36980, loss = 0.75 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:57.399680: step 36990, loss = 0.81 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:27:59.307961: step 37000, loss = 0.89 (671.0 examples/sec; 0.191 sec/batch)
2017-03-25 22:28:00.921848: step 37010, loss = 0.81 (792.8 examples/sec; 0.161 sec/batch)
2017-03-25 22:28:02.657872: step 37020, loss = 0.59 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:28:04.391332: step 37030, loss = 0.74 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:06.129426: step 37040, loss = 0.82 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:28:07.862060: step 37050, loss = 0.94 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:09.582677: step 37060, loss = 0.76 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:28:11.308493: step 37070, loss = 0.79 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:13.033890: step 37080, loss = 0.85 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:14.766845: step 37090, loss = 0.62 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:16.640138: step 37100, loss = 0.71 (683.3 examples/sec; 0.187 sec/batch)
2017-03-25 22:28:18.268409: step 37110, loss = 0.82 (786.1 examples/sec; 0.163 sec/batch)
2017-03-25 22:28:20.011048: step 37120, loss = 0.72 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:28:21.745027: step 37130, loss = 0.76 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:23.480717: step 37140, loss = 0.64 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:28:25.213594: step 37150, loss = 0.76 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:26.941964: step 37160, loss = 0.58 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:28.677072: step 37170, loss = 0.86 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:28:30.413723: step 37180, loss = 0.75 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:28:32.149685: step 37190, loss = 0.91 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:28:34.040909: step 37200, loss = 0.74 (676.8 examples/sec; 0.189 sec/batch)
2017-03-25 22:28:35.655257: step 37210, loss = 0.74 (792.9 examples/sec; 0.161 sec/batch)
2017-03-25 22:28:37.383156: step 37220, loss = 0.76 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:39.117853: step 37230, loss = 0.67 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:40.845527: step 37240, loss = 0.74 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:42.572276: step 37250, loss = 0.81 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:44.298499: step 37260, loss = 0.81 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:46.034104: step 37270, loss = 0.81 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:28:47.761369: step 37280, loss = 0.77 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:49.503448: step 37290, loss = 0.66 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:28:51.401066: step 37300, loss = 0.73 (674.5 examples/sec; 0.190 sec/batch)
2017-03-25 22:28:52.986633: step 37310, loss = 0.97 (807.4 examples/sec; 0.159 sec/batch)
2017-03-25 22:28:54.711584: step 37320, loss = 0.77 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:56.445646: step 37330, loss = 1.04 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:28:58.189940: step 37340, loss = 0.83 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:28:59.923232: step 37350, loss = 0.73 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:29:01.671817: step 37360, loss = 0.76 (732.0 examples/sec; 0.175 sec/batch)
2017-03-25 22:29:03.396087: step 37370, loss = 0.70 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 22:29:05.123152: step 37380, loss = 0.69 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:29:06.861706: step 37390, loss = 0.63 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:29:08.823715: step 37400, loss = 0.86 (652.4 examples/sec; 0.196 sec/batch)
2017-03-25 22:29:10.373251: step 37410, loss = 0.95 (826.1 examples/sec; 0.155 sec/batch)
2017-03-25 22:29:12.110522: step 37420, loss = 0.89 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:29:13.848314: step 37430, loss = 1.04 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:29:15.576606: step 37440, loss = 0.83 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:29:17.313382: step 37450, loss = 0.65 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:29:19.057483: step 37460, loss = 0.85 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:29:20.786745: step 37470, loss = 0.86 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:29:22.520714: step 37480, loss = 0.74 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:29:24.260045: step 37490, loss = 0.85 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:29:26.165863: step 37500, loss = 0.64 (671.6 examples/sec; 0.191 sec/batch)
2017-03-25 22:29:27.745026: step 37510, loss = 0.65 (810.6 examples/sec; 0.158 sec/batch)
2017-03-25 22:29:29.476322: step 37520, loss = 0.65 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:29:31.213877: step 37530, loss = 0.72 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:29:32.939587: step 37540, loss = 0.64 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:29:34.682980: step 37550, loss = 0.88 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:29:36.410713: step 37560, loss = 0.66 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:29:38.131716: step 37570, loss = 0.97 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:29:39.873727: step 37580, loss = 0.97 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:29:41.626471: step 37590, loss = 0.75 (730.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:29:43.503756: step 37600, loss = 0.83 (681.8 examples/sec; 0.188 sec/batch)
2017-03-25 22:29:45.123844: step 37610, loss = 0.83 (790.1 examples/sec; 0.162 sec/batch)
2017-03-25 22:29:46.865038: step 37620, loss = 0.64 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:29:48.600543: step 37630, loss = 0.81 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:29:50.334431: step 37640, loss = 0.71 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:29:52.064291: step 37650, loss = 0.80 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:29:53.804420: step 37660, loss = 0.97 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:29:55.544678: step 37670, loss = 0.76 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:29:57.284660: step 37680, loss = 0.77 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:29:59.017803: step 37690, loss = 0.92 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:30:00.913994: step 37700, loss = 0.77 (675.0 examples/sec; 0.190 sec/batch)
2017-03-25 22:30:02.559868: step 37710, loss = 0.65 (777.7 examples/sec; 0.165 sec/batch)
2017-03-25 22:30:04.300175: step 37720, loss = 0.89 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:30:06.038421: step 37730, loss = 0.69 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:30:07.767478: step 37740, loss = 0.72 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:30:09.499258: step 37750, loss = 0.80 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:30:11.226801: step 37760, loss = 0.79 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:30:12.961055: step 37770, loss = 0.68 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:30:14.691559: step 37780, loss = 0.69 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:30:16.417399: step 37790, loss = 0.88 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:30:18.303951: step 37800, loss = 0.92 (678.8 examples/sec; 0.189 sec/batch)
2017-03-25 22:30:19.924506: step 37810, loss = 0.83 (789.4 examples/sec; 0.162 sec/batch)
2017-03-25 22:30:21.660284: step 37820, loss = 0.78 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:30:23.386894: step 37830, loss = 0.66 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:30:25.115771: step 37840, loss = 0.88 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:30:26.850826: step 37850, loss = 0.85 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:30:28.595447: step 37860, loss = 0.72 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:30:30.335852: step 37870, loss = 0.83 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:30:32.088767: step 37880, loss = 0.66 (730.2 examples/sec; 0.175 sec/batch)
2017-03-25 22:30:33.819663: step 37890, loss = 0.85 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:30:35.814961: step 37900, loss = 0.90 (641.5 examples/sec; 0.200 sec/batch)
2017-03-25 22:30:37.321288: step 37910, loss = 0.78 (849.8 examples/sec; 0.151 sec/batch)
2017-03-25 22:30:39.064818: step 37920, loss = 0.79 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:30:40.796222: step 37930, loss = 0.70 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:30:42.535746: step 37940, loss = 0.67 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:30:44.276220: step 37950, loss = 0.91 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:30:46.019867: step 37960, loss = 0.66 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:30:47.755062: step 37970, loss = 0.82 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:30:49.497116: step 37980, loss = 0.76 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:30:51.239499: step 37990, loss = 0.61 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:30:53.230484: step 38000, loss = 0.66 (643.1 examples/sec; 0.199 sec/batch)
2017-03-25 22:30:54.754521: step 38010, loss = 0.73 (840.1 examples/sec; 0.152 sec/batch)
2017-03-25 22:30:56.493100: step 38020, loss = 0.71 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:30:58.227830: step 38030, loss = 0.66 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:30:59.953131: step 38040, loss = 0.70 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:31:01.691596: step 38050, loss = 0.54 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:31:03.418655: step 38060, loss = 0.72 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:31:05.158264: step 38070, loss = 0.78 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:31:06.892407: step 38080, loss = 0.88 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:31:08.634126: step 38090, loss = 0.76 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:31:10.588277: step 38100, loss = 0.71 (655.0 examples/sec; 0.195 sec/batch)
2017-03-25 22:31:12.143626: step 38110, loss = 0.77 (823.0 examples/sec; 0.156 sec/batch)
2017-03-25 22:31:13.881363: step 38120, loss = 0.83 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:31:15.603983: step 38130, loss = 0.65 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:31:17.331327: step 38140, loss = 0.85 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:31:19.061244: step 38150, loss = 0.59 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:31:20.791198: step 38160, loss = 0.85 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:31:22.530869: step 38170, loss = 0.85 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:31:24.273934: step 38180, loss = 0.87 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:31:25.999834: step 38190, loss = 0.78 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:31:27.877113: step 38200, loss = 0.74 (681.9 examples/sec; 0.188 sec/batch)
2017-03-25 22:31:29.509071: step 38210, loss = 0.66 (784.0 examples/sec; 0.163 sec/batch)
2017-03-25 22:31:31.247235: step 38220, loss = 0.72 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:31:32.976278: step 38230, loss = 1.00 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:31:34.720497: step 38240, loss = 0.72 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:31:36.447933: step 38250, loss = 0.91 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:31:38.184526: step 38260, loss = 0.76 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:31:39.934684: step 38270, loss = 0.83 (731.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:31:41.650778: step 38280, loss = 0.63 (745.8 examples/sec; 0.172 sec/batch)
2017-03-25 22:31:43.392110: step 38290, loss = 0.92 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:31:45.296916: step 38300, loss = 0.78 (672.0 examples/sec; 0.190 sec/batch)
2017-03-25 22:31:46.903135: step 38310, loss = 0.81 (796.9 examples/sec; 0.161 sec/batch)
2017-03-25 22:31:48.651522: step 38320, loss = 0.84 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 22:31:50.385510: step 38330, loss = 0.64 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:31:52.123529: step 38340, loss = 0.83 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:31:53.857212: step 38350, loss = 0.78 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:31:55.590567: step 38360, loss = 0.59 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:31:57.319779: step 38370, loss = 0.64 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:31:59.053338: step 38380, loss = 0.75 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:00.787201: step 38390, loss = 0.87 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:02.690914: step 38400, loss = 0.79 (672.4 examples/sec; 0.190 sec/batch)
2017-03-25 22:32:04.296717: step 38410, loss = 0.60 (797.1 examples/sec; 0.161 sec/batch)
2017-03-25 22:32:06.022385: step 38420, loss = 0.73 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:07.774788: step 38430, loss = 0.60 (730.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:32:09.518796: step 38440, loss = 0.71 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:32:11.261583: step 38450, loss = 0.72 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:32:12.994352: step 38460, loss = 0.70 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:14.731372: step 38470, loss = 0.83 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:32:16.457405: step 38480, loss = 0.71 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:18.197190: step 38490, loss = 0.74 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:32:20.118472: step 38500, loss = 0.82 (666.1 examples/sec; 0.192 sec/batch)
2017-03-25 22:32:21.721362: step 38510, loss = 0.97 (798.6 examples/sec; 0.160 sec/batch)
2017-03-25 22:32:23.452490: step 38520, loss = 0.82 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:25.182964: step 38530, loss = 0.67 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:26.924784: step 38540, loss = 0.57 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:32:28.656856: step 38550, loss = 0.72 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:30.382488: step 38560, loss = 0.79 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:32.124269: step 38570, loss = 0.62 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:32:33.855255: step 38580, loss = 0.73 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:35.579753: step 38590, loss = 0.88 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:32:37.581147: step 38600, loss = 0.80 (639.5 examples/sec; 0.200 sec/batch)
2017-03-25 22:32:39.103321: step 38610, loss = 0.64 (841.3 examples/sec; 0.152 sec/batch)
2017-03-25 22:32:40.836763: step 38620, loss = 0.63 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:42.579619: step 38630, loss = 0.88 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:32:44.306638: step 38640, loss = 0.69 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:46.041389: step 38650, loss = 0.77 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:47.786024: step 38660, loss = 0.89 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:32:49.523724: step 38670, loss = 0.53 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:32:51.258481: step 38680, loss = 0.75 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:52.992607: step 38690, loss = 0.72 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:32:54.886382: step 38700, loss = 0.88 (675.9 examples/sec; 0.189 sec/batch)
2017-03-25 22:32:56.500482: step 38710, loss = 0.79 (793.0 examples/sec; 0.161 sec/batch)
2017-03-25 22:32:58.247333: step 38720, loss = 0.70 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 22:32:59.973197: step 38730, loss = 0.76 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:33:01.725036: step 38740, loss = 0.64 (730.7 examples/sec; 0.175 sec/batch)
2017-03-25 22:33:03.456188: step 38750, loss = 0.77 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:33:05.183910: step 38760, loss = 0.64 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:33:06.915877: step 38770, loss = 0.70 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:33:08.638231: step 38780, loss = 0.95 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:33:10.385857: step 38790, loss = 0.78 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:33:12.271524: step 38800, loss = 0.73 (678.8 examples/sec; 0.189 sec/batch)
2017-03-25 22:33:13.893337: step 38810, loss = 0.77 (789.2 examples/sec; 0.162 sec/batch)
2017-03-25 22:33:15.638574: step 38820, loss = 0.80 (733.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:33:17.355806: step 38830, loss = 0.78 (745.8 examples/sec; 0.172 sec/batch)
2017-03-25 22:33:19.092471: step 38840, loss = 0.84 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:33:20.821444: step 38850, loss = 0.80 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:33:22.549248: step 38860, loss = 0.69 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:33:24.278924: step 38870, loss = 0.68 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:33:26.014339: step 38880, loss = 0.85 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:33:27.761016: step 38890, loss = 0.73 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 22:33:29.724242: step 38900, loss = 0.72 (652.0 examples/sec; 0.196 sec/batch)
2017-03-25 22:33:31.223301: step 38910, loss = 0.63 (853.9 examples/sec; 0.150 sec/batch)
2017-03-25 22:33:32.912376: step 38920, loss = 0.88 (757.8 examples/sec; 0.169 sec/batch)
2017-03-25 22:33:34.638249: step 38930, loss = 0.77 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:33:36.361672: step 38940, loss = 0.79 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:33:38.097243: step 38950, loss = 0.91 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:33:39.821754: step 38960, loss = 0.68 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:33:41.559624: step 38970, loss = 0.80 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:33:43.297830: step 38980, loss = 0.79 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:33:45.034860: step 38990, loss = 0.69 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:33:47.005581: step 39000, loss = 0.67 (649.7 examples/sec; 0.197 sec/batch)
2017-03-25 22:33:48.546559: step 39010, loss = 0.68 (830.3 examples/sec; 0.154 sec/batch)
2017-03-25 22:33:50.272562: step 39020, loss = 0.85 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:33:52.003137: step 39030, loss = 0.70 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:33:53.747109: step 39040, loss = 0.85 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:33:55.481807: step 39050, loss = 0.80 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:33:57.227335: step 39060, loss = 0.88 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 22:33:58.964911: step 39070, loss = 0.75 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:00.695884: step 39080, loss = 0.73 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:34:02.435346: step 39090, loss = 0.80 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:04.329448: step 39100, loss = 0.69 (675.8 examples/sec; 0.189 sec/batch)
2017-03-25 22:34:05.943256: step 39110, loss = 0.67 (793.2 examples/sec; 0.161 sec/batch)
2017-03-25 22:34:07.680495: step 39120, loss = 0.84 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:09.415724: step 39130, loss = 0.65 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:11.156809: step 39140, loss = 0.85 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:12.881860: step 39150, loss = 0.62 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:34:14.610316: step 39160, loss = 0.79 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:34:16.344646: step 39170, loss = 0.64 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:34:18.073526: step 39180, loss = 0.64 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:34:19.811164: step 39190, loss = 0.69 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:21.766720: step 39200, loss = 0.84 (654.5 examples/sec; 0.196 sec/batch)
2017-03-25 22:34:23.316985: step 39210, loss = 0.78 (825.7 examples/sec; 0.155 sec/batch)
2017-03-25 22:34:25.051396: step 39220, loss = 0.89 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:34:26.791290: step 39230, loss = 0.77 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:28.537783: step 39240, loss = 0.94 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:34:30.279772: step 39250, loss = 0.71 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:32.007554: step 39260, loss = 0.87 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:34:33.741250: step 39270, loss = 0.66 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:34:35.471202: step 39280, loss = 0.81 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:34:37.207811: step 39290, loss = 0.66 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:39.163762: step 39300, loss = 0.81 (654.4 examples/sec; 0.196 sec/batch)
2017-03-25 22:34:40.720906: step 39310, loss = 0.67 (822.0 examples/sec; 0.156 sec/batch)
2017-03-25 22:34:42.459949: step 39320, loss = 0.80 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:44.191131: step 39330, loss = 0.86 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:34:45.929668: step 39340, loss = 0.85 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:47.672929: step 39350, loss = 0.74 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:49.412470: step 39360, loss = 0.62 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:51.150785: step 39370, loss = 0.73 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:52.883899: step 39380, loss = 0.50 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:34:54.623564: step 39390, loss = 0.61 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:34:56.504461: step 39400, loss = 0.66 (680.6 examples/sec; 0.188 sec/batch)
2017-03-25 22:34:58.132116: step 39410, loss = 0.76 (786.3 examples/sec; 0.163 sec/batch)
2017-03-25 22:34:59.853957: step 39420, loss = 0.69 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:35:01.594707: step 39430, loss = 0.81 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:35:03.339728: step 39440, loss = 0.88 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:35:05.081590: step 39450, loss = 0.93 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:35:06.822769: step 39460, loss = 0.70 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:35:08.555002: step 39470, loss = 0.67 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:35:10.298685: step 39480, loss = 0.81 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:35:12.035926: step 39490, loss = 0.64 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:35:13.920118: step 39500, loss = 0.65 (679.3 examples/sec; 0.188 sec/batch)
2017-03-25 22:35:15.535971: step 39510, loss = 0.70 (792.1 examples/sec; 0.162 sec/batch)
2017-03-25 22:35:17.277969: step 39520, loss = 0.79 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:35:19.004300: step 39530, loss = 0.70 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:35:20.733837: step 39540, loss = 0.86 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:35:22.465765: step 39550, loss = 0.78 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:35:24.191458: step 39560, loss = 0.82 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:35:25.925328: step 39570, loss = 0.68 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:35:27.667151: step 39580, loss = 0.76 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:35:29.393694: step 39590, loss = 0.82 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:35:31.275802: step 39600, loss = 0.80 (680.2 examples/sec; 0.188 sec/batch)
2017-03-25 22:35:32.885376: step 39610, loss = 0.77 (794.9 examples/sec; 0.161 sec/batch)
2017-03-25 22:35:34.618951: step 39620, loss = 0.67 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:35:36.350657: step 39630, loss = 0.95 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:35:38.082994: step 39640, loss = 0.93 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:35:39.802153: step 39650, loss = 0.63 (744.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:35:41.537127: step 39660, loss = 0.74 (737.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:35:43.286648: step 39670, loss = 0.75 (731.6 examples/sec; 0.175 sec/batch)
2017-03-25 22:35:45.022037: step 39680, loss = 0.78 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:35:46.778198: step 39690, loss = 0.85 (728.9 examples/sec; 0.176 sec/batch)
2017-03-25 22:35:48.672544: step 39700, loss = 0.80 (676.0 examples/sec; 0.189 sec/batch)
2017-03-25 22:35:50.303306: step 39710, loss = 0.84 (784.5 examples/sec; 0.163 sec/batch)
2017-03-25 22:35:52.047497: step 39720, loss = 0.63 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:35:53.780763: step 39730, loss = 0.76 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:35:55.516052: step 39740, loss = 0.82 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:35:57.256614: step 39750, loss = 0.87 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:35:58.993407: step 39760, loss = 0.68 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:36:00.739260: step 39770, loss = 0.82 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 22:36:02.475935: step 39780, loss = 0.80 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:36:04.211829: step 39790, loss = 0.74 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:36:06.141897: step 39800, loss = 0.75 (663.2 examples/sec; 0.193 sec/batch)
2017-03-25 22:36:07.662272: step 39810, loss = 0.82 (841.9 examples/sec; 0.152 sec/batch)
2017-03-25 22:36:09.370688: step 39820, loss = 0.80 (749.2 examples/sec; 0.171 sec/batch)
2017-03-25 22:36:11.120042: step 39830, loss = 0.74 (731.7 examples/sec; 0.175 sec/batch)
2017-03-25 22:36:12.853342: step 39840, loss = 0.78 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:36:14.587181: step 39850, loss = 0.77 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:36:16.320669: step 39860, loss = 0.78 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:36:18.059543: step 39870, loss = 0.71 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:36:19.788743: step 39880, loss = 0.85 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:36:21.520512: step 39890, loss = 0.70 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:36:23.431417: step 39900, loss = 0.78 (669.8 examples/sec; 0.191 sec/batch)
2017-03-25 22:36:25.021891: step 39910, loss = 0.78 (804.8 examples/sec; 0.159 sec/batch)
2017-03-25 22:36:26.763913: step 39920, loss = 0.83 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:36:28.512666: step 39930, loss = 0.65 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:36:30.242117: step 39940, loss = 0.62 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:36:31.970662: step 39950, loss = 0.77 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:36:33.691999: step 39960, loss = 0.80 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:36:35.430045: step 39970, loss = 0.87 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:36:37.160009: step 39980, loss = 0.61 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:36:38.891132: step 39990, loss = 0.84 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:36:40.790250: step 40000, loss = 0.69 (674.4 examples/sec; 0.190 sec/batch)
2017-03-25 22:36:42.403900: step 40010, loss = 0.90 (792.8 examples/sec; 0.161 sec/batch)
2017-03-25 22:36:44.126838: step 40020, loss = 0.63 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 22:36:45.864232: step 40030, loss = 0.76 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:36:47.587261: step 40040, loss = 0.76 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:36:49.334730: step 40050, loss = 0.70 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:36:51.076059: step 40060, loss = 0.84 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:36:52.797520: step 40070, loss = 0.83 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:36:54.541780: step 40080, loss = 0.65 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:36:56.284174: step 40090, loss = 0.70 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:36:58.264870: step 40100, loss = 0.92 (646.2 examples/sec; 0.198 sec/batch)
2017-03-25 22:36:59.737904: step 40110, loss = 0.82 (869.0 examples/sec; 0.147 sec/batch)
2017-03-25 22:37:01.440757: step 40120, loss = 0.82 (751.7 examples/sec; 0.170 sec/batch)
2017-03-25 22:37:03.192719: step 40130, loss = 0.72 (730.6 examples/sec; 0.175 sec/batch)
2017-03-25 22:37:04.922559: step 40140, loss = 0.74 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:37:06.653605: step 40150, loss = 0.75 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:37:08.396261: step 40160, loss = 0.65 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:37:10.134472: step 40170, loss = 0.77 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:37:11.879431: step 40180, loss = 0.79 (733.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:37:13.611907: step 40190, loss = 1.08 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:37:15.552948: step 40200, loss = 0.70 (659.3 examples/sec; 0.194 sec/batch)
2017-03-25 22:37:17.102170: step 40210, loss = 0.65 (826.1 examples/sec; 0.155 sec/batch)
2017-03-25 22:37:18.834102: step 40220, loss = 0.84 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:37:20.563527: step 40230, loss = 0.79 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:37:22.296460: step 40240, loss = 0.70 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:37:24.034449: step 40250, loss = 0.63 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:37:25.769774: step 40260, loss = 0.70 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:37:27.505887: step 40270, loss = 0.55 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:37:29.250681: step 40280, loss = 0.69 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:37:30.980402: step 40290, loss = 0.78 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:37:32.911990: step 40300, loss = 0.77 (662.7 examples/sec; 0.193 sec/batch)
2017-03-25 22:37:34.451836: step 40310, loss = 0.57 (831.2 examples/sec; 0.154 sec/batch)
2017-03-25 22:37:36.174318: step 40320, loss = 0.77 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:37:37.874911: step 40330, loss = 0.66 (752.7 examples/sec; 0.170 sec/batch)
2017-03-25 22:37:39.576155: step 40340, loss = 0.78 (752.4 examples/sec; 0.170 sec/batch)
2017-03-25 22:37:41.276857: step 40350, loss = 0.83 (752.7 examples/sec; 0.170 sec/batch)
2017-03-25 22:37:42.972708: step 40360, loss = 0.69 (754.7 examples/sec; 0.170 sec/batch)
2017-03-25 22:37:44.673256: step 40370, loss = 0.93 (752.7 examples/sec; 0.170 sec/batch)
2017-03-25 22:37:46.372159: step 40380, loss = 0.82 (753.5 examples/sec; 0.170 sec/batch)
2017-03-25 22:37:48.102190: step 40390, loss = 0.98 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:37:49.996487: step 40400, loss = 0.68 (675.7 examples/sec; 0.189 sec/batch)
2017-03-25 22:37:51.614700: step 40410, loss = 0.86 (791.0 examples/sec; 0.162 sec/batch)
2017-03-25 22:37:53.336427: step 40420, loss = 0.84 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:37:55.073008: step 40430, loss = 0.74 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:37:56.811418: step 40440, loss = 0.81 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:37:58.542083: step 40450, loss = 0.77 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:00.288725: step 40460, loss = 0.86 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 22:38:02.033033: step 40470, loss = 0.69 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:38:03.755786: step 40480, loss = 0.96 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:38:05.499108: step 40490, loss = 0.72 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:38:07.393975: step 40500, loss = 0.69 (675.5 examples/sec; 0.189 sec/batch)
2017-03-25 22:38:09.002509: step 40510, loss = 0.83 (795.8 examples/sec; 0.161 sec/batch)
2017-03-25 22:38:10.735517: step 40520, loss = 0.63 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:12.461465: step 40530, loss = 0.77 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:14.191598: step 40540, loss = 0.73 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:15.919194: step 40550, loss = 0.68 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:17.659950: step 40560, loss = 0.70 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:38:19.396685: step 40570, loss = 0.65 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:38:21.135417: step 40580, loss = 0.77 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:38:22.865110: step 40590, loss = 0.80 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:24.736631: step 40600, loss = 0.82 (684.2 examples/sec; 0.187 sec/batch)
2017-03-25 22:38:26.351461: step 40610, loss = 0.80 (792.6 examples/sec; 0.162 sec/batch)
2017-03-25 22:38:28.093014: step 40620, loss = 0.74 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:38:29.824977: step 40630, loss = 0.79 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:31.555090: step 40640, loss = 0.65 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:33.287955: step 40650, loss = 0.81 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:35.027907: step 40660, loss = 0.77 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:38:36.754945: step 40670, loss = 0.71 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:38.483038: step 40680, loss = 0.83 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:40.211428: step 40690, loss = 0.73 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:42.157745: step 40700, loss = 0.56 (657.7 examples/sec; 0.195 sec/batch)
2017-03-25 22:38:43.716002: step 40710, loss = 0.66 (821.4 examples/sec; 0.156 sec/batch)
2017-03-25 22:38:45.451258: step 40720, loss = 0.94 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:38:47.185168: step 40730, loss = 0.59 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:48.907592: step 40740, loss = 0.75 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:38:50.646104: step 40750, loss = 0.75 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:38:52.374929: step 40760, loss = 0.77 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:54.104397: step 40770, loss = 0.94 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:55.835754: step 40780, loss = 0.67 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:57.564699: step 40790, loss = 0.89 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:38:59.464137: step 40800, loss = 0.87 (674.3 examples/sec; 0.190 sec/batch)
2017-03-25 22:39:01.086163: step 40810, loss = 0.81 (788.6 examples/sec; 0.162 sec/batch)
2017-03-25 22:39:02.824215: step 40820, loss = 0.70 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:39:04.554767: step 40830, loss = 0.69 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:39:06.288976: step 40840, loss = 0.81 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:39:08.019124: step 40850, loss = 0.75 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:39:09.761882: step 40860, loss = 0.84 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:39:11.499012: step 40870, loss = 0.71 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:39:13.219775: step 40880, loss = 0.95 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:39:14.960189: step 40890, loss = 0.82 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:39:16.843572: step 40900, loss = 0.87 (680.0 examples/sec; 0.188 sec/batch)
2017-03-25 22:39:18.455749: step 40910, loss = 0.86 (793.3 examples/sec; 0.161 sec/batch)
2017-03-25 22:39:20.193347: step 40920, loss = 0.82 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:39:21.931749: step 40930, loss = 0.67 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:39:23.662934: step 40940, loss = 0.73 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:39:25.418580: step 40950, loss = 0.76 (729.1 examples/sec; 0.176 sec/batch)
2017-03-25 22:39:27.142380: step 40960, loss = 0.76 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:39:28.882275: step 40970, loss = 0.74 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:39:30.603821: step 40980, loss = 0.82 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:39:32.349570: step 40990, loss = 0.73 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 22:39:34.232747: step 41000, loss = 0.67 (679.7 examples/sec; 0.188 sec/batch)
2017-03-25 22:39:35.854280: step 41010, loss = 0.67 (789.4 examples/sec; 0.162 sec/batch)
2017-03-25 22:39:37.586733: step 41020, loss = 0.80 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:39:39.332230: step 41030, loss = 0.82 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:39:41.069895: step 41040, loss = 0.74 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:39:42.804703: step 41050, loss = 0.71 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:39:44.543776: step 41060, loss = 0.71 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:39:46.284172: step 41070, loss = 0.73 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:39:48.005809: step 41080, loss = 0.77 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:39:49.740706: step 41090, loss = 0.81 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:39:51.666200: step 41100, loss = 0.60 (664.8 examples/sec; 0.193 sec/batch)
2017-03-25 22:39:53.247059: step 41110, loss = 0.68 (809.7 examples/sec; 0.158 sec/batch)
2017-03-25 22:39:54.978515: step 41120, loss = 0.66 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:39:56.698129: step 41130, loss = 0.94 (744.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:39:58.435551: step 41140, loss = 0.64 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:40:00.172879: step 41150, loss = 0.56 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:40:01.894510: step 41160, loss = 0.64 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:40:03.632780: step 41170, loss = 0.81 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:40:05.362943: step 41180, loss = 0.57 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:07.087132: step 41190, loss = 0.77 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:40:08.978592: step 41200, loss = 0.76 (676.7 examples/sec; 0.189 sec/batch)
2017-03-25 22:40:10.596077: step 41210, loss = 0.78 (791.4 examples/sec; 0.162 sec/batch)
2017-03-25 22:40:12.328699: step 41220, loss = 0.79 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:14.059582: step 41230, loss = 0.74 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:15.787566: step 41240, loss = 0.66 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:17.521853: step 41250, loss = 0.76 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:19.268123: step 41260, loss = 0.84 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 22:40:21.009601: step 41270, loss = 0.79 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:40:22.736655: step 41280, loss = 0.69 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:24.470617: step 41290, loss = 0.96 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:26.454049: step 41300, loss = 0.74 (645.3 examples/sec; 0.198 sec/batch)
2017-03-25 22:40:27.927069: step 41310, loss = 0.70 (869.2 examples/sec; 0.147 sec/batch)
2017-03-25 22:40:29.626132: step 41320, loss = 0.77 (753.1 examples/sec; 0.170 sec/batch)
2017-03-25 22:40:31.360588: step 41330, loss = 0.72 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:33.098068: step 41340, loss = 0.73 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:40:34.833972: step 41350, loss = 0.65 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:40:36.567168: step 41360, loss = 0.70 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:38.295666: step 41370, loss = 0.71 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:40.023178: step 41380, loss = 0.78 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:41.743862: step 41390, loss = 0.86 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:40:43.725531: step 41400, loss = 0.67 (645.9 examples/sec; 0.198 sec/batch)
2017-03-25 22:40:45.272553: step 41410, loss = 0.67 (827.5 examples/sec; 0.155 sec/batch)
2017-03-25 22:40:46.997376: step 41420, loss = 0.64 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:48.728875: step 41430, loss = 0.65 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:50.477613: step 41440, loss = 0.76 (732.0 examples/sec; 0.175 sec/batch)
2017-03-25 22:40:52.206200: step 41450, loss = 0.84 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:53.968157: step 41460, loss = 0.54 (726.5 examples/sec; 0.176 sec/batch)
2017-03-25 22:40:55.707170: step 41470, loss = 0.73 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:40:57.435662: step 41480, loss = 0.72 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:40:59.174920: step 41490, loss = 0.89 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:41:01.128223: step 41500, loss = 0.77 (655.3 examples/sec; 0.195 sec/batch)
2017-03-25 22:41:02.681992: step 41510, loss = 0.62 (823.9 examples/sec; 0.155 sec/batch)
2017-03-25 22:41:04.398710: step 41520, loss = 0.78 (745.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:41:06.130322: step 41530, loss = 0.77 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:41:07.854916: step 41540, loss = 0.71 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:41:09.570737: step 41550, loss = 0.70 (745.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:41:11.295180: step 41560, loss = 0.76 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 22:41:13.014072: step 41570, loss = 0.88 (744.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:41:14.736017: step 41580, loss = 0.71 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 22:41:16.462132: step 41590, loss = 0.72 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:41:18.337969: step 41600, loss = 0.80 (682.4 examples/sec; 0.188 sec/batch)
2017-03-25 22:41:19.969224: step 41610, loss = 0.88 (784.7 examples/sec; 0.163 sec/batch)
2017-03-25 22:41:21.695910: step 41620, loss = 0.67 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:41:23.431324: step 41630, loss = 0.68 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:41:25.171193: step 41640, loss = 0.72 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:41:26.908095: step 41650, loss = 0.89 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:41:28.661371: step 41660, loss = 0.97 (730.1 examples/sec; 0.175 sec/batch)
2017-03-25 22:41:30.402233: step 41670, loss = 0.96 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:41:32.153416: step 41680, loss = 0.69 (730.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:41:33.871896: step 41690, loss = 0.88 (744.8 examples/sec; 0.172 sec/batch)
2017-03-25 22:41:35.764982: step 41700, loss = 0.79 (676.1 examples/sec; 0.189 sec/batch)
2017-03-25 22:41:37.379837: step 41710, loss = 0.83 (792.6 examples/sec; 0.161 sec/batch)
2017-03-25 22:41:39.107780: step 41720, loss = 0.80 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:41:40.846078: step 41730, loss = 0.72 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:41:42.586678: step 41740, loss = 0.80 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:41:44.319727: step 41750, loss = 0.66 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:41:46.059579: step 41760, loss = 0.77 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:41:47.792625: step 41770, loss = 0.75 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:41:49.532937: step 41780, loss = 0.85 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:41:51.259641: step 41790, loss = 0.76 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:41:53.179893: step 41800, loss = 0.69 (666.6 examples/sec; 0.192 sec/batch)
2017-03-25 22:41:54.777588: step 41810, loss = 0.80 (801.2 examples/sec; 0.160 sec/batch)
2017-03-25 22:41:56.509627: step 41820, loss = 0.76 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:41:58.250775: step 41830, loss = 0.77 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:41:59.989809: step 41840, loss = 0.86 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:42:01.717626: step 41850, loss = 0.77 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:42:03.455486: step 41860, loss = 0.73 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:42:05.185125: step 41870, loss = 0.67 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:42:06.929178: step 41880, loss = 0.65 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:42:08.672103: step 41890, loss = 0.75 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:42:10.577690: step 41900, loss = 0.86 (671.6 examples/sec; 0.191 sec/batch)
2017-03-25 22:42:12.206525: step 41910, loss = 0.67 (785.8 examples/sec; 0.163 sec/batch)
2017-03-25 22:42:13.940341: step 41920, loss = 0.61 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:42:15.668287: step 41930, loss = 0.85 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:42:17.407841: step 41940, loss = 0.78 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:42:19.155305: step 41950, loss = 0.76 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:42:20.896584: step 41960, loss = 0.65 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:42:22.650329: step 41970, loss = 0.69 (729.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:42:24.393202: step 41980, loss = 0.80 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:42:26.134869: step 41990, loss = 0.91 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:42:28.019432: step 42000, loss = 0.81 (679.2 examples/sec; 0.188 sec/batch)
2017-03-25 22:42:29.653906: step 42010, loss = 0.82 (783.1 examples/sec; 0.163 sec/batch)
2017-03-25 22:42:31.403919: step 42020, loss = 0.84 (731.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:42:33.123595: step 42030, loss = 0.83 (744.3 examples/sec; 0.172 sec/batch)
2017-03-25 22:42:34.861633: step 42040, loss = 0.75 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:42:36.596922: step 42050, loss = 0.94 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:42:38.325932: step 42060, loss = 0.84 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:42:40.069988: step 42070, loss = 0.76 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:42:41.797059: step 42080, loss = 0.77 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:42:43.523229: step 42090, loss = 0.80 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:42:45.475106: step 42100, loss = 0.77 (656.2 examples/sec; 0.195 sec/batch)
2017-03-25 22:42:47.001730: step 42110, loss = 0.76 (837.7 examples/sec; 0.153 sec/batch)
2017-03-25 22:42:48.706971: step 42120, loss = 0.74 (750.6 examples/sec; 0.171 sec/batch)
2017-03-25 22:42:50.439467: step 42130, loss = 0.76 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:42:52.178245: step 42140, loss = 0.92 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:42:53.904040: step 42150, loss = 0.80 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:42:55.634233: step 42160, loss = 0.70 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:42:57.368612: step 42170, loss = 0.90 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:42:59.115392: step 42180, loss = 0.63 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 22:43:00.854644: step 42190, loss = 0.72 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:43:02.760060: step 42200, loss = 0.93 (671.8 examples/sec; 0.191 sec/batch)
2017-03-25 22:43:04.383391: step 42210, loss = 0.76 (788.5 examples/sec; 0.162 sec/batch)
2017-03-25 22:43:06.111521: step 42220, loss = 0.81 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:43:07.839011: step 42230, loss = 0.83 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:43:09.556058: step 42240, loss = 0.83 (745.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:43:11.301795: step 42250, loss = 0.86 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 22:43:13.031354: step 42260, loss = 0.76 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:43:14.755315: step 42270, loss = 0.83 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:43:16.487795: step 42280, loss = 0.66 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:43:18.224506: step 42290, loss = 0.55 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:43:20.189158: step 42300, loss = 0.73 (651.5 examples/sec; 0.196 sec/batch)
2017-03-25 22:43:21.723942: step 42310, loss = 0.72 (834.0 examples/sec; 0.153 sec/batch)
2017-03-25 22:43:23.456988: step 42320, loss = 0.68 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:43:25.177531: step 42330, loss = 0.80 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:43:26.916406: step 42340, loss = 0.71 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:43:28.653479: step 42350, loss = 0.82 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:43:30.382767: step 42360, loss = 0.59 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:43:32.108369: step 42370, loss = 0.74 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:43:33.847523: step 42380, loss = 0.95 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:43:35.585813: step 42390, loss = 0.71 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:43:37.478713: step 42400, loss = 0.86 (676.2 examples/sec; 0.189 sec/batch)
2017-03-25 22:43:39.102574: step 42410, loss = 0.75 (788.3 examples/sec; 0.162 sec/batch)
2017-03-25 22:43:40.841864: step 42420, loss = 0.66 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:43:42.585414: step 42430, loss = 0.72 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:43:44.322883: step 42440, loss = 0.67 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:43:46.044021: step 42450, loss = 0.73 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:43:47.771583: step 42460, loss = 0.88 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:43:49.511198: step 42470, loss = 0.82 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:43:51.241523: step 42480, loss = 0.75 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:43:52.973764: step 42490, loss = 0.68 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:43:54.875272: step 42500, loss = 0.70 (673.5 examples/sec; 0.190 sec/batch)
2017-03-25 22:43:56.480609: step 42510, loss = 0.90 (797.0 examples/sec; 0.161 sec/batch)
2017-03-25 22:43:58.224064: step 42520, loss = 0.90 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:43:59.944863: step 42530, loss = 0.72 (744.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:44:01.686450: step 42540, loss = 0.94 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:44:03.415189: step 42550, loss = 0.81 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:44:05.159854: step 42560, loss = 0.80 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:44:06.894573: step 42570, loss = 0.90 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:44:08.626718: step 42580, loss = 0.66 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:44:10.365165: step 42590, loss = 0.84 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:44:12.274175: step 42600, loss = 0.52 (670.7 examples/sec; 0.191 sec/batch)
2017-03-25 22:44:13.856458: step 42610, loss = 0.62 (808.8 examples/sec; 0.158 sec/batch)
2017-03-25 22:44:15.575347: step 42620, loss = 0.80 (744.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:44:17.310910: step 42630, loss = 0.65 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:44:19.036210: step 42640, loss = 0.76 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:44:20.772671: step 42650, loss = 0.74 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:44:22.491040: step 42660, loss = 0.75 (744.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:44:24.241416: step 42670, loss = 0.71 (731.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:44:25.963306: step 42680, loss = 0.91 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:44:27.700498: step 42690, loss = 0.82 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:44:29.581319: step 42700, loss = 0.74 (680.9 examples/sec; 0.188 sec/batch)
2017-03-25 22:44:31.209835: step 42710, loss = 0.64 (785.5 examples/sec; 0.163 sec/batch)
2017-03-25 22:44:32.939042: step 42720, loss = 0.84 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:44:34.680119: step 42730, loss = 0.84 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:44:36.409735: step 42740, loss = 0.70 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:44:38.151734: step 42750, loss = 0.78 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:44:39.885696: step 42760, loss = 0.81 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:44:41.611341: step 42770, loss = 0.71 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:44:43.340321: step 42780, loss = 0.68 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:44:45.076009: step 42790, loss = 0.77 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:44:46.981707: step 42800, loss = 0.76 (671.7 examples/sec; 0.191 sec/batch)
2017-03-25 22:44:48.597083: step 42810, loss = 0.82 (792.4 examples/sec; 0.162 sec/batch)
2017-03-25 22:44:50.333472: step 42820, loss = 0.86 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:44:52.063199: step 42830, loss = 0.70 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:44:53.794736: step 42840, loss = 0.79 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:44:55.533387: step 42850, loss = 0.66 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:44:57.272487: step 42860, loss = 0.70 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:44:59.007801: step 42870, loss = 0.78 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:45:00.747009: step 42880, loss = 0.66 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:45:02.476118: step 42890, loss = 0.64 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:45:04.456911: step 42900, loss = 0.71 (646.4 examples/sec; 0.198 sec/batch)
2017-03-25 22:45:05.991249: step 42910, loss = 0.78 (833.9 examples/sec; 0.154 sec/batch)
2017-03-25 22:45:07.698810: step 42920, loss = 0.70 (749.6 examples/sec; 0.171 sec/batch)
2017-03-25 22:45:09.435008: step 42930, loss = 0.84 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:45:11.185260: step 42940, loss = 0.76 (731.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:45:12.919388: step 42950, loss = 0.75 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:45:14.646504: step 42960, loss = 0.72 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:45:16.384762: step 42970, loss = 0.75 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:45:18.107884: step 42980, loss = 0.71 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 22:45:19.852901: step 42990, loss = 0.89 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:45:21.755071: step 43000, loss = 0.73 (672.9 examples/sec; 0.190 sec/batch)
2017-03-25 22:45:23.368318: step 43010, loss = 0.86 (793.4 examples/sec; 0.161 sec/batch)
2017-03-25 22:45:25.106905: step 43020, loss = 0.85 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:45:26.842916: step 43030, loss = 0.92 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:45:28.580887: step 43040, loss = 0.76 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:45:30.312224: step 43050, loss = 0.99 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:45:32.053656: step 43060, loss = 0.73 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:45:33.788233: step 43070, loss = 0.87 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:45:35.538065: step 43080, loss = 0.81 (731.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:45:37.271738: step 43090, loss = 0.98 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:45:39.229030: step 43100, loss = 0.81 (653.9 examples/sec; 0.196 sec/batch)
2017-03-25 22:45:40.786181: step 43110, loss = 0.83 (822.0 examples/sec; 0.156 sec/batch)
2017-03-25 22:45:42.535223: step 43120, loss = 0.80 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:45:44.265966: step 43130, loss = 0.71 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:45:45.998172: step 43140, loss = 0.72 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:45:47.728710: step 43150, loss = 0.73 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:45:49.460298: step 43160, loss = 0.85 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:45:51.195632: step 43170, loss = 0.76 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:45:52.929487: step 43180, loss = 0.73 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:45:54.667053: step 43190, loss = 0.79 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:45:56.566860: step 43200, loss = 0.64 (673.8 examples/sec; 0.190 sec/batch)
2017-03-25 22:45:58.191634: step 43210, loss = 0.79 (787.8 examples/sec; 0.162 sec/batch)
2017-03-25 22:45:59.921573: step 43220, loss = 0.76 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:46:01.658473: step 43230, loss = 0.69 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:46:03.402022: step 43240, loss = 0.75 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:46:05.136712: step 43250, loss = 0.86 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:46:06.865520: step 43260, loss = 0.73 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:46:08.593584: step 43270, loss = 0.69 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:46:10.353192: step 43280, loss = 0.65 (727.4 examples/sec; 0.176 sec/batch)
2017-03-25 22:46:12.072993: step 43290, loss = 0.80 (744.3 examples/sec; 0.172 sec/batch)
2017-03-25 22:46:13.979332: step 43300, loss = 0.87 (671.4 examples/sec; 0.191 sec/batch)
2017-03-25 22:46:15.582142: step 43310, loss = 0.88 (798.6 examples/sec; 0.160 sec/batch)
2017-03-25 22:46:17.311260: step 43320, loss = 0.71 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:46:19.050404: step 43330, loss = 0.74 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:46:20.777385: step 43340, loss = 0.89 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:46:22.513261: step 43350, loss = 0.79 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:46:24.241109: step 43360, loss = 0.62 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:46:25.980839: step 43370, loss = 0.84 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:46:27.714163: step 43380, loss = 0.78 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:46:29.440971: step 43390, loss = 0.83 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:46:31.344244: step 43400, loss = 0.70 (672.5 examples/sec; 0.190 sec/batch)
2017-03-25 22:46:32.951844: step 43410, loss = 0.78 (796.2 examples/sec; 0.161 sec/batch)
2017-03-25 22:46:34.684475: step 43420, loss = 0.61 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:46:36.408769: step 43430, loss = 0.78 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:46:38.151339: step 43440, loss = 0.68 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:46:39.872471: step 43450, loss = 0.66 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:46:41.619923: step 43460, loss = 0.58 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:46:43.369781: step 43470, loss = 0.81 (731.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:46:45.103611: step 43480, loss = 0.85 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:46:46.849203: step 43490, loss = 0.58 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:46:48.832852: step 43500, loss = 0.78 (645.3 examples/sec; 0.198 sec/batch)
2017-03-25 22:46:50.320411: step 43510, loss = 0.87 (860.5 examples/sec; 0.149 sec/batch)
2017-03-25 22:46:52.025324: step 43520, loss = 0.84 (750.8 examples/sec; 0.170 sec/batch)
2017-03-25 22:46:53.761884: step 43530, loss = 0.70 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:46:55.492184: step 43540, loss = 0.77 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:46:57.239031: step 43550, loss = 0.70 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:46:58.976426: step 43560, loss = 0.97 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:47:00.716715: step 43570, loss = 0.65 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:47:02.454564: step 43580, loss = 0.84 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:47:04.201161: step 43590, loss = 0.63 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:47:06.097769: step 43600, loss = 0.74 (674.9 examples/sec; 0.190 sec/batch)
2017-03-25 22:47:07.714428: step 43610, loss = 0.83 (791.8 examples/sec; 0.162 sec/batch)
2017-03-25 22:47:09.446479: step 43620, loss = 0.73 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:11.191837: step 43630, loss = 0.57 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:47:12.924141: step 43640, loss = 0.64 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:14.667252: step 43650, loss = 0.67 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:47:16.398482: step 43660, loss = 0.77 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:18.125257: step 43670, loss = 0.76 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:19.857624: step 43680, loss = 0.72 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:21.589341: step 43690, loss = 0.65 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:23.485236: step 43700, loss = 0.76 (675.1 examples/sec; 0.190 sec/batch)
2017-03-25 22:47:25.109040: step 43710, loss = 0.87 (788.4 examples/sec; 0.162 sec/batch)
2017-03-25 22:47:26.841187: step 43720, loss = 0.71 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:28.579617: step 43730, loss = 0.72 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:47:30.309135: step 43740, loss = 0.76 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:32.035431: step 43750, loss = 0.69 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:33.770290: step 43760, loss = 0.65 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:35.509003: step 43770, loss = 0.80 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:47:37.243039: step 43780, loss = 0.80 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:38.976216: step 43790, loss = 0.76 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:40.853912: step 43800, loss = 0.68 (682.0 examples/sec; 0.188 sec/batch)
2017-03-25 22:47:42.470090: step 43810, loss = 0.64 (791.7 examples/sec; 0.162 sec/batch)
2017-03-25 22:47:44.196930: step 43820, loss = 0.82 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:45.932399: step 43830, loss = 0.78 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:47:47.666708: step 43840, loss = 0.57 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:49.399177: step 43850, loss = 1.00 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:51.141517: step 43860, loss = 0.81 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:47:52.877457: step 43870, loss = 0.88 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:47:54.612978: step 43880, loss = 0.73 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:47:56.339550: step 43890, loss = 0.71 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:47:58.213200: step 43900, loss = 0.69 (683.1 examples/sec; 0.187 sec/batch)
2017-03-25 22:47:59.832594: step 43910, loss = 0.86 (790.4 examples/sec; 0.162 sec/batch)
2017-03-25 22:48:01.570537: step 43920, loss = 0.69 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:48:03.305920: step 43930, loss = 0.69 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:48:05.026229: step 43940, loss = 0.73 (744.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:48:06.763657: step 43950, loss = 0.71 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:48:08.498560: step 43960, loss = 0.61 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:48:10.231863: step 43970, loss = 0.79 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:48:11.962792: step 43980, loss = 0.81 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:48:13.703921: step 43990, loss = 0.71 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:48:15.593676: step 44000, loss = 0.85 (677.7 examples/sec; 0.189 sec/batch)
2017-03-25 22:48:17.186601: step 44010, loss = 0.81 (803.1 examples/sec; 0.159 sec/batch)
2017-03-25 22:48:18.912290: step 44020, loss = 0.79 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:48:20.645645: step 44030, loss = 0.75 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:48:22.397849: step 44040, loss = 0.73 (730.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:48:24.121042: step 44050, loss = 0.76 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 22:48:25.843284: step 44060, loss = 0.78 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:48:27.572852: step 44070, loss = 0.82 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:48:29.305870: step 44080, loss = 0.84 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:48:31.057094: step 44090, loss = 0.61 (731.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:48:32.924548: step 44100, loss = 0.80 (685.0 examples/sec; 0.187 sec/batch)
2017-03-25 22:48:34.548721: step 44110, loss = 0.83 (788.1 examples/sec; 0.162 sec/batch)
2017-03-25 22:48:36.298470: step 44120, loss = 0.78 (731.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:48:38.035136: step 44130, loss = 0.94 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:48:39.763749: step 44140, loss = 0.72 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:48:41.499902: step 44150, loss = 0.78 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:48:43.243662: step 44160, loss = 0.66 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:48:44.977941: step 44170, loss = 0.76 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:48:46.706857: step 44180, loss = 0.79 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:48:48.436399: step 44190, loss = 0.75 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:48:50.321961: step 44200, loss = 0.88 (678.8 examples/sec; 0.189 sec/batch)
2017-03-25 22:48:51.931008: step 44210, loss = 0.79 (795.5 examples/sec; 0.161 sec/batch)
2017-03-25 22:48:53.663379: step 44220, loss = 0.83 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:48:55.390190: step 44230, loss = 0.74 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:48:57.126619: step 44240, loss = 0.71 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:48:58.865075: step 44250, loss = 0.77 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:49:00.603677: step 44260, loss = 0.78 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:49:02.340768: step 44270, loss = 0.63 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:49:04.069844: step 44280, loss = 0.88 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:49:05.807917: step 44290, loss = 0.79 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:49:07.704949: step 44300, loss = 0.74 (674.7 examples/sec; 0.190 sec/batch)
2017-03-25 22:49:09.323379: step 44310, loss = 0.77 (790.9 examples/sec; 0.162 sec/batch)
2017-03-25 22:49:11.064396: step 44320, loss = 0.84 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:49:12.795041: step 44330, loss = 0.70 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:49:14.527132: step 44340, loss = 0.87 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:49:16.250924: step 44350, loss = 0.74 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:49:17.972753: step 44360, loss = 0.80 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:49:19.702800: step 44370, loss = 0.87 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:49:21.438814: step 44380, loss = 0.77 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:49:23.181196: step 44390, loss = 0.71 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:49:25.055996: step 44400, loss = 0.64 (682.9 examples/sec; 0.187 sec/batch)
2017-03-25 22:49:26.691790: step 44410, loss = 0.78 (782.3 examples/sec; 0.164 sec/batch)
2017-03-25 22:49:28.429503: step 44420, loss = 0.75 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:49:30.160994: step 44430, loss = 0.93 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:49:31.891000: step 44440, loss = 0.72 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:49:33.632225: step 44450, loss = 0.68 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:49:35.358082: step 44460, loss = 0.75 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:49:37.096804: step 44470, loss = 0.84 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:49:38.837857: step 44480, loss = 0.69 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:49:40.562700: step 44490, loss = 0.72 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:49:42.450787: step 44500, loss = 0.82 (677.9 examples/sec; 0.189 sec/batch)
2017-03-25 22:49:44.066685: step 44510, loss = 0.70 (792.1 examples/sec; 0.162 sec/batch)
2017-03-25 22:49:45.794818: step 44520, loss = 0.77 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:49:47.533613: step 44530, loss = 0.61 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:49:49.266946: step 44540, loss = 0.78 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:49:50.993648: step 44550, loss = 0.77 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:49:52.728754: step 44560, loss = 0.74 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:49:54.458898: step 44570, loss = 0.70 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:49:56.190886: step 44580, loss = 0.79 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:49:57.923208: step 44590, loss = 0.79 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:49:59.824715: step 44600, loss = 0.78 (673.2 examples/sec; 0.190 sec/batch)
2017-03-25 22:50:01.426436: step 44610, loss = 0.76 (799.1 examples/sec; 0.160 sec/batch)
2017-03-25 22:50:03.159444: step 44620, loss = 0.92 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:04.900992: step 44630, loss = 0.68 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:50:06.635452: step 44640, loss = 0.87 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:08.370234: step 44650, loss = 0.74 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:10.092998: step 44660, loss = 0.80 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:50:11.816351: step 44670, loss = 0.76 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:50:13.553522: step 44680, loss = 0.73 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:50:15.281659: step 44690, loss = 0.86 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:17.186619: step 44700, loss = 0.71 (672.2 examples/sec; 0.190 sec/batch)
2017-03-25 22:50:18.765110: step 44710, loss = 0.76 (810.6 examples/sec; 0.158 sec/batch)
2017-03-25 22:50:20.497670: step 44720, loss = 0.83 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:22.233242: step 44730, loss = 0.82 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:50:23.965016: step 44740, loss = 0.75 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:25.694647: step 44750, loss = 0.86 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:27.426583: step 44760, loss = 0.67 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:29.157493: step 44770, loss = 0.79 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:30.892962: step 44780, loss = 0.85 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:50:32.619557: step 44790, loss = 0.70 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:34.518737: step 44800, loss = 0.77 (674.0 examples/sec; 0.190 sec/batch)
2017-03-25 22:50:36.116369: step 44810, loss = 0.66 (801.2 examples/sec; 0.160 sec/batch)
2017-03-25 22:50:37.857873: step 44820, loss = 0.74 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:50:39.599607: step 44830, loss = 0.77 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:50:41.344676: step 44840, loss = 0.69 (733.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:50:43.072155: step 44850, loss = 0.67 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:44.802975: step 44860, loss = 0.81 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:46.525782: step 44870, loss = 0.62 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:50:48.252210: step 44880, loss = 0.79 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:49.978666: step 44890, loss = 0.60 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:51.883153: step 44900, loss = 0.61 (672.1 examples/sec; 0.190 sec/batch)
2017-03-25 22:50:53.510029: step 44910, loss = 0.86 (786.8 examples/sec; 0.163 sec/batch)
2017-03-25 22:50:55.244767: step 44920, loss = 0.77 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:56.972038: step 44930, loss = 0.59 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:50:58.702622: step 44940, loss = 0.80 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:51:00.435233: step 44950, loss = 0.82 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:51:02.172146: step 44960, loss = 0.74 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:51:03.907955: step 44970, loss = 0.74 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:51:05.643467: step 44980, loss = 0.76 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:51:07.376777: step 44990, loss = 0.77 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:51:09.309782: step 45000, loss = 0.75 (662.2 examples/sec; 0.193 sec/batch)
2017-03-25 22:51:10.904026: step 45010, loss = 0.61 (802.9 examples/sec; 0.159 sec/batch)
2017-03-25 22:51:12.637189: step 45020, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:51:14.384001: step 45030, loss = 0.88 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 22:51:16.116040: step 45040, loss = 0.81 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:51:17.855230: step 45050, loss = 0.85 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:51:19.587195: step 45060, loss = 0.72 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:51:21.326372: step 45070, loss = 0.65 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:51:23.042031: step 45080, loss = 0.76 (746.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:51:24.781523: step 45090, loss = 0.81 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:51:26.659417: step 45100, loss = 0.77 (681.6 examples/sec; 0.188 sec/batch)
2017-03-25 22:51:28.281059: step 45110, loss = 0.71 (789.3 examples/sec; 0.162 sec/batch)
2017-03-25 22:51:30.018939: step 45120, loss = 0.85 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:51:31.766580: step 45130, loss = 0.74 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:51:33.495450: step 45140, loss = 0.84 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:51:35.240964: step 45150, loss = 0.76 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:51:36.964336: step 45160, loss = 0.72 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:51:38.706071: step 45170, loss = 0.72 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:51:40.439248: step 45180, loss = 0.66 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:51:42.168010: step 45190, loss = 0.77 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:51:44.059508: step 45200, loss = 0.82 (676.7 examples/sec; 0.189 sec/batch)
2017-03-25 22:51:45.672192: step 45210, loss = 0.87 (793.7 examples/sec; 0.161 sec/batch)
2017-03-25 22:51:47.392853: step 45220, loss = 0.83 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:51:49.138533: step 45230, loss = 0.68 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 22:51:50.872325: step 45240, loss = 0.81 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:51:52.603075: step 45250, loss = 0.75 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:51:54.351597: step 45260, loss = 0.77 (732.0 examples/sec; 0.175 sec/batch)
2017-03-25 22:51:56.087747: step 45270, loss = 0.83 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:51:57.813259: step 45280, loss = 0.89 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:51:59.551811: step 45290, loss = 0.69 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:52:01.437447: step 45300, loss = 0.63 (678.8 examples/sec; 0.189 sec/batch)
2017-03-25 22:52:03.061917: step 45310, loss = 0.75 (787.9 examples/sec; 0.162 sec/batch)
2017-03-25 22:52:04.792291: step 45320, loss = 0.84 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:52:06.524471: step 45330, loss = 0.84 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:52:08.258748: step 45340, loss = 0.72 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:52:09.998942: step 45350, loss = 0.64 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:52:11.736106: step 45360, loss = 0.84 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:52:13.461416: step 45370, loss = 0.71 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:52:15.217641: step 45380, loss = 0.71 (728.8 examples/sec; 0.176 sec/batch)
2017-03-25 22:52:16.951626: step 45390, loss = 0.86 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:52:18.845426: step 45400, loss = 0.66 (675.9 examples/sec; 0.189 sec/batch)
2017-03-25 22:52:20.439263: step 45410, loss = 0.63 (803.1 examples/sec; 0.159 sec/batch)
2017-03-25 22:52:22.166386: step 45420, loss = 0.76 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:52:23.894749: step 45430, loss = 0.82 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:52:25.638002: step 45440, loss = 0.69 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:52:27.369658: step 45450, loss = 0.75 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:52:29.117114: step 45460, loss = 0.78 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 22:52:30.842634: step 45470, loss = 0.70 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:52:32.565168: step 45480, loss = 0.69 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:52:34.294626: step 45490, loss = 0.78 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:52:36.203962: step 45500, loss = 0.85 (670.1 examples/sec; 0.191 sec/batch)
2017-03-25 22:52:37.816511: step 45510, loss = 0.76 (793.8 examples/sec; 0.161 sec/batch)
2017-03-25 22:52:39.567690: step 45520, loss = 0.70 (730.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:52:41.289598: step 45530, loss = 0.71 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:52:43.025147: step 45540, loss = 0.92 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:52:44.767282: step 45550, loss = 0.77 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:52:46.496111: step 45560, loss = 0.83 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:52:48.224619: step 45570, loss = 0.76 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:52:49.965073: step 45580, loss = 0.74 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:52:51.696809: step 45590, loss = 0.86 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:52:53.607031: step 45600, loss = 0.67 (670.3 examples/sec; 0.191 sec/batch)
2017-03-25 22:52:55.197648: step 45610, loss = 0.68 (804.4 examples/sec; 0.159 sec/batch)
2017-03-25 22:52:56.936148: step 45620, loss = 0.77 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:52:58.676164: step 45630, loss = 0.70 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:53:00.397385: step 45640, loss = 0.75 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:53:02.131550: step 45650, loss = 0.68 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:53:03.871325: step 45660, loss = 0.76 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:53:05.602716: step 45670, loss = 0.72 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:53:07.348169: step 45680, loss = 0.80 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:53:09.071347: step 45690, loss = 0.75 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:53:11.031086: step 45700, loss = 0.88 (653.1 examples/sec; 0.196 sec/batch)
2017-03-25 22:53:12.571407: step 45710, loss = 0.83 (831.0 examples/sec; 0.154 sec/batch)
2017-03-25 22:53:14.310282: step 45720, loss = 0.82 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:53:16.031467: step 45730, loss = 0.67 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:53:17.771087: step 45740, loss = 0.94 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:53:19.493502: step 45750, loss = 0.84 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:53:21.233987: step 45760, loss = 0.81 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:53:22.971928: step 45770, loss = 0.82 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:53:24.708681: step 45780, loss = 0.79 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:53:26.435769: step 45790, loss = 0.76 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:53:28.349140: step 45800, loss = 0.69 (668.9 examples/sec; 0.191 sec/batch)
2017-03-25 22:53:29.942093: step 45810, loss = 0.60 (803.5 examples/sec; 0.159 sec/batch)
2017-03-25 22:53:31.677280: step 45820, loss = 0.69 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:53:33.402065: step 45830, loss = 0.70 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:53:35.151930: step 45840, loss = 0.65 (731.8 examples/sec; 0.175 sec/batch)
2017-03-25 22:53:36.887489: step 45850, loss = 0.72 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:53:38.630569: step 45860, loss = 0.74 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:53:40.379375: step 45870, loss = 0.88 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:53:42.124071: step 45880, loss = 0.82 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:53:43.860712: step 45890, loss = 0.83 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:53:45.759045: step 45900, loss = 0.66 (674.5 examples/sec; 0.190 sec/batch)
2017-03-25 22:53:47.371016: step 45910, loss = 0.64 (793.7 examples/sec; 0.161 sec/batch)
2017-03-25 22:53:49.116240: step 45920, loss = 0.71 (733.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:53:50.849575: step 45930, loss = 0.75 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:53:52.583329: step 45940, loss = 0.78 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:53:54.310951: step 45950, loss = 0.87 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:53:56.039014: step 45960, loss = 0.79 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:53:57.771511: step 45970, loss = 0.81 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:53:59.505979: step 45980, loss = 0.75 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:01.230381: step 45990, loss = 0.76 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 22:54:03.124416: step 46000, loss = 0.73 (676.2 examples/sec; 0.189 sec/batch)
2017-03-25 22:54:04.757618: step 46010, loss = 0.78 (783.3 examples/sec; 0.163 sec/batch)
2017-03-25 22:54:06.487218: step 46020, loss = 0.76 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:08.223811: step 46030, loss = 0.54 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:54:09.960531: step 46040, loss = 0.80 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:54:11.692973: step 46050, loss = 0.75 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:13.430567: step 46060, loss = 0.69 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:54:15.166865: step 46070, loss = 0.75 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:54:16.901573: step 46080, loss = 0.86 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:18.635601: step 46090, loss = 0.79 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:20.539326: step 46100, loss = 0.74 (672.7 examples/sec; 0.190 sec/batch)
2017-03-25 22:54:22.154169: step 46110, loss = 0.89 (792.2 examples/sec; 0.162 sec/batch)
2017-03-25 22:54:23.894235: step 46120, loss = 0.76 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:54:25.631894: step 46130, loss = 0.76 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:54:27.354480: step 46140, loss = 0.60 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:54:29.082336: step 46150, loss = 0.81 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:30.814229: step 46160, loss = 0.76 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:32.547333: step 46170, loss = 0.77 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:34.262734: step 46180, loss = 0.87 (746.2 examples/sec; 0.172 sec/batch)
2017-03-25 22:54:35.990184: step 46190, loss = 0.65 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:37.901822: step 46200, loss = 0.71 (669.6 examples/sec; 0.191 sec/batch)
2017-03-25 22:54:39.502300: step 46210, loss = 0.71 (799.9 examples/sec; 0.160 sec/batch)
2017-03-25 22:54:41.236242: step 46220, loss = 0.67 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:42.977145: step 46230, loss = 0.83 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:54:44.710877: step 46240, loss = 0.80 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:46.440311: step 46250, loss = 0.77 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:48.165887: step 46260, loss = 0.84 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:49.897362: step 46270, loss = 0.84 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:51.629180: step 46280, loss = 0.73 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:53.355575: step 46290, loss = 0.71 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:54:55.238954: step 46300, loss = 0.67 (680.1 examples/sec; 0.188 sec/batch)
2017-03-25 22:54:56.866677: step 46310, loss = 0.72 (785.8 examples/sec; 0.163 sec/batch)
2017-03-25 22:54:58.611991: step 46320, loss = 0.70 (733.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:55:00.342670: step 46330, loss = 0.74 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:55:02.081992: step 46340, loss = 0.95 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:55:03.805644: step 46350, loss = 0.84 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 22:55:05.539758: step 46360, loss = 0.76 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:55:07.269614: step 46370, loss = 0.87 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:55:09.004851: step 46380, loss = 0.84 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:55:10.744494: step 46390, loss = 0.68 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:55:12.651363: step 46400, loss = 0.56 (671.3 examples/sec; 0.191 sec/batch)
2017-03-25 22:55:14.265505: step 46410, loss = 0.58 (793.0 examples/sec; 0.161 sec/batch)
2017-03-25 22:55:15.991043: step 46420, loss = 0.69 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:55:17.732288: step 46430, loss = 0.81 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:55:19.469763: step 46440, loss = 0.74 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:55:21.198349: step 46450, loss = 0.74 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:55:22.937665: step 46460, loss = 0.78 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:55:24.672210: step 46470, loss = 0.69 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:55:26.406648: step 46480, loss = 0.78 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:55:28.153215: step 46490, loss = 0.67 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:55:30.043098: step 46500, loss = 0.47 (677.8 examples/sec; 0.189 sec/batch)
2017-03-25 22:55:31.663632: step 46510, loss = 0.76 (789.1 examples/sec; 0.162 sec/batch)
2017-03-25 22:55:33.418563: step 46520, loss = 0.94 (729.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:55:35.155341: step 46530, loss = 0.65 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:55:36.898098: step 46540, loss = 0.92 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:55:38.644311: step 46550, loss = 0.70 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 22:55:40.364920: step 46560, loss = 0.80 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:55:42.105498: step 46570, loss = 0.72 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:55:43.834678: step 46580, loss = 0.77 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:55:45.559467: step 46590, loss = 0.68 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:55:47.443561: step 46600, loss = 0.74 (679.4 examples/sec; 0.188 sec/batch)
2017-03-25 22:55:49.053539: step 46610, loss = 0.84 (795.1 examples/sec; 0.161 sec/batch)
2017-03-25 22:55:50.786656: step 46620, loss = 0.70 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:55:52.527308: step 46630, loss = 0.67 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:55:54.276240: step 46640, loss = 0.77 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:55:55.994629: step 46650, loss = 0.82 (744.8 examples/sec; 0.172 sec/batch)
2017-03-25 22:55:57.723079: step 46660, loss = 0.65 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:55:59.444195: step 46670, loss = 0.74 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:56:01.179936: step 46680, loss = 0.70 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:56:02.912572: step 46690, loss = 0.90 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:56:04.810459: step 46700, loss = 0.74 (674.9 examples/sec; 0.190 sec/batch)
2017-03-25 22:56:06.424035: step 46710, loss = 0.72 (792.7 examples/sec; 0.161 sec/batch)
2017-03-25 22:56:08.152837: step 46720, loss = 0.77 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:56:09.888306: step 46730, loss = 0.76 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:56:11.624876: step 46740, loss = 0.82 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:56:13.358594: step 46750, loss = 0.74 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:56:15.083290: step 46760, loss = 0.66 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:56:16.812000: step 46770, loss = 0.67 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:56:18.553750: step 46780, loss = 0.72 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:56:20.289069: step 46790, loss = 0.78 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:56:22.221531: step 46800, loss = 0.70 (662.8 examples/sec; 0.193 sec/batch)
2017-03-25 22:56:23.782440: step 46810, loss = 0.80 (819.4 examples/sec; 0.156 sec/batch)
2017-03-25 22:56:25.508766: step 46820, loss = 0.80 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:56:27.242104: step 46830, loss = 0.77 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:56:28.968999: step 46840, loss = 0.79 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:56:30.704564: step 46850, loss = 0.71 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:56:32.438103: step 46860, loss = 0.78 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:56:34.171280: step 46870, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:56:35.928171: step 46880, loss = 0.61 (728.6 examples/sec; 0.176 sec/batch)
2017-03-25 22:56:37.646443: step 46890, loss = 0.64 (744.9 examples/sec; 0.172 sec/batch)
2017-03-25 22:56:39.532067: step 46900, loss = 0.63 (678.8 examples/sec; 0.189 sec/batch)
2017-03-25 22:56:41.149069: step 46910, loss = 0.62 (791.6 examples/sec; 0.162 sec/batch)
2017-03-25 22:56:42.882606: step 46920, loss = 0.79 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:56:44.601201: step 46930, loss = 0.76 (744.8 examples/sec; 0.172 sec/batch)
2017-03-25 22:56:46.336237: step 46940, loss = 0.73 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:56:48.074851: step 46950, loss = 0.73 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:56:49.815203: step 46960, loss = 0.54 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:56:51.540709: step 46970, loss = 0.64 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:56:53.267501: step 46980, loss = 0.71 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:56:55.000998: step 46990, loss = 0.73 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:56:56.898884: step 47000, loss = 0.79 (674.8 examples/sec; 0.190 sec/batch)
2017-03-25 22:56:58.496033: step 47010, loss = 0.84 (800.9 examples/sec; 0.160 sec/batch)
2017-03-25 22:57:00.228335: step 47020, loss = 0.75 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:01.966586: step 47030, loss = 0.68 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 22:57:03.698820: step 47040, loss = 0.80 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:05.451601: step 47050, loss = 0.87 (730.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:57:07.186425: step 47060, loss = 0.77 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:08.907912: step 47070, loss = 0.74 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 22:57:10.634420: step 47080, loss = 0.60 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:12.378390: step 47090, loss = 0.78 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:57:14.262427: step 47100, loss = 0.69 (679.6 examples/sec; 0.188 sec/batch)
2017-03-25 22:57:15.868354: step 47110, loss = 0.77 (796.8 examples/sec; 0.161 sec/batch)
2017-03-25 22:57:17.593683: step 47120, loss = 0.75 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:19.324911: step 47130, loss = 0.68 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:21.064919: step 47140, loss = 0.66 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:57:22.806921: step 47150, loss = 0.74 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:57:24.551152: step 47160, loss = 0.60 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:57:26.280258: step 47170, loss = 0.74 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:28.022953: step 47180, loss = 0.65 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 22:57:29.751858: step 47190, loss = 0.66 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:31.642126: step 47200, loss = 1.27 (677.3 examples/sec; 0.189 sec/batch)
2017-03-25 22:57:33.255888: step 47210, loss = 0.68 (792.9 examples/sec; 0.161 sec/batch)
2017-03-25 22:57:34.992938: step 47220, loss = 0.69 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:57:36.714813: step 47230, loss = 1.00 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 22:57:38.434957: step 47240, loss = 0.61 (744.1 examples/sec; 0.172 sec/batch)
2017-03-25 22:57:40.169032: step 47250, loss = 0.71 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:41.894891: step 47260, loss = 0.76 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:43.623076: step 47270, loss = 0.74 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:45.352039: step 47280, loss = 0.74 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:47.080628: step 47290, loss = 0.74 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:48.975399: step 47300, loss = 0.75 (675.5 examples/sec; 0.189 sec/batch)
2017-03-25 22:57:50.591030: step 47310, loss = 0.74 (792.3 examples/sec; 0.162 sec/batch)
2017-03-25 22:57:52.316891: step 47320, loss = 0.75 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:54.033246: step 47330, loss = 0.78 (745.7 examples/sec; 0.172 sec/batch)
2017-03-25 22:57:55.768696: step 47340, loss = 0.79 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:57:57.493870: step 47350, loss = 0.74 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:57:59.227979: step 47360, loss = 0.78 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:58:00.970045: step 47370, loss = 0.85 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:58:02.714802: step 47380, loss = 1.01 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:58:04.446775: step 47390, loss = 0.74 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:58:06.339799: step 47400, loss = 0.68 (676.2 examples/sec; 0.189 sec/batch)
2017-03-25 22:58:07.952240: step 47410, loss = 0.84 (793.8 examples/sec; 0.161 sec/batch)
2017-03-25 22:58:09.678338: step 47420, loss = 0.73 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:58:11.404204: step 47430, loss = 0.90 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:58:13.134652: step 47440, loss = 0.69 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 22:58:14.874578: step 47450, loss = 0.79 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:58:16.615303: step 47460, loss = 0.61 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 22:58:18.365872: step 47470, loss = 0.72 (731.2 examples/sec; 0.175 sec/batch)
2017-03-25 22:58:20.100770: step 47480, loss = 0.85 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:58:21.844957: step 47490, loss = 0.73 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:58:23.727360: step 47500, loss = 0.72 (680.0 examples/sec; 0.188 sec/batch)
2017-03-25 22:58:25.344713: step 47510, loss = 0.81 (791.4 examples/sec; 0.162 sec/batch)
2017-03-25 22:58:27.074858: step 47520, loss = 0.78 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:58:28.812245: step 47530, loss = 0.76 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:58:30.548886: step 47540, loss = 0.61 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:58:32.288493: step 47550, loss = 0.85 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:58:34.003125: step 47560, loss = 0.60 (746.5 examples/sec; 0.171 sec/batch)
2017-03-25 22:58:35.744240: step 47570, loss = 0.59 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 22:58:37.479679: step 47580, loss = 0.75 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:58:39.208418: step 47590, loss = 0.67 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:58:41.090602: step 47600, loss = 0.79 (680.1 examples/sec; 0.188 sec/batch)
2017-03-25 22:58:42.724147: step 47610, loss = 0.82 (783.6 examples/sec; 0.163 sec/batch)
2017-03-25 22:58:44.457210: step 47620, loss = 0.63 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:58:46.204161: step 47630, loss = 0.72 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 22:58:47.938062: step 47640, loss = 0.85 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:58:49.682221: step 47650, loss = 0.63 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 22:58:51.409082: step 47660, loss = 0.74 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:58:53.137519: step 47670, loss = 0.71 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:58:54.866641: step 47680, loss = 0.83 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:58:56.595760: step 47690, loss = 0.83 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:58:58.504364: step 47700, loss = 0.86 (671.0 examples/sec; 0.191 sec/batch)
2017-03-25 22:59:00.095028: step 47710, loss = 0.73 (804.2 examples/sec; 0.159 sec/batch)
2017-03-25 22:59:01.839686: step 47720, loss = 0.68 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 22:59:03.557821: step 47730, loss = 0.70 (745.0 examples/sec; 0.172 sec/batch)
2017-03-25 22:59:05.301604: step 47740, loss = 0.95 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:59:07.036160: step 47750, loss = 0.65 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:59:08.775058: step 47760, loss = 0.72 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:59:10.502673: step 47770, loss = 0.80 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:59:12.233375: step 47780, loss = 0.61 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 22:59:13.978654: step 47790, loss = 0.76 (733.4 examples/sec; 0.175 sec/batch)
2017-03-25 22:59:15.870585: step 47800, loss = 0.66 (676.8 examples/sec; 0.189 sec/batch)
2017-03-25 22:59:17.493780: step 47810, loss = 0.77 (788.3 examples/sec; 0.162 sec/batch)
2017-03-25 22:59:19.219055: step 47820, loss = 0.72 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:59:20.949139: step 47830, loss = 0.83 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:59:22.688682: step 47840, loss = 0.70 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:59:24.417358: step 47850, loss = 0.71 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 22:59:26.145708: step 47860, loss = 0.95 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:59:27.885206: step 47870, loss = 0.80 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 22:59:29.633204: step 47880, loss = 0.68 (732.3 examples/sec; 0.175 sec/batch)
2017-03-25 22:59:31.362445: step 47890, loss = 0.75 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 22:59:33.252953: step 47900, loss = 0.82 (677.1 examples/sec; 0.189 sec/batch)
2017-03-25 22:59:34.865084: step 47910, loss = 0.70 (794.0 examples/sec; 0.161 sec/batch)
2017-03-25 22:59:36.601686: step 47920, loss = 0.74 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:59:38.336946: step 47930, loss = 0.79 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 22:59:40.065568: step 47940, loss = 0.81 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 22:59:41.802442: step 47950, loss = 0.73 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 22:59:43.553759: step 47960, loss = 0.85 (730.9 examples/sec; 0.175 sec/batch)
2017-03-25 22:59:45.282775: step 47970, loss = 0.75 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 22:59:47.021694: step 47980, loss = 0.72 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 22:59:48.750488: step 47990, loss = 0.70 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 22:59:50.654430: step 48000, loss = 0.83 (672.3 examples/sec; 0.190 sec/batch)
2017-03-25 22:59:52.253021: step 48010, loss = 0.69 (800.7 examples/sec; 0.160 sec/batch)
2017-03-25 22:59:54.001411: step 48020, loss = 0.71 (732.1 examples/sec; 0.175 sec/batch)
2017-03-25 22:59:55.731331: step 48030, loss = 0.87 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 22:59:57.465749: step 48040, loss = 0.62 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 22:59:59.226100: step 48050, loss = 0.72 (727.1 examples/sec; 0.176 sec/batch)
2017-03-25 23:00:00.966731: step 48060, loss = 0.76 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:02.714017: step 48070, loss = 0.77 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 23:00:04.449739: step 48080, loss = 0.78 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:06.185326: step 48090, loss = 0.67 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:08.082624: step 48100, loss = 0.75 (674.9 examples/sec; 0.190 sec/batch)
2017-03-25 23:00:09.697891: step 48110, loss = 0.72 (792.1 examples/sec; 0.162 sec/batch)
2017-03-25 23:00:11.437661: step 48120, loss = 0.84 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:13.179763: step 48130, loss = 0.94 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:14.921870: step 48140, loss = 0.87 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:16.666275: step 48150, loss = 0.65 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:18.407771: step 48160, loss = 0.74 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:20.141551: step 48170, loss = 0.72 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:00:21.864736: step 48180, loss = 0.68 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:00:23.605448: step 48190, loss = 0.80 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:25.530766: step 48200, loss = 0.70 (664.8 examples/sec; 0.193 sec/batch)
2017-03-25 23:00:27.135836: step 48210, loss = 0.62 (797.5 examples/sec; 0.161 sec/batch)
2017-03-25 23:00:28.871861: step 48220, loss = 0.88 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:30.613690: step 48230, loss = 0.82 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:32.359016: step 48240, loss = 0.85 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:34.099857: step 48250, loss = 0.71 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:35.842710: step 48260, loss = 0.68 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:37.584325: step 48270, loss = 0.74 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:39.317695: step 48280, loss = 0.53 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:00:41.046968: step 48290, loss = 0.76 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:00:42.955919: step 48300, loss = 0.68 (670.5 examples/sec; 0.191 sec/batch)
2017-03-25 23:00:44.565686: step 48310, loss = 0.77 (795.1 examples/sec; 0.161 sec/batch)
2017-03-25 23:00:46.306041: step 48320, loss = 0.73 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:48.038041: step 48330, loss = 0.87 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:00:49.768989: step 48340, loss = 0.73 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:00:51.519195: step 48350, loss = 0.84 (731.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:00:53.285645: step 48360, loss = 0.84 (724.6 examples/sec; 0.177 sec/batch)
2017-03-25 23:00:55.030692: step 48370, loss = 0.80 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:00:56.768428: step 48380, loss = 0.67 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:00:58.509205: step 48390, loss = 0.79 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:01:00.393314: step 48400, loss = 0.86 (679.6 examples/sec; 0.188 sec/batch)
2017-03-25 23:01:02.032003: step 48410, loss = 0.74 (780.8 examples/sec; 0.164 sec/batch)
2017-03-25 23:01:03.783274: step 48420, loss = 0.62 (730.9 examples/sec; 0.175 sec/batch)
2017-03-25 23:01:05.514015: step 48430, loss = 0.80 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:01:07.252129: step 48440, loss = 0.82 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:01:08.990977: step 48450, loss = 0.81 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:01:10.743126: step 48460, loss = 0.68 (730.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:01:12.487027: step 48470, loss = 0.70 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:01:14.245230: step 48480, loss = 0.90 (728.0 examples/sec; 0.176 sec/batch)
2017-03-25 23:01:15.990190: step 48490, loss = 0.81 (733.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:01:17.946647: step 48500, loss = 0.63 (654.2 examples/sec; 0.196 sec/batch)
2017-03-25 23:01:19.512399: step 48510, loss = 0.74 (817.5 examples/sec; 0.157 sec/batch)
2017-03-25 23:01:21.250094: step 48520, loss = 0.63 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:01:22.989415: step 48530, loss = 0.74 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:01:24.721955: step 48540, loss = 0.61 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:01:26.455173: step 48550, loss = 0.78 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:01:28.203875: step 48560, loss = 0.73 (732.0 examples/sec; 0.175 sec/batch)
2017-03-25 23:01:29.938858: step 48570, loss = 0.83 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:01:31.667663: step 48580, loss = 0.66 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:01:33.399994: step 48590, loss = 0.99 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:01:35.350076: step 48600, loss = 0.72 (656.6 examples/sec; 0.195 sec/batch)
2017-03-25 23:01:36.900145: step 48610, loss = 0.60 (825.4 examples/sec; 0.155 sec/batch)
2017-03-25 23:01:38.640471: step 48620, loss = 0.87 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:01:40.375132: step 48630, loss = 0.68 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:01:42.112757: step 48640, loss = 0.71 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:01:43.849442: step 48650, loss = 0.79 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:01:45.580848: step 48660, loss = 0.80 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:01:47.311623: step 48670, loss = 0.69 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:01:49.047042: step 48680, loss = 0.80 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:01:50.788017: step 48690, loss = 0.66 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:01:52.775478: step 48700, loss = 0.74 (644.3 examples/sec; 0.199 sec/batch)
2017-03-25 23:01:54.301588: step 48710, loss = 0.82 (838.3 examples/sec; 0.153 sec/batch)
2017-03-25 23:01:56.033201: step 48720, loss = 0.77 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:01:57.766338: step 48730, loss = 0.69 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:01:59.491720: step 48740, loss = 0.79 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:02:01.227469: step 48750, loss = 0.74 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:02:02.969706: step 48760, loss = 0.67 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:02:04.696352: step 48770, loss = 0.75 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:02:06.429489: step 48780, loss = 0.61 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:02:08.163630: step 48790, loss = 0.75 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:02:10.040182: step 48800, loss = 0.80 (682.6 examples/sec; 0.188 sec/batch)
2017-03-25 23:02:11.653535: step 48810, loss = 0.59 (792.8 examples/sec; 0.161 sec/batch)
2017-03-25 23:02:13.383956: step 48820, loss = 0.82 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:02:15.108179: step 48830, loss = 0.58 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:02:16.840334: step 48840, loss = 0.67 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:02:18.564782: step 48850, loss = 0.73 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:02:20.302574: step 48860, loss = 0.83 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:02:22.032506: step 48870, loss = 0.85 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:02:23.774981: step 48880, loss = 0.65 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:02:25.501752: step 48890, loss = 0.70 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:02:27.467830: step 48900, loss = 0.84 (651.3 examples/sec; 0.197 sec/batch)
2017-03-25 23:02:29.026984: step 48910, loss = 0.68 (820.6 examples/sec; 0.156 sec/batch)
2017-03-25 23:02:30.755091: step 48920, loss = 0.85 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:02:32.488606: step 48930, loss = 0.74 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:02:34.227806: step 48940, loss = 0.69 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:02:35.967707: step 48950, loss = 0.71 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:02:37.700157: step 48960, loss = 0.70 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:02:39.421062: step 48970, loss = 0.59 (743.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:02:41.164200: step 48980, loss = 0.76 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:02:42.906359: step 48990, loss = 0.78 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:02:44.789573: step 49000, loss = 0.62 (679.5 examples/sec; 0.188 sec/batch)
2017-03-25 23:02:46.410990: step 49010, loss = 0.76 (789.4 examples/sec; 0.162 sec/batch)
2017-03-25 23:02:48.134471: step 49020, loss = 0.79 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:02:49.873510: step 49030, loss = 0.81 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:02:51.596820: step 49040, loss = 0.73 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:02:53.328520: step 49050, loss = 0.71 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:02:55.063988: step 49060, loss = 0.67 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:02:56.807642: step 49070, loss = 0.60 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:02:58.547026: step 49080, loss = 0.71 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:03:00.262180: step 49090, loss = 0.68 (746.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:03:02.174332: step 49100, loss = 0.66 (669.4 examples/sec; 0.191 sec/batch)
2017-03-25 23:03:03.789982: step 49110, loss = 0.88 (792.2 examples/sec; 0.162 sec/batch)
2017-03-25 23:03:05.525882: step 49120, loss = 0.70 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:03:07.261710: step 49130, loss = 0.87 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:03:08.985414: step 49140, loss = 0.64 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:03:10.720852: step 49150, loss = 0.69 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:03:12.454116: step 49160, loss = 0.75 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:03:14.201456: step 49170, loss = 0.70 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:03:15.931362: step 49180, loss = 0.77 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:03:17.666310: step 49190, loss = 0.76 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:03:19.574272: step 49200, loss = 0.72 (670.9 examples/sec; 0.191 sec/batch)
2017-03-25 23:03:21.164072: step 49210, loss = 0.76 (805.1 examples/sec; 0.159 sec/batch)
2017-03-25 23:03:22.903496: step 49220, loss = 0.73 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:03:24.637460: step 49230, loss = 0.70 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:03:26.384342: step 49240, loss = 0.70 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 23:03:28.117681: step 49250, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:03:29.838143: step 49260, loss = 0.83 (744.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:03:31.570298: step 49270, loss = 0.71 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:03:33.305542: step 49280, loss = 0.88 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:03:35.032083: step 49290, loss = 0.77 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:03:36.942411: step 49300, loss = 0.97 (670.1 examples/sec; 0.191 sec/batch)
2017-03-25 23:03:38.547782: step 49310, loss = 0.87 (797.3 examples/sec; 0.161 sec/batch)
2017-03-25 23:03:40.285980: step 49320, loss = 0.77 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:03:42.013926: step 49330, loss = 0.62 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:03:43.751386: step 49340, loss = 0.59 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:03:45.487252: step 49350, loss = 0.71 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:03:47.215743: step 49360, loss = 0.70 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:03:48.956681: step 49370, loss = 0.71 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:03:50.684485: step 49380, loss = 0.80 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:03:52.425919: step 49390, loss = 0.67 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:03:54.320503: step 49400, loss = 0.72 (675.6 examples/sec; 0.189 sec/batch)
2017-03-25 23:03:55.918787: step 49410, loss = 0.65 (800.8 examples/sec; 0.160 sec/batch)
2017-03-25 23:03:57.654403: step 49420, loss = 0.82 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:03:59.387399: step 49430, loss = 0.70 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:04:01.123304: step 49440, loss = 0.74 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:04:02.856425: step 49450, loss = 0.75 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:04:04.594768: step 49460, loss = 0.71 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:04:06.325529: step 49470, loss = 0.63 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:04:08.053518: step 49480, loss = 0.58 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:04:09.791707: step 49490, loss = 0.71 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:04:11.668684: step 49500, loss = 0.74 (682.3 examples/sec; 0.188 sec/batch)
2017-03-25 23:04:13.308648: step 49510, loss = 0.75 (780.1 examples/sec; 0.164 sec/batch)
2017-03-25 23:04:15.043842: step 49520, loss = 0.64 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:04:16.776964: step 49530, loss = 0.88 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:04:18.508789: step 49540, loss = 0.78 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:04:20.246458: step 49550, loss = 0.81 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:04:21.987259: step 49560, loss = 0.86 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:04:23.709223: step 49570, loss = 0.80 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:04:25.444679: step 49580, loss = 0.73 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:04:27.167024: step 49590, loss = 0.59 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:04:29.062923: step 49600, loss = 0.76 (675.4 examples/sec; 0.190 sec/batch)
2017-03-25 23:04:30.669897: step 49610, loss = 0.69 (796.1 examples/sec; 0.161 sec/batch)
2017-03-25 23:04:32.397912: step 49620, loss = 0.81 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:04:34.136832: step 49630, loss = 0.75 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:04:35.856975: step 49640, loss = 0.88 (744.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:04:37.580041: step 49650, loss = 0.73 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:04:39.327053: step 49660, loss = 0.78 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 23:04:41.048262: step 49670, loss = 0.61 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:04:42.779226: step 49680, loss = 0.89 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:04:44.520183: step 49690, loss = 0.89 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:04:46.404212: step 49700, loss = 0.62 (679.7 examples/sec; 0.188 sec/batch)
2017-03-25 23:04:48.015423: step 49710, loss = 0.77 (794.0 examples/sec; 0.161 sec/batch)
2017-03-25 23:04:49.744360: step 49720, loss = 0.72 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:04:51.491390: step 49730, loss = 0.78 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 23:04:53.222216: step 49740, loss = 0.61 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:04:54.947454: step 49750, loss = 0.81 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:04:56.677805: step 49760, loss = 0.79 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:04:58.415574: step 49770, loss = 0.68 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:05:00.146469: step 49780, loss = 0.72 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:01.899127: step 49790, loss = 0.66 (730.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:05:03.832463: step 49800, loss = 0.70 (662.1 examples/sec; 0.193 sec/batch)
2017-03-25 23:05:05.419686: step 49810, loss = 0.89 (806.4 examples/sec; 0.159 sec/batch)
2017-03-25 23:05:07.145287: step 49820, loss = 0.69 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:08.867634: step 49830, loss = 0.75 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:05:10.599156: step 49840, loss = 0.75 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:12.333606: step 49850, loss = 0.77 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:14.068855: step 49860, loss = 0.72 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:05:15.802858: step 49870, loss = 0.70 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:17.541640: step 49880, loss = 0.68 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:05:19.275170: step 49890, loss = 0.72 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:21.179178: step 49900, loss = 0.85 (672.5 examples/sec; 0.190 sec/batch)
2017-03-25 23:05:22.753607: step 49910, loss = 0.89 (812.7 examples/sec; 0.157 sec/batch)
2017-03-25 23:05:24.482970: step 49920, loss = 0.86 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:26.221882: step 49930, loss = 0.52 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:05:27.952710: step 49940, loss = 0.67 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:29.680506: step 49950, loss = 0.70 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:31.412644: step 49960, loss = 0.81 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:33.148775: step 49970, loss = 0.87 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:05:34.874885: step 49980, loss = 0.85 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:36.612863: step 49990, loss = 0.80 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:05:38.519390: step 50000, loss = 0.78 (671.4 examples/sec; 0.191 sec/batch)
2017-03-25 23:05:40.122503: step 50010, loss = 0.73 (798.4 examples/sec; 0.160 sec/batch)
2017-03-25 23:05:41.856059: step 50020, loss = 0.74 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:43.588854: step 50030, loss = 0.68 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:45.321714: step 50040, loss = 0.73 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:47.056515: step 50050, loss = 0.87 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:48.796253: step 50060, loss = 0.74 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:05:50.527448: step 50070, loss = 0.76 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:05:52.265697: step 50080, loss = 0.73 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:05:53.984277: step 50090, loss = 0.74 (744.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:05:55.877179: step 50100, loss = 0.74 (676.2 examples/sec; 0.189 sec/batch)
2017-03-25 23:05:57.480335: step 50110, loss = 0.90 (798.4 examples/sec; 0.160 sec/batch)
2017-03-25 23:05:59.205065: step 50120, loss = 0.76 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:06:00.948918: step 50130, loss = 0.76 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:06:02.683476: step 50140, loss = 0.80 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:06:04.420983: step 50150, loss = 0.80 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:06:06.152042: step 50160, loss = 0.74 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:06:07.890124: step 50170, loss = 0.75 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:06:09.626357: step 50180, loss = 0.86 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:06:11.362064: step 50190, loss = 0.72 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:06:13.244685: step 50200, loss = 0.68 (679.8 examples/sec; 0.188 sec/batch)
2017-03-25 23:06:14.865930: step 50210, loss = 0.88 (789.5 examples/sec; 0.162 sec/batch)
2017-03-25 23:06:16.598926: step 50220, loss = 0.85 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:06:18.327827: step 50230, loss = 0.60 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:06:20.052018: step 50240, loss = 0.74 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:06:21.796746: step 50250, loss = 0.63 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:06:23.527600: step 50260, loss = 0.71 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:06:25.246652: step 50270, loss = 0.78 (744.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:06:26.982180: step 50280, loss = 0.74 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:06:28.713740: step 50290, loss = 0.82 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:06:30.674605: step 50300, loss = 0.84 (652.8 examples/sec; 0.196 sec/batch)
2017-03-25 23:06:32.218152: step 50310, loss = 0.68 (829.3 examples/sec; 0.154 sec/batch)
2017-03-25 23:06:33.958221: step 50320, loss = 0.70 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:06:35.694144: step 50330, loss = 0.69 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:06:37.427009: step 50340, loss = 0.64 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:06:39.157736: step 50350, loss = 0.77 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:06:40.890928: step 50360, loss = 0.75 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:06:42.636373: step 50370, loss = 0.75 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:06:44.364849: step 50380, loss = 0.92 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:06:46.089077: step 50390, loss = 0.87 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:06:48.053611: step 50400, loss = 0.81 (651.7 examples/sec; 0.196 sec/batch)
2017-03-25 23:06:49.596921: step 50410, loss = 0.60 (829.2 examples/sec; 0.154 sec/batch)
2017-03-25 23:06:51.335772: step 50420, loss = 0.79 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:06:53.072907: step 50430, loss = 0.75 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:06:54.807121: step 50440, loss = 0.80 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:06:56.547618: step 50450, loss = 0.71 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:06:58.288460: step 50460, loss = 0.79 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:07:00.013890: step 50470, loss = 0.67 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:07:01.745510: step 50480, loss = 0.64 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:07:03.483065: step 50490, loss = 0.73 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:07:05.364851: step 50500, loss = 0.75 (680.2 examples/sec; 0.188 sec/batch)
2017-03-25 23:07:06.977971: step 50510, loss = 0.80 (793.5 examples/sec; 0.161 sec/batch)
2017-03-25 23:07:08.717855: step 50520, loss = 0.58 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:07:10.448976: step 50530, loss = 0.88 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:07:12.178751: step 50540, loss = 0.91 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:07:13.915161: step 50550, loss = 0.70 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:07:15.632025: step 50560, loss = 0.85 (745.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:07:17.358414: step 50570, loss = 0.79 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:07:19.079279: step 50580, loss = 0.78 (743.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:07:20.815785: step 50590, loss = 0.82 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:07:22.700995: step 50600, loss = 0.79 (679.3 examples/sec; 0.188 sec/batch)
2017-03-25 23:07:24.334525: step 50610, loss = 0.76 (783.2 examples/sec; 0.163 sec/batch)
2017-03-25 23:07:26.057304: step 50620, loss = 0.73 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:07:27.778474: step 50630, loss = 0.78 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:07:29.521091: step 50640, loss = 0.72 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:07:31.245388: step 50650, loss = 0.87 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:07:32.981274: step 50660, loss = 0.73 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:07:34.724288: step 50670, loss = 0.74 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:07:36.462947: step 50680, loss = 0.89 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:07:38.198570: step 50690, loss = 0.65 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:07:40.146240: step 50700, loss = 0.74 (657.2 examples/sec; 0.195 sec/batch)
2017-03-25 23:07:41.702891: step 50710, loss = 0.66 (822.3 examples/sec; 0.156 sec/batch)
2017-03-25 23:07:43.440017: step 50720, loss = 0.66 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:07:45.171302: step 50730, loss = 0.57 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:07:46.892817: step 50740, loss = 0.59 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:07:48.628539: step 50750, loss = 0.90 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:07:50.366024: step 50760, loss = 0.77 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:07:52.089521: step 50770, loss = 0.79 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:07:53.818779: step 50780, loss = 0.77 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:07:55.550955: step 50790, loss = 0.76 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:07:57.414258: step 50800, loss = 0.64 (687.2 examples/sec; 0.186 sec/batch)
2017-03-25 23:07:59.048629: step 50810, loss = 0.65 (782.8 examples/sec; 0.164 sec/batch)
2017-03-25 23:08:00.769510: step 50820, loss = 0.84 (743.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:08:02.518675: step 50830, loss = 0.80 (731.8 examples/sec; 0.175 sec/batch)
2017-03-25 23:08:04.253262: step 50840, loss = 0.76 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:08:05.986620: step 50850, loss = 0.69 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:08:07.716787: step 50860, loss = 0.85 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:08:09.451921: step 50870, loss = 0.76 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:08:11.175600: step 50880, loss = 0.76 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:08:12.919524: step 50890, loss = 0.89 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:08:14.905502: step 50900, loss = 0.85 (644.5 examples/sec; 0.199 sec/batch)
2017-03-25 23:08:16.417319: step 50910, loss = 0.74 (846.7 examples/sec; 0.151 sec/batch)
2017-03-25 23:08:18.102360: step 50920, loss = 0.85 (759.6 examples/sec; 0.169 sec/batch)
2017-03-25 23:08:19.836360: step 50930, loss = 0.71 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:08:21.569747: step 50940, loss = 0.89 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:08:23.306726: step 50950, loss = 0.59 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:08:25.026914: step 50960, loss = 0.82 (744.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:08:26.751673: step 50970, loss = 0.70 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:08:28.484461: step 50980, loss = 0.70 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:08:30.216378: step 50990, loss = 0.75 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:08:32.103344: step 51000, loss = 0.72 (678.8 examples/sec; 0.189 sec/batch)
2017-03-25 23:08:33.714026: step 51010, loss = 1.02 (794.0 examples/sec; 0.161 sec/batch)
2017-03-25 23:08:35.442379: step 51020, loss = 0.74 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:08:37.171066: step 51030, loss = 0.68 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:08:38.908276: step 51040, loss = 0.65 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:08:40.647408: step 51050, loss = 0.84 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:08:42.376613: step 51060, loss = 0.55 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:08:44.121182: step 51070, loss = 0.85 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:08:45.852104: step 51080, loss = 0.72 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:08:47.574636: step 51090, loss = 0.60 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:08:49.470113: step 51100, loss = 0.83 (675.3 examples/sec; 0.190 sec/batch)
2017-03-25 23:08:51.083292: step 51110, loss = 0.80 (793.5 examples/sec; 0.161 sec/batch)
2017-03-25 23:08:52.827959: step 51120, loss = 0.69 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:08:54.559937: step 51130, loss = 0.77 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:08:56.279898: step 51140, loss = 0.76 (744.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:08:58.012355: step 51150, loss = 0.64 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:08:59.740046: step 51160, loss = 0.87 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:09:01.479094: step 51170, loss = 0.67 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:09:03.219244: step 51180, loss = 0.76 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:09:04.948403: step 51190, loss = 1.05 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:09:06.871668: step 51200, loss = 0.73 (665.5 examples/sec; 0.192 sec/batch)
2017-03-25 23:09:08.458181: step 51210, loss = 0.76 (806.8 examples/sec; 0.159 sec/batch)
2017-03-25 23:09:10.196984: step 51220, loss = 0.68 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:09:11.928687: step 51230, loss = 0.66 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:09:13.650724: step 51240, loss = 0.71 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:09:15.387636: step 51250, loss = 0.70 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:09:17.133468: step 51260, loss = 0.57 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 23:09:18.874220: step 51270, loss = 0.78 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:09:20.613438: step 51280, loss = 0.66 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:09:22.363726: step 51290, loss = 0.71 (731.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:09:24.328596: step 51300, loss = 0.66 (651.7 examples/sec; 0.196 sec/batch)
2017-03-25 23:09:25.875121: step 51310, loss = 0.68 (827.2 examples/sec; 0.155 sec/batch)
2017-03-25 23:09:27.619535: step 51320, loss = 0.96 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:09:29.361331: step 51330, loss = 0.88 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:09:31.094777: step 51340, loss = 0.77 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:09:32.837692: step 51350, loss = 0.72 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:09:34.580350: step 51360, loss = 0.87 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:09:36.325386: step 51370, loss = 0.92 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:09:38.056466: step 51380, loss = 0.80 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:09:39.801008: step 51390, loss = 0.65 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:09:41.712609: step 51400, loss = 0.74 (669.5 examples/sec; 0.191 sec/batch)
2017-03-25 23:09:43.298722: step 51410, loss = 0.59 (807.0 examples/sec; 0.159 sec/batch)
2017-03-25 23:09:45.031835: step 51420, loss = 0.77 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:09:46.767545: step 51430, loss = 0.84 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:09:48.512235: step 51440, loss = 0.89 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:09:50.239917: step 51450, loss = 0.70 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:09:51.992287: step 51460, loss = 0.78 (730.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:09:53.717659: step 51470, loss = 0.79 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:09:55.437003: step 51480, loss = 0.80 (744.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:09:57.178118: step 51490, loss = 0.74 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:09:59.077763: step 51500, loss = 0.62 (673.8 examples/sec; 0.190 sec/batch)
2017-03-25 23:10:00.698625: step 51510, loss = 0.69 (789.7 examples/sec; 0.162 sec/batch)
2017-03-25 23:10:02.435734: step 51520, loss = 0.80 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:10:04.166403: step 51530, loss = 0.69 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:10:05.898058: step 51540, loss = 0.79 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:10:07.644258: step 51550, loss = 0.84 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 23:10:09.386437: step 51560, loss = 0.86 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:10:11.145328: step 51570, loss = 0.80 (727.7 examples/sec; 0.176 sec/batch)
2017-03-25 23:10:12.898205: step 51580, loss = 0.74 (730.2 examples/sec; 0.175 sec/batch)
2017-03-25 23:10:14.631425: step 51590, loss = 0.76 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:10:16.529885: step 51600, loss = 0.82 (674.2 examples/sec; 0.190 sec/batch)
2017-03-25 23:10:18.150859: step 51610, loss = 0.78 (789.7 examples/sec; 0.162 sec/batch)
2017-03-25 23:10:19.900184: step 51620, loss = 0.85 (731.7 examples/sec; 0.175 sec/batch)
2017-03-25 23:10:21.638704: step 51630, loss = 0.66 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:10:23.383412: step 51640, loss = 0.79 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:10:25.118168: step 51650, loss = 0.76 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:10:26.867679: step 51660, loss = 0.76 (731.6 examples/sec; 0.175 sec/batch)
2017-03-25 23:10:28.590651: step 51670, loss = 0.72 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:10:30.337477: step 51680, loss = 0.77 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 23:10:32.060382: step 51690, loss = 0.69 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:10:33.973151: step 51700, loss = 0.78 (669.2 examples/sec; 0.191 sec/batch)
2017-03-25 23:10:35.573481: step 51710, loss = 0.70 (799.8 examples/sec; 0.160 sec/batch)
2017-03-25 23:10:37.307144: step 51720, loss = 0.57 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:10:39.051647: step 51730, loss = 0.84 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:10:40.779617: step 51740, loss = 0.68 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:10:42.518916: step 51750, loss = 0.71 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:10:44.252180: step 51760, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:10:45.987693: step 51770, loss = 0.72 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:10:47.713670: step 51780, loss = 0.80 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:10:49.440766: step 51790, loss = 0.77 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:10:51.319752: step 51800, loss = 0.60 (681.2 examples/sec; 0.188 sec/batch)
2017-03-25 23:10:52.939702: step 51810, loss = 0.76 (790.2 examples/sec; 0.162 sec/batch)
2017-03-25 23:10:54.704210: step 51820, loss = 0.71 (725.4 examples/sec; 0.176 sec/batch)
2017-03-25 23:10:56.439726: step 51830, loss = 0.66 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:10:58.160294: step 51840, loss = 0.74 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:10:59.902035: step 51850, loss = 0.66 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:11:01.645459: step 51860, loss = 0.75 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:11:03.378176: step 51870, loss = 0.86 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:05.105834: step 51880, loss = 0.79 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:06.849713: step 51890, loss = 0.82 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:11:08.717518: step 51900, loss = 0.60 (685.6 examples/sec; 0.187 sec/batch)
2017-03-25 23:11:10.335845: step 51910, loss = 0.79 (790.6 examples/sec; 0.162 sec/batch)
2017-03-25 23:11:12.070606: step 51920, loss = 0.76 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:13.801241: step 51930, loss = 0.80 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:15.556582: step 51940, loss = 0.87 (729.2 examples/sec; 0.176 sec/batch)
2017-03-25 23:11:17.282036: step 51950, loss = 0.67 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:19.008300: step 51960, loss = 0.73 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:20.734076: step 51970, loss = 0.70 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:22.468384: step 51980, loss = 0.74 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:24.200190: step 51990, loss = 0.68 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:26.115991: step 52000, loss = 0.96 (668.4 examples/sec; 0.191 sec/batch)
2017-03-25 23:11:27.728883: step 52010, loss = 0.63 (793.2 examples/sec; 0.161 sec/batch)
2017-03-25 23:11:29.458533: step 52020, loss = 0.70 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:31.195152: step 52030, loss = 0.82 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:11:32.929748: step 52040, loss = 0.67 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:34.659163: step 52050, loss = 0.62 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:36.397326: step 52060, loss = 0.64 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:11:38.132561: step 52070, loss = 0.79 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:39.865765: step 52080, loss = 0.76 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:41.595703: step 52090, loss = 0.73 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:43.470664: step 52100, loss = 0.83 (682.7 examples/sec; 0.187 sec/batch)
2017-03-25 23:11:45.095601: step 52110, loss = 0.94 (787.7 examples/sec; 0.162 sec/batch)
2017-03-25 23:11:46.841033: step 52120, loss = 0.68 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:11:48.571016: step 52130, loss = 0.71 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:50.305860: step 52140, loss = 0.82 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:52.040511: step 52150, loss = 0.74 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:11:53.776438: step 52160, loss = 0.68 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:11:55.515241: step 52170, loss = 0.57 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:11:57.260192: step 52180, loss = 0.88 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:11:59.000974: step 52190, loss = 0.54 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:12:00.902450: step 52200, loss = 0.78 (673.4 examples/sec; 0.190 sec/batch)
2017-03-25 23:12:02.531474: step 52210, loss = 0.68 (785.4 examples/sec; 0.163 sec/batch)
2017-03-25 23:12:04.258643: step 52220, loss = 0.87 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:05.994666: step 52230, loss = 0.69 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:12:07.715114: step 52240, loss = 0.73 (744.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:12:09.447634: step 52250, loss = 0.73 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:11.192677: step 52260, loss = 0.76 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:12:12.917613: step 52270, loss = 0.82 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:12:14.647484: step 52280, loss = 0.79 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:16.384381: step 52290, loss = 0.70 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:12:18.272764: step 52300, loss = 0.54 (677.8 examples/sec; 0.189 sec/batch)
2017-03-25 23:12:19.897917: step 52310, loss = 0.69 (787.6 examples/sec; 0.163 sec/batch)
2017-03-25 23:12:21.623477: step 52320, loss = 0.72 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:23.366694: step 52330, loss = 0.67 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:12:25.099258: step 52340, loss = 0.79 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:26.829402: step 52350, loss = 0.63 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:28.561036: step 52360, loss = 0.90 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:30.289619: step 52370, loss = 0.60 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:32.026500: step 52380, loss = 0.68 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:12:33.734274: step 52390, loss = 0.62 (749.3 examples/sec; 0.171 sec/batch)
2017-03-25 23:12:35.703776: step 52400, loss = 0.90 (650.5 examples/sec; 0.197 sec/batch)
2017-03-25 23:12:37.244937: step 52410, loss = 0.67 (829.5 examples/sec; 0.154 sec/batch)
2017-03-25 23:12:38.978702: step 52420, loss = 0.74 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:40.713234: step 52430, loss = 0.76 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:42.441658: step 52440, loss = 0.66 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:44.172361: step 52450, loss = 0.68 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:45.900627: step 52460, loss = 0.75 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:47.630029: step 52470, loss = 0.72 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:49.349709: step 52480, loss = 0.77 (744.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:12:51.093271: step 52490, loss = 0.79 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:12:52.964649: step 52500, loss = 0.74 (684.0 examples/sec; 0.187 sec/batch)
2017-03-25 23:12:54.595010: step 52510, loss = 0.62 (785.1 examples/sec; 0.163 sec/batch)
2017-03-25 23:12:56.324033: step 52520, loss = 0.69 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:58.052219: step 52530, loss = 0.78 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:12:59.790433: step 52540, loss = 1.00 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:13:01.526695: step 52550, loss = 0.80 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:13:03.246091: step 52560, loss = 0.83 (744.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:13:04.977920: step 52570, loss = 0.81 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:13:06.706854: step 52580, loss = 0.75 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:13:08.440062: step 52590, loss = 0.71 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:13:10.330075: step 52600, loss = 0.82 (677.5 examples/sec; 0.189 sec/batch)
2017-03-25 23:13:11.943059: step 52610, loss = 0.77 (793.3 examples/sec; 0.161 sec/batch)
2017-03-25 23:13:13.679283: step 52620, loss = 0.77 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:13:15.401844: step 52630, loss = 0.64 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:13:17.139887: step 52640, loss = 0.77 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:13:18.861864: step 52650, loss = 0.68 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:13:20.602963: step 52660, loss = 0.59 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:13:22.340755: step 52670, loss = 0.63 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:13:24.075879: step 52680, loss = 0.84 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:13:25.809307: step 52690, loss = 0.89 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:13:27.698230: step 52700, loss = 0.78 (677.7 examples/sec; 0.189 sec/batch)
2017-03-25 23:13:29.322204: step 52710, loss = 0.76 (788.0 examples/sec; 0.162 sec/batch)
2017-03-25 23:13:31.055377: step 52720, loss = 0.63 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:13:32.784981: step 52730, loss = 0.69 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:13:34.505347: step 52740, loss = 0.83 (744.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:13:36.233599: step 52750, loss = 0.80 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:13:37.961126: step 52760, loss = 0.82 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:13:39.697335: step 52770, loss = 0.79 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:13:41.428332: step 52780, loss = 0.62 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:13:43.166691: step 52790, loss = 0.82 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:13:45.049717: step 52800, loss = 0.85 (680.1 examples/sec; 0.188 sec/batch)
2017-03-25 23:13:46.664951: step 52810, loss = 0.69 (792.0 examples/sec; 0.162 sec/batch)
2017-03-25 23:13:48.386626: step 52820, loss = 0.65 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:13:50.133644: step 52830, loss = 0.63 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 23:13:51.861957: step 52840, loss = 0.81 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:13:53.596252: step 52850, loss = 0.68 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:13:55.321272: step 52860, loss = 0.78 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:13:57.052755: step 52870, loss = 0.67 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:13:58.785386: step 52880, loss = 0.73 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:14:00.520227: step 52890, loss = 0.64 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:14:02.418956: step 52900, loss = 0.71 (674.1 examples/sec; 0.190 sec/batch)
2017-03-25 23:14:04.022560: step 52910, loss = 0.68 (798.2 examples/sec; 0.160 sec/batch)
2017-03-25 23:14:05.752256: step 52920, loss = 0.77 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:14:07.484472: step 52930, loss = 0.61 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:14:09.214244: step 52940, loss = 0.87 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:14:10.946431: step 52950, loss = 0.64 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:14:12.688425: step 52960, loss = 0.76 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:14:14.393630: step 52970, loss = 0.64 (750.6 examples/sec; 0.171 sec/batch)
2017-03-25 23:14:16.128103: step 52980, loss = 0.72 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:14:17.860912: step 52990, loss = 0.73 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:14:19.753389: step 53000, loss = 0.73 (676.4 examples/sec; 0.189 sec/batch)
2017-03-25 23:14:21.360686: step 53010, loss = 0.59 (796.4 examples/sec; 0.161 sec/batch)
2017-03-25 23:14:23.100832: step 53020, loss = 0.80 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:14:24.824061: step 53030, loss = 0.70 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:14:26.564312: step 53040, loss = 0.87 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:14:28.289942: step 53050, loss = 0.79 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:14:30.031655: step 53060, loss = 0.82 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:14:31.755773: step 53070, loss = 0.63 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:14:33.499134: step 53080, loss = 0.85 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:14:35.230913: step 53090, loss = 0.65 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:14:37.156439: step 53100, loss = 0.74 (664.8 examples/sec; 0.193 sec/batch)
2017-03-25 23:14:38.737111: step 53110, loss = 0.66 (809.6 examples/sec; 0.158 sec/batch)
2017-03-25 23:14:40.466919: step 53120, loss = 0.77 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:14:42.205089: step 53130, loss = 0.64 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:14:43.952468: step 53140, loss = 0.76 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:14:45.686884: step 53150, loss = 0.72 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:14:47.416960: step 53160, loss = 0.73 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:14:49.154470: step 53170, loss = 0.63 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:14:50.874881: step 53180, loss = 0.77 (744.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:14:52.611868: step 53190, loss = 0.72 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:14:54.498459: step 53200, loss = 0.58 (678.8 examples/sec; 0.189 sec/batch)
2017-03-25 23:14:56.108917: step 53210, loss = 0.87 (794.4 examples/sec; 0.161 sec/batch)
2017-03-25 23:14:57.834257: step 53220, loss = 0.79 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:14:59.577905: step 53230, loss = 0.74 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:01.315915: step 53240, loss = 0.66 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:03.058014: step 53250, loss = 0.72 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:04.797191: step 53260, loss = 0.90 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:06.521320: step 53270, loss = 0.66 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:15:08.265549: step 53280, loss = 0.68 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:09.983941: step 53290, loss = 0.67 (744.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:15:11.943912: step 53300, loss = 0.72 (653.4 examples/sec; 0.196 sec/batch)
2017-03-25 23:15:13.457865: step 53310, loss = 0.81 (845.0 examples/sec; 0.151 sec/batch)
2017-03-25 23:15:15.153767: step 53320, loss = 0.63 (754.6 examples/sec; 0.170 sec/batch)
2017-03-25 23:15:16.894764: step 53330, loss = 0.69 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:18.641955: step 53340, loss = 0.84 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 23:15:20.377018: step 53350, loss = 0.77 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:22.123721: step 53360, loss = 0.80 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 23:15:23.868890: step 53370, loss = 0.95 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:15:25.590674: step 53380, loss = 0.73 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:15:27.331446: step 53390, loss = 0.70 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:29.221519: step 53400, loss = 0.84 (677.2 examples/sec; 0.189 sec/batch)
2017-03-25 23:15:30.840799: step 53410, loss = 0.71 (790.5 examples/sec; 0.162 sec/batch)
2017-03-25 23:15:32.565431: step 53420, loss = 0.67 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:15:34.293629: step 53430, loss = 0.76 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:15:36.042677: step 53440, loss = 0.72 (731.8 examples/sec; 0.175 sec/batch)
2017-03-25 23:15:37.779270: step 53450, loss = 0.67 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:39.515233: step 53460, loss = 0.73 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:41.257420: step 53470, loss = 0.71 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:42.991272: step 53480, loss = 0.70 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:15:44.728198: step 53490, loss = 0.88 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:46.623295: step 53500, loss = 0.70 (675.7 examples/sec; 0.189 sec/batch)
2017-03-25 23:15:48.229649: step 53510, loss = 0.63 (796.5 examples/sec; 0.161 sec/batch)
2017-03-25 23:15:49.968593: step 53520, loss = 0.75 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:51.695357: step 53530, loss = 0.83 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:15:53.433164: step 53540, loss = 0.80 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:55.170665: step 53550, loss = 0.93 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:15:56.877587: step 53560, loss = 0.73 (749.9 examples/sec; 0.171 sec/batch)
2017-03-25 23:15:58.618227: step 53570, loss = 0.75 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:16:00.345048: step 53580, loss = 0.78 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:02.097822: step 53590, loss = 0.83 (730.2 examples/sec; 0.175 sec/batch)
2017-03-25 23:16:03.969970: step 53600, loss = 0.69 (683.7 examples/sec; 0.187 sec/batch)
2017-03-25 23:16:05.593661: step 53610, loss = 0.77 (788.3 examples/sec; 0.162 sec/batch)
2017-03-25 23:16:07.324971: step 53620, loss = 0.78 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:09.059893: step 53630, loss = 0.73 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:10.785218: step 53640, loss = 0.84 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:12.520571: step 53650, loss = 0.72 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:16:14.257095: step 53660, loss = 0.69 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:16:15.998273: step 53670, loss = 0.69 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:16:17.734347: step 53680, loss = 0.98 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:16:19.460870: step 53690, loss = 0.73 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:21.430755: step 53700, loss = 0.69 (649.8 examples/sec; 0.197 sec/batch)
2017-03-25 23:16:22.948183: step 53710, loss = 0.70 (843.7 examples/sec; 0.152 sec/batch)
2017-03-25 23:16:24.674292: step 53720, loss = 0.67 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:26.417289: step 53730, loss = 0.76 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:16:28.140717: step 53740, loss = 0.71 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:16:29.875439: step 53750, loss = 0.77 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:31.605321: step 53760, loss = 0.58 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:33.345649: step 53770, loss = 0.72 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:16:35.080578: step 53780, loss = 0.64 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:36.805642: step 53790, loss = 0.74 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:38.690887: step 53800, loss = 0.75 (679.0 examples/sec; 0.189 sec/batch)
2017-03-25 23:16:40.303059: step 53810, loss = 0.90 (794.0 examples/sec; 0.161 sec/batch)
2017-03-25 23:16:42.026776: step 53820, loss = 0.60 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:16:43.767502: step 53830, loss = 0.69 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:16:45.499095: step 53840, loss = 0.71 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:47.231901: step 53850, loss = 0.88 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:48.962921: step 53860, loss = 0.69 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:50.696960: step 53870, loss = 0.99 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:52.418656: step 53880, loss = 0.73 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:16:54.150922: step 53890, loss = 0.79 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:16:56.059236: step 53900, loss = 0.75 (671.0 examples/sec; 0.191 sec/batch)
2017-03-25 23:16:57.677248: step 53910, loss = 0.59 (790.8 examples/sec; 0.162 sec/batch)
2017-03-25 23:16:59.409233: step 53920, loss = 0.64 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:17:01.142494: step 53930, loss = 0.85 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:17:02.893323: step 53940, loss = 0.73 (731.1 examples/sec; 0.175 sec/batch)
2017-03-25 23:17:04.624794: step 53950, loss = 0.73 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:17:06.361223: step 53960, loss = 0.75 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:17:08.088223: step 53970, loss = 0.76 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:17:09.824143: step 53980, loss = 0.73 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:17:11.557296: step 53990, loss = 0.64 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:17:13.440135: step 54000, loss = 0.70 (680.1 examples/sec; 0.188 sec/batch)
2017-03-25 23:17:15.053484: step 54010, loss = 0.60 (793.1 examples/sec; 0.161 sec/batch)
2017-03-25 23:17:16.778464: step 54020, loss = 0.81 (742.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:17:18.521481: step 54030, loss = 0.78 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:17:20.258739: step 54040, loss = 0.68 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:17:21.981416: step 54050, loss = 0.93 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:17:23.704061: step 54060, loss = 0.82 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:17:25.440900: step 54070, loss = 0.74 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:17:27.168125: step 54080, loss = 0.68 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:17:28.910664: step 54090, loss = 0.78 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:17:30.820433: step 54100, loss = 0.75 (670.2 examples/sec; 0.191 sec/batch)
2017-03-25 23:17:32.443158: step 54110, loss = 0.75 (788.8 examples/sec; 0.162 sec/batch)
2017-03-25 23:17:34.172276: step 54120, loss = 0.62 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:17:35.916017: step 54130, loss = 0.75 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:17:37.652408: step 54140, loss = 0.73 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:17:39.379951: step 54150, loss = 0.61 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:17:41.115544: step 54160, loss = 0.74 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:17:42.846083: step 54170, loss = 0.83 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:17:44.561449: step 54180, loss = 0.70 (746.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:17:46.296313: step 54190, loss = 0.84 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:17:48.182944: step 54200, loss = 0.81 (678.7 examples/sec; 0.189 sec/batch)
2017-03-25 23:17:49.801743: step 54210, loss = 0.73 (790.3 examples/sec; 0.162 sec/batch)
2017-03-25 23:17:51.526038: step 54220, loss = 0.76 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:17:53.262761: step 54230, loss = 0.73 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:17:54.981527: step 54240, loss = 0.76 (744.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:17:56.711779: step 54250, loss = 0.66 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:17:58.451755: step 54260, loss = 0.66 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:18:00.174294: step 54270, loss = 0.87 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:18:01.927916: step 54280, loss = 0.77 (729.9 examples/sec; 0.175 sec/batch)
2017-03-25 23:18:03.650560: step 54290, loss = 0.75 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:18:05.544200: step 54300, loss = 0.69 (675.9 examples/sec; 0.189 sec/batch)
2017-03-25 23:18:07.133569: step 54310, loss = 0.77 (805.4 examples/sec; 0.159 sec/batch)
2017-03-25 23:18:08.873733: step 54320, loss = 0.64 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:18:10.615844: step 54330, loss = 0.74 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:18:12.351782: step 54340, loss = 0.68 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:18:14.096385: step 54350, loss = 0.62 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:18:15.831701: step 54360, loss = 0.65 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:18:17.557625: step 54370, loss = 0.71 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:18:19.309629: step 54380, loss = 0.78 (730.6 examples/sec; 0.175 sec/batch)
2017-03-25 23:18:21.041539: step 54390, loss = 0.77 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:18:22.926897: step 54400, loss = 0.81 (679.1 examples/sec; 0.188 sec/batch)
2017-03-25 23:18:24.564077: step 54410, loss = 0.73 (781.6 examples/sec; 0.164 sec/batch)
2017-03-25 23:18:26.302594: step 54420, loss = 0.84 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:18:28.039274: step 54430, loss = 0.67 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:18:29.779404: step 54440, loss = 0.52 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:18:31.510613: step 54450, loss = 0.78 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:18:33.245465: step 54460, loss = 0.73 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:18:34.968570: step 54470, loss = 0.75 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:18:36.702286: step 54480, loss = 0.68 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:18:38.439007: step 54490, loss = 0.70 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:18:40.335857: step 54500, loss = 0.81 (674.8 examples/sec; 0.190 sec/batch)
2017-03-25 23:18:41.937494: step 54510, loss = 0.88 (799.2 examples/sec; 0.160 sec/batch)
2017-03-25 23:18:43.656487: step 54520, loss = 0.91 (744.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:18:45.383536: step 54530, loss = 0.86 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:18:47.119827: step 54540, loss = 0.71 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:18:48.843424: step 54550, loss = 0.80 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:18:50.567758: step 54560, loss = 0.64 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:18:52.300599: step 54570, loss = 0.68 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:18:54.030520: step 54580, loss = 0.70 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:18:55.749682: step 54590, loss = 0.69 (744.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:18:57.630707: step 54600, loss = 0.71 (680.5 examples/sec; 0.188 sec/batch)
2017-03-25 23:18:59.257791: step 54610, loss = 0.73 (786.7 examples/sec; 0.163 sec/batch)
2017-03-25 23:19:00.997579: step 54620, loss = 0.76 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:19:02.724845: step 54630, loss = 0.75 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:04.454739: step 54640, loss = 0.60 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:06.185276: step 54650, loss = 0.72 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:07.909931: step 54660, loss = 0.74 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:19:09.666780: step 54670, loss = 0.69 (728.6 examples/sec; 0.176 sec/batch)
2017-03-25 23:19:11.394869: step 54680, loss = 0.76 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:13.142633: step 54690, loss = 0.64 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 23:19:15.024481: step 54700, loss = 0.65 (680.2 examples/sec; 0.188 sec/batch)
2017-03-25 23:19:16.635846: step 54710, loss = 0.91 (794.3 examples/sec; 0.161 sec/batch)
2017-03-25 23:19:18.367343: step 54720, loss = 0.69 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:20.104852: step 54730, loss = 0.57 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:19:21.834548: step 54740, loss = 0.81 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:23.566028: step 54750, loss = 0.77 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:25.293765: step 54760, loss = 0.72 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:27.024092: step 54770, loss = 0.74 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:28.752947: step 54780, loss = 0.60 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:30.481293: step 54790, loss = 0.67 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:32.417865: step 54800, loss = 0.84 (661.2 examples/sec; 0.194 sec/batch)
2017-03-25 23:19:33.990285: step 54810, loss = 0.68 (813.6 examples/sec; 0.157 sec/batch)
2017-03-25 23:19:35.730293: step 54820, loss = 0.90 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:19:37.464762: step 54830, loss = 0.66 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:39.190552: step 54840, loss = 0.69 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:40.931449: step 54850, loss = 0.79 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:19:42.667191: step 54860, loss = 0.64 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:19:44.412527: step 54870, loss = 0.56 (733.4 examples/sec; 0.175 sec/batch)
2017-03-25 23:19:46.146938: step 54880, loss = 0.64 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:47.882559: step 54890, loss = 0.77 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:19:49.762698: step 54900, loss = 0.64 (681.1 examples/sec; 0.188 sec/batch)
2017-03-25 23:19:51.381461: step 54910, loss = 0.82 (790.3 examples/sec; 0.162 sec/batch)
2017-03-25 23:19:53.114508: step 54920, loss = 0.71 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:54.843311: step 54930, loss = 0.69 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:56.570985: step 54940, loss = 0.69 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:19:58.319302: step 54950, loss = 0.91 (732.1 examples/sec; 0.175 sec/batch)
2017-03-25 23:20:00.048784: step 54960, loss = 0.62 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:20:01.781416: step 54970, loss = 0.65 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:20:03.519369: step 54980, loss = 0.78 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:20:05.271586: step 54990, loss = 0.80 (730.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:20:07.154036: step 55000, loss = 0.69 (680.2 examples/sec; 0.188 sec/batch)
2017-03-25 23:20:08.770617: step 55010, loss = 0.71 (791.5 examples/sec; 0.162 sec/batch)
2017-03-25 23:20:10.509429: step 55020, loss = 0.69 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:20:12.245800: step 55030, loss = 0.60 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:20:13.959863: step 55040, loss = 0.74 (746.8 examples/sec; 0.171 sec/batch)
2017-03-25 23:20:15.702947: step 55050, loss = 0.76 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:20:17.432679: step 55060, loss = 0.91 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:20:19.161692: step 55070, loss = 0.74 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:20:20.894762: step 55080, loss = 0.89 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:20:22.622006: step 55090, loss = 0.63 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:20:24.559299: step 55100, loss = 0.89 (661.0 examples/sec; 0.194 sec/batch)
2017-03-25 23:20:26.133298: step 55110, loss = 0.73 (812.9 examples/sec; 0.157 sec/batch)
2017-03-25 23:20:27.858678: step 55120, loss = 0.64 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:20:29.607210: step 55130, loss = 0.73 (732.0 examples/sec; 0.175 sec/batch)
2017-03-25 23:20:31.328726: step 55140, loss = 0.71 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:20:33.067189: step 55150, loss = 0.86 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:20:34.788925: step 55160, loss = 0.71 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:20:36.521306: step 55170, loss = 1.20 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:20:38.256063: step 55180, loss = 0.78 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:20:39.986294: step 55190, loss = 0.69 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:20:41.878303: step 55200, loss = 0.70 (676.5 examples/sec; 0.189 sec/batch)
2017-03-25 23:20:43.484705: step 55210, loss = 0.80 (796.8 examples/sec; 0.161 sec/batch)
2017-03-25 23:20:45.234367: step 55220, loss = 0.74 (731.6 examples/sec; 0.175 sec/batch)
2017-03-25 23:20:46.982385: step 55230, loss = 0.74 (732.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:20:48.714390: step 55240, loss = 0.67 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:20:50.445859: step 55250, loss = 0.74 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:20:52.171656: step 55260, loss = 0.71 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:20:53.938041: step 55270, loss = 0.82 (724.7 examples/sec; 0.177 sec/batch)
2017-03-25 23:20:55.680084: step 55280, loss = 0.74 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:20:57.399951: step 55290, loss = 0.73 (744.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:20:59.406888: step 55300, loss = 0.66 (637.9 examples/sec; 0.201 sec/batch)
2017-03-25 23:21:00.894830: step 55310, loss = 0.65 (860.0 examples/sec; 0.149 sec/batch)
2017-03-25 23:21:02.604478: step 55320, loss = 0.79 (748.7 examples/sec; 0.171 sec/batch)
2017-03-25 23:21:04.333124: step 55330, loss = 0.71 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:21:06.074383: step 55340, loss = 0.79 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:21:07.808398: step 55350, loss = 0.62 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:21:09.542639: step 55360, loss = 0.62 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:21:11.270909: step 55370, loss = 0.60 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:21:13.001990: step 55380, loss = 0.78 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:21:14.739019: step 55390, loss = 0.89 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:21:16.628983: step 55400, loss = 0.73 (677.3 examples/sec; 0.189 sec/batch)
2017-03-25 23:21:18.244507: step 55410, loss = 0.65 (792.3 examples/sec; 0.162 sec/batch)
2017-03-25 23:21:19.966725: step 55420, loss = 0.66 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:21:21.709927: step 55430, loss = 0.74 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:21:23.432384: step 55440, loss = 0.64 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:21:25.178298: step 55450, loss = 0.80 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 23:21:26.900479: step 55460, loss = 0.75 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:21:28.629443: step 55470, loss = 0.73 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:21:30.374088: step 55480, loss = 0.64 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:21:32.111098: step 55490, loss = 0.81 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:21:34.008141: step 55500, loss = 0.73 (674.7 examples/sec; 0.190 sec/batch)
2017-03-25 23:21:35.625721: step 55510, loss = 0.79 (791.4 examples/sec; 0.162 sec/batch)
2017-03-25 23:21:37.362347: step 55520, loss = 0.76 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:21:39.088410: step 55530, loss = 0.82 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:21:40.823206: step 55540, loss = 0.60 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:21:42.557444: step 55550, loss = 0.73 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:21:44.291212: step 55560, loss = 0.85 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:21:46.026275: step 55570, loss = 0.79 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:21:47.746002: step 55580, loss = 0.72 (744.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:21:49.483771: step 55590, loss = 0.90 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:21:51.373463: step 55600, loss = 0.80 (677.4 examples/sec; 0.189 sec/batch)
2017-03-25 23:21:53.027234: step 55610, loss = 0.64 (774.0 examples/sec; 0.165 sec/batch)
2017-03-25 23:21:54.751729: step 55620, loss = 0.91 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:21:56.480295: step 55630, loss = 0.80 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:21:58.232211: step 55640, loss = 0.58 (730.6 examples/sec; 0.175 sec/batch)
2017-03-25 23:21:59.966389: step 55650, loss = 0.76 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:22:01.697652: step 55660, loss = 0.69 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:22:03.438401: step 55670, loss = 0.61 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:22:05.167802: step 55680, loss = 0.71 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:22:06.904964: step 55690, loss = 0.64 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:22:08.870097: step 55700, loss = 0.67 (651.4 examples/sec; 0.197 sec/batch)
2017-03-25 23:22:10.404758: step 55710, loss = 0.68 (834.1 examples/sec; 0.153 sec/batch)
2017-03-25 23:22:12.136807: step 55720, loss = 0.76 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:22:13.872613: step 55730, loss = 0.89 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:22:15.609460: step 55740, loss = 0.78 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:22:17.362594: step 55750, loss = 0.71 (730.1 examples/sec; 0.175 sec/batch)
2017-03-25 23:22:19.083711: step 55760, loss = 0.64 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:22:20.820852: step 55770, loss = 0.64 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:22:22.564800: step 55780, loss = 0.84 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:22:24.285270: step 55790, loss = 0.81 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:22:26.209063: step 55800, loss = 0.64 (665.7 examples/sec; 0.192 sec/batch)
2017-03-25 23:22:27.803898: step 55810, loss = 0.66 (802.1 examples/sec; 0.160 sec/batch)
2017-03-25 23:22:29.546365: step 55820, loss = 0.84 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:22:31.281010: step 55830, loss = 0.87 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:22:33.026816: step 55840, loss = 0.89 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 23:22:34.768680: step 55850, loss = 0.62 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:22:36.512632: step 55860, loss = 0.69 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:22:38.243370: step 55870, loss = 0.75 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:22:39.962401: step 55880, loss = 0.72 (744.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:22:41.715908: step 55890, loss = 0.65 (729.9 examples/sec; 0.175 sec/batch)
2017-03-25 23:22:43.616461: step 55900, loss = 0.72 (673.5 examples/sec; 0.190 sec/batch)
2017-03-25 23:22:45.193446: step 55910, loss = 0.76 (811.7 examples/sec; 0.158 sec/batch)
2017-03-25 23:22:46.928607: step 55920, loss = 0.74 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:22:48.660616: step 55930, loss = 0.68 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:22:50.399679: step 55940, loss = 0.72 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:22:52.124651: step 55950, loss = 0.65 (742.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:22:53.859412: step 55960, loss = 0.89 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:22:55.590456: step 55970, loss = 0.72 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:22:57.323579: step 55980, loss = 0.63 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:22:59.060961: step 55990, loss = 0.70 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:23:00.965014: step 56000, loss = 0.88 (672.2 examples/sec; 0.190 sec/batch)
2017-03-25 23:23:02.587698: step 56010, loss = 0.96 (788.8 examples/sec; 0.162 sec/batch)
2017-03-25 23:23:04.325914: step 56020, loss = 0.83 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:23:06.047435: step 56030, loss = 0.64 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:23:07.794731: step 56040, loss = 0.75 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 23:23:09.531363: step 56050, loss = 0.84 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:23:11.271635: step 56060, loss = 0.66 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:23:13.018485: step 56070, loss = 0.76 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 23:23:14.747946: step 56080, loss = 0.71 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:23:16.496014: step 56090, loss = 0.84 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 23:23:18.416085: step 56100, loss = 0.76 (666.7 examples/sec; 0.192 sec/batch)
2017-03-25 23:23:19.999811: step 56110, loss = 0.77 (808.2 examples/sec; 0.158 sec/batch)
2017-03-25 23:23:21.739679: step 56120, loss = 0.80 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:23:23.475335: step 56130, loss = 0.62 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:23:25.199523: step 56140, loss = 0.66 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:23:26.927331: step 56150, loss = 0.52 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:23:28.658191: step 56160, loss = 0.57 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:23:30.384608: step 56170, loss = 0.71 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:23:32.116821: step 56180, loss = 0.89 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:23:33.852621: step 56190, loss = 0.71 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:23:35.746327: step 56200, loss = 0.66 (676.1 examples/sec; 0.189 sec/batch)
2017-03-25 23:23:37.347845: step 56210, loss = 0.91 (798.9 examples/sec; 0.160 sec/batch)
2017-03-25 23:23:39.088539: step 56220, loss = 0.74 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:23:40.826330: step 56230, loss = 0.71 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:23:42.547497: step 56240, loss = 0.69 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:23:44.292591: step 56250, loss = 0.67 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:23:46.039828: step 56260, loss = 0.75 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 23:23:47.759707: step 56270, loss = 0.87 (744.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:23:49.497429: step 56280, loss = 0.81 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:23:51.236435: step 56290, loss = 0.63 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:23:53.136295: step 56300, loss = 0.65 (674.1 examples/sec; 0.190 sec/batch)
2017-03-25 23:23:54.747967: step 56310, loss = 0.86 (793.7 examples/sec; 0.161 sec/batch)
2017-03-25 23:23:56.475125: step 56320, loss = 0.63 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:23:58.209649: step 56330, loss = 0.75 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:23:59.945052: step 56340, loss = 0.72 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:24:01.683873: step 56350, loss = 0.67 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:24:03.408361: step 56360, loss = 0.67 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:24:05.147817: step 56370, loss = 0.88 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:24:06.874823: step 56380, loss = 0.71 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:24:08.610614: step 56390, loss = 0.70 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:24:10.496547: step 56400, loss = 0.77 (678.7 examples/sec; 0.189 sec/batch)
2017-03-25 23:24:12.104759: step 56410, loss = 0.73 (795.9 examples/sec; 0.161 sec/batch)
2017-03-25 23:24:13.852846: step 56420, loss = 0.90 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 23:24:15.574553: step 56430, loss = 0.71 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:24:17.298899: step 56440, loss = 0.72 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:24:19.026702: step 56450, loss = 0.79 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:24:20.756737: step 56460, loss = 0.80 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:24:22.499106: step 56470, loss = 0.70 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:24:24.243126: step 56480, loss = 0.97 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:24:25.976255: step 56490, loss = 0.64 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:24:27.873118: step 56500, loss = 0.74 (674.8 examples/sec; 0.190 sec/batch)
2017-03-25 23:24:29.486668: step 56510, loss = 0.91 (793.3 examples/sec; 0.161 sec/batch)
2017-03-25 23:24:31.218620: step 56520, loss = 0.61 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:24:32.932889: step 56530, loss = 0.73 (746.7 examples/sec; 0.171 sec/batch)
2017-03-25 23:24:34.666900: step 56540, loss = 0.58 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:24:36.395594: step 56550, loss = 0.71 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:24:38.126855: step 56560, loss = 0.85 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:24:39.856826: step 56570, loss = 0.78 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:24:41.590275: step 56580, loss = 0.73 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:24:43.321274: step 56590, loss = 0.66 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:24:45.273544: step 56600, loss = 0.73 (656.0 examples/sec; 0.195 sec/batch)
2017-03-25 23:24:46.788898: step 56610, loss = 0.76 (844.0 examples/sec; 0.152 sec/batch)
2017-03-25 23:24:48.478974: step 56620, loss = 0.72 (757.4 examples/sec; 0.169 sec/batch)
2017-03-25 23:24:50.201917: step 56630, loss = 0.77 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:24:51.941401: step 56640, loss = 0.67 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:24:53.669691: step 56650, loss = 0.82 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:24:55.427103: step 56660, loss = 0.69 (728.3 examples/sec; 0.176 sec/batch)
2017-03-25 23:24:57.148926: step 56670, loss = 0.83 (743.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:24:58.888139: step 56680, loss = 0.78 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:25:00.625739: step 56690, loss = 0.86 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:25:02.579188: step 56700, loss = 0.84 (655.6 examples/sec; 0.195 sec/batch)
2017-03-25 23:25:04.130928: step 56710, loss = 0.76 (824.4 examples/sec; 0.155 sec/batch)
2017-03-25 23:25:05.854675: step 56720, loss = 0.71 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:25:07.596782: step 56730, loss = 0.85 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:25:09.331748: step 56740, loss = 0.71 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:25:11.075251: step 56750, loss = 0.75 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:25:12.818292: step 56760, loss = 0.59 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:25:14.550732: step 56770, loss = 0.61 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:25:16.298969: step 56780, loss = 0.77 (732.1 examples/sec; 0.175 sec/batch)
2017-03-25 23:25:18.036360: step 56790, loss = 0.76 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:25:19.960151: step 56800, loss = 0.73 (665.6 examples/sec; 0.192 sec/batch)
2017-03-25 23:25:21.528547: step 56810, loss = 0.77 (815.8 examples/sec; 0.157 sec/batch)
2017-03-25 23:25:23.250905: step 56820, loss = 0.68 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:25:24.993483: step 56830, loss = 0.72 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:25:26.724744: step 56840, loss = 0.64 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:25:28.467350: step 56850, loss = 0.80 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:25:30.203389: step 56860, loss = 0.69 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:25:31.929334: step 56870, loss = 0.73 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:25:33.655937: step 56880, loss = 0.82 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:25:35.389493: step 56890, loss = 0.72 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:25:37.272485: step 56900, loss = 0.73 (680.5 examples/sec; 0.188 sec/batch)
2017-03-25 23:25:38.901532: step 56910, loss = 0.91 (784.9 examples/sec; 0.163 sec/batch)
2017-03-25 23:25:40.629619: step 56920, loss = 0.64 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:25:42.356312: step 56930, loss = 0.81 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:25:44.082876: step 56940, loss = 0.79 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:25:45.816127: step 56950, loss = 0.86 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:25:47.547974: step 56960, loss = 0.62 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:25:49.281544: step 56970, loss = 0.80 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:25:51.007612: step 56980, loss = 0.82 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:25:52.744979: step 56990, loss = 0.64 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:25:54.684129: step 57000, loss = 0.74 (660.1 examples/sec; 0.194 sec/batch)
2017-03-25 23:25:56.235674: step 57010, loss = 0.75 (825.0 examples/sec; 0.155 sec/batch)
2017-03-25 23:25:57.975077: step 57020, loss = 0.68 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:25:59.707898: step 57030, loss = 0.65 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:26:01.447479: step 57040, loss = 0.81 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:26:03.180826: step 57050, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:26:04.922646: step 57060, loss = 0.72 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:26:06.659189: step 57070, loss = 0.69 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:26:08.383137: step 57080, loss = 0.68 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:26:10.126440: step 57090, loss = 0.62 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:26:12.020047: step 57100, loss = 0.59 (676.3 examples/sec; 0.189 sec/batch)
2017-03-25 23:26:13.633585: step 57110, loss = 0.71 (792.8 examples/sec; 0.161 sec/batch)
2017-03-25 23:26:15.372164: step 57120, loss = 0.81 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:26:17.095155: step 57130, loss = 0.70 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:26:18.837367: step 57140, loss = 0.79 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:26:20.578287: step 57150, loss = 0.75 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:26:22.338830: step 57160, loss = 0.64 (727.1 examples/sec; 0.176 sec/batch)
2017-03-25 23:26:24.078451: step 57170, loss = 0.68 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:26:25.804724: step 57180, loss = 0.75 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:26:27.543269: step 57190, loss = 0.74 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:26:29.441480: step 57200, loss = 0.69 (674.3 examples/sec; 0.190 sec/batch)
2017-03-25 23:26:31.067227: step 57210, loss = 0.84 (787.3 examples/sec; 0.163 sec/batch)
2017-03-25 23:26:32.812667: step 57220, loss = 0.74 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:26:34.549911: step 57230, loss = 0.67 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:26:36.276665: step 57240, loss = 0.77 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:26:38.018831: step 57250, loss = 0.62 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:26:39.749205: step 57260, loss = 0.73 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:26:41.486111: step 57270, loss = 0.75 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:26:43.216672: step 57280, loss = 0.67 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:26:44.953784: step 57290, loss = 0.69 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:26:46.851670: step 57300, loss = 0.77 (674.5 examples/sec; 0.190 sec/batch)
2017-03-25 23:26:48.463383: step 57310, loss = 0.86 (794.1 examples/sec; 0.161 sec/batch)
2017-03-25 23:26:50.188936: step 57320, loss = 0.91 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:26:51.915701: step 57330, loss = 0.84 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:26:53.649832: step 57340, loss = 0.79 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:26:55.373405: step 57350, loss = 0.82 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:26:57.107261: step 57360, loss = 0.73 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:26:58.844755: step 57370, loss = 0.79 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:27:00.577162: step 57380, loss = 0.85 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:02.304059: step 57390, loss = 0.62 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:04.205664: step 57400, loss = 0.81 (673.5 examples/sec; 0.190 sec/batch)
2017-03-25 23:27:05.808894: step 57410, loss = 0.83 (798.0 examples/sec; 0.160 sec/batch)
2017-03-25 23:27:07.545160: step 57420, loss = 0.73 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:27:09.270486: step 57430, loss = 0.81 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:11.003708: step 57440, loss = 0.69 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:12.738395: step 57450, loss = 0.68 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:14.475317: step 57460, loss = 0.66 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:27:16.208963: step 57470, loss = 0.69 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:17.936842: step 57480, loss = 0.80 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:19.662634: step 57490, loss = 0.88 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:21.548755: step 57500, loss = 0.66 (678.6 examples/sec; 0.189 sec/batch)
2017-03-25 23:27:23.175710: step 57510, loss = 0.64 (786.5 examples/sec; 0.163 sec/batch)
2017-03-25 23:27:24.928225: step 57520, loss = 0.73 (730.4 examples/sec; 0.175 sec/batch)
2017-03-25 23:27:26.671800: step 57530, loss = 0.81 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:27:28.410648: step 57540, loss = 0.81 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:27:30.143551: step 57550, loss = 0.84 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:31.887417: step 57560, loss = 0.54 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:27:33.615981: step 57570, loss = 0.78 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:35.352623: step 57580, loss = 0.80 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:27:37.086591: step 57590, loss = 0.76 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:38.973458: step 57600, loss = 0.77 (678.6 examples/sec; 0.189 sec/batch)
2017-03-25 23:27:40.567697: step 57610, loss = 0.71 (802.9 examples/sec; 0.159 sec/batch)
2017-03-25 23:27:42.305279: step 57620, loss = 0.65 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:27:44.038093: step 57630, loss = 0.72 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:45.760128: step 57640, loss = 1.00 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:27:47.487514: step 57650, loss = 0.80 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:49.219372: step 57660, loss = 0.63 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:50.966161: step 57670, loss = 0.81 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 23:27:52.694017: step 57680, loss = 0.77 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:27:54.417589: step 57690, loss = 0.67 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:27:56.309806: step 57700, loss = 0.74 (676.1 examples/sec; 0.189 sec/batch)
2017-03-25 23:27:57.924432: step 57710, loss = 0.67 (792.6 examples/sec; 0.161 sec/batch)
2017-03-25 23:27:59.658130: step 57720, loss = 0.98 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:01.389701: step 57730, loss = 0.74 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:03.133688: step 57740, loss = 0.76 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:28:04.856722: step 57750, loss = 0.75 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:28:06.596395: step 57760, loss = 0.60 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:28:08.317637: step 57770, loss = 0.86 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:28:10.041351: step 57780, loss = 0.87 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:28:11.775898: step 57790, loss = 0.81 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:13.667624: step 57800, loss = 0.61 (677.0 examples/sec; 0.189 sec/batch)
2017-03-25 23:28:15.277093: step 57810, loss = 0.65 (794.7 examples/sec; 0.161 sec/batch)
2017-03-25 23:28:17.008507: step 57820, loss = 0.75 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:18.740768: step 57830, loss = 0.68 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:20.468561: step 57840, loss = 0.75 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:22.215696: step 57850, loss = 0.71 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:28:23.948652: step 57860, loss = 0.73 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:25.695222: step 57870, loss = 0.62 (732.9 examples/sec; 0.175 sec/batch)
2017-03-25 23:28:27.420562: step 57880, loss = 0.81 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:29.153830: step 57890, loss = 0.68 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:31.048346: step 57900, loss = 0.96 (675.8 examples/sec; 0.189 sec/batch)
2017-03-25 23:28:32.651091: step 57910, loss = 0.75 (798.3 examples/sec; 0.160 sec/batch)
2017-03-25 23:28:34.384504: step 57920, loss = 0.75 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:36.124473: step 57930, loss = 0.65 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:28:37.854618: step 57940, loss = 0.87 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:39.585860: step 57950, loss = 0.87 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:41.317944: step 57960, loss = 0.66 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:43.046274: step 57970, loss = 0.79 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:44.782405: step 57980, loss = 0.80 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:28:46.523412: step 57990, loss = 0.73 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:28:48.423712: step 58000, loss = 0.74 (673.8 examples/sec; 0.190 sec/batch)
2017-03-25 23:28:50.033747: step 58010, loss = 1.01 (794.7 examples/sec; 0.161 sec/batch)
2017-03-25 23:28:51.772702: step 58020, loss = 0.77 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:28:53.503681: step 58030, loss = 0.75 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:28:55.242103: step 58040, loss = 0.83 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:28:56.977967: step 58050, loss = 0.77 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:28:58.703577: step 58060, loss = 0.80 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:00.430845: step 58070, loss = 0.69 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:02.169024: step 58080, loss = 0.82 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:29:03.913352: step 58090, loss = 0.70 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:29:05.796545: step 58100, loss = 0.78 (679.7 examples/sec; 0.188 sec/batch)
2017-03-25 23:29:07.400195: step 58110, loss = 0.71 (798.2 examples/sec; 0.160 sec/batch)
2017-03-25 23:29:09.140568: step 58120, loss = 0.77 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:29:10.882170: step 58130, loss = 0.73 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:29:12.610013: step 58140, loss = 0.76 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:14.344360: step 58150, loss = 0.73 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:16.077562: step 58160, loss = 0.67 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:17.819695: step 58170, loss = 0.79 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:29:19.551145: step 58180, loss = 0.84 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:21.286990: step 58190, loss = 0.58 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:29:23.183106: step 58200, loss = 1.00 (675.2 examples/sec; 0.190 sec/batch)
2017-03-25 23:29:24.790381: step 58210, loss = 0.67 (796.1 examples/sec; 0.161 sec/batch)
2017-03-25 23:29:26.517384: step 58220, loss = 0.77 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:28.246842: step 58230, loss = 0.74 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:29.988503: step 58240, loss = 0.98 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:29:31.716181: step 58250, loss = 0.77 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:33.467630: step 58260, loss = 0.91 (730.8 examples/sec; 0.175 sec/batch)
2017-03-25 23:29:35.200552: step 58270, loss = 0.67 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:36.943032: step 58280, loss = 0.76 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:29:38.677243: step 58290, loss = 0.71 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:40.565065: step 58300, loss = 0.76 (678.1 examples/sec; 0.189 sec/batch)
2017-03-25 23:29:42.167639: step 58310, loss = 0.69 (798.7 examples/sec; 0.160 sec/batch)
2017-03-25 23:29:43.897335: step 58320, loss = 0.82 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:45.631123: step 58330, loss = 0.77 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:47.368025: step 58340, loss = 0.71 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:29:49.103684: step 58350, loss = 0.71 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:29:50.855534: step 58360, loss = 0.64 (730.7 examples/sec; 0.175 sec/batch)
2017-03-25 23:29:52.589052: step 58370, loss = 0.74 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:54.320086: step 58380, loss = 0.90 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:29:56.059049: step 58390, loss = 0.76 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:29:57.938949: step 58400, loss = 0.69 (680.9 examples/sec; 0.188 sec/batch)
2017-03-25 23:29:59.555422: step 58410, loss = 0.72 (791.9 examples/sec; 0.162 sec/batch)
2017-03-25 23:30:01.291682: step 58420, loss = 0.76 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:30:03.039786: step 58430, loss = 0.74 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 23:30:04.774133: step 58440, loss = 0.78 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:30:06.516830: step 58450, loss = 0.72 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:30:08.264280: step 58460, loss = 0.67 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:30:09.993307: step 58470, loss = 0.75 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:30:11.731028: step 58480, loss = 0.69 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:30:13.463248: step 58490, loss = 0.58 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:30:15.369966: step 58500, loss = 0.73 (671.5 examples/sec; 0.191 sec/batch)
2017-03-25 23:30:16.987334: step 58510, loss = 0.89 (791.1 examples/sec; 0.162 sec/batch)
2017-03-25 23:30:18.729854: step 58520, loss = 0.72 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:30:20.467013: step 58530, loss = 0.85 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:30:22.187207: step 58540, loss = 1.04 (744.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:30:23.920992: step 58550, loss = 0.81 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:30:25.654162: step 58560, loss = 0.70 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:30:27.399900: step 58570, loss = 0.79 (733.2 examples/sec; 0.175 sec/batch)
2017-03-25 23:30:29.142106: step 58580, loss = 0.67 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:30:30.863811: step 58590, loss = 0.61 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:30:32.766602: step 58600, loss = 0.68 (673.0 examples/sec; 0.190 sec/batch)
2017-03-25 23:30:34.366802: step 58610, loss = 0.79 (799.5 examples/sec; 0.160 sec/batch)
2017-03-25 23:30:36.096203: step 58620, loss = 0.65 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:30:37.824124: step 58630, loss = 0.66 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:30:39.562231: step 58640, loss = 0.80 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:30:41.296171: step 58650, loss = 0.77 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:30:43.019582: step 58660, loss = 0.76 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:30:44.741676: step 58670, loss = 0.66 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:30:46.467065: step 58680, loss = 0.62 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:30:48.209942: step 58690, loss = 0.83 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:30:50.093572: step 58700, loss = 0.71 (679.9 examples/sec; 0.188 sec/batch)
2017-03-25 23:30:51.710752: step 58710, loss = 0.89 (791.0 examples/sec; 0.162 sec/batch)
2017-03-25 23:30:53.471470: step 58720, loss = 0.71 (727.0 examples/sec; 0.176 sec/batch)
2017-03-25 23:30:55.195528: step 58730, loss = 0.73 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:30:56.923692: step 58740, loss = 0.69 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:30:58.666212: step 58750, loss = 0.69 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:31:00.412355: step 58760, loss = 0.85 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 23:31:02.159734: step 58770, loss = 0.82 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:31:03.890474: step 58780, loss = 0.74 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:31:05.613176: step 58790, loss = 0.79 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:31:07.516014: step 58800, loss = 0.74 (672.7 examples/sec; 0.190 sec/batch)
2017-03-25 23:31:09.112376: step 58810, loss = 0.86 (802.0 examples/sec; 0.160 sec/batch)
2017-03-25 23:31:10.845176: step 58820, loss = 0.58 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:31:12.571841: step 58830, loss = 0.79 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:31:14.306695: step 58840, loss = 0.76 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:31:16.034431: step 58850, loss = 0.62 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:31:17.768086: step 58860, loss = 0.99 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:31:19.499562: step 58870, loss = 0.66 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:31:21.236435: step 58880, loss = 0.61 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:31:22.961378: step 58890, loss = 0.90 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:31:24.851456: step 58900, loss = 0.70 (677.2 examples/sec; 0.189 sec/batch)
2017-03-25 23:31:26.458850: step 58910, loss = 0.67 (796.3 examples/sec; 0.161 sec/batch)
2017-03-25 23:31:28.192503: step 58920, loss = 0.74 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:31:29.923414: step 58930, loss = 0.74 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:31:31.649246: step 58940, loss = 0.83 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:31:33.385971: step 58950, loss = 0.66 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:31:35.107491: step 58960, loss = 0.66 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:31:36.843770: step 58970, loss = 0.75 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:31:38.592186: step 58980, loss = 0.76 (732.1 examples/sec; 0.175 sec/batch)
2017-03-25 23:31:40.340172: step 58990, loss = 0.82 (732.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:31:42.237684: step 59000, loss = 0.85 (674.5 examples/sec; 0.190 sec/batch)
2017-03-25 23:31:43.868766: step 59010, loss = 0.91 (784.8 examples/sec; 0.163 sec/batch)
2017-03-25 23:31:45.608541: step 59020, loss = 0.89 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:31:47.331641: step 59030, loss = 0.67 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:31:49.055838: step 59040, loss = 0.81 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:31:50.794247: step 59050, loss = 0.82 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:31:52.519131: step 59060, loss = 0.83 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:31:54.257169: step 59070, loss = 0.68 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:31:55.993998: step 59080, loss = 0.77 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:31:57.716625: step 59090, loss = 0.68 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:31:59.622959: step 59100, loss = 0.66 (671.8 examples/sec; 0.191 sec/batch)
2017-03-25 23:32:01.232529: step 59110, loss = 0.76 (794.8 examples/sec; 0.161 sec/batch)
2017-03-25 23:32:02.965911: step 59120, loss = 0.69 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:32:04.695449: step 59130, loss = 0.73 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:32:06.429427: step 59140, loss = 0.56 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:32:08.168049: step 59150, loss = 0.86 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:32:09.897155: step 59160, loss = 0.63 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:32:11.624078: step 59170, loss = 0.81 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:32:13.372768: step 59180, loss = 0.63 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 23:32:15.094411: step 59190, loss = 0.73 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:32:16.999452: step 59200, loss = 0.79 (672.1 examples/sec; 0.190 sec/batch)
2017-03-25 23:32:18.596433: step 59210, loss = 0.82 (801.2 examples/sec; 0.160 sec/batch)
2017-03-25 23:32:20.334010: step 59220, loss = 0.56 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:32:22.062496: step 59230, loss = 0.78 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:32:23.803184: step 59240, loss = 0.94 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:32:25.538528: step 59250, loss = 0.74 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:32:27.270346: step 59260, loss = 0.68 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:32:29.008098: step 59270, loss = 0.71 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:32:30.731762: step 59280, loss = 0.60 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:32:32.478248: step 59290, loss = 0.80 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 23:32:34.364787: step 59300, loss = 0.69 (678.4 examples/sec; 0.189 sec/batch)
2017-03-25 23:32:35.973233: step 59310, loss = 0.69 (795.8 examples/sec; 0.161 sec/batch)
2017-03-25 23:32:37.721347: step 59320, loss = 0.76 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 23:32:39.451281: step 59330, loss = 0.59 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:32:41.191698: step 59340, loss = 0.88 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:32:42.918298: step 59350, loss = 0.77 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:32:44.661031: step 59360, loss = 1.06 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:32:46.385699: step 59370, loss = 0.68 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:32:48.108464: step 59380, loss = 0.76 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:32:49.830391: step 59390, loss = 0.59 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:32:51.744975: step 59400, loss = 0.65 (668.6 examples/sec; 0.191 sec/batch)
2017-03-25 23:32:53.347412: step 59410, loss = 0.59 (798.8 examples/sec; 0.160 sec/batch)
2017-03-25 23:32:55.087839: step 59420, loss = 0.75 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:32:56.813081: step 59430, loss = 0.57 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:32:58.548633: step 59440, loss = 0.65 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:33:00.292720: step 59450, loss = 0.77 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:33:02.031542: step 59460, loss = 0.66 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:33:03.754794: step 59470, loss = 0.70 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:33:05.487845: step 59480, loss = 0.68 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:33:07.231271: step 59490, loss = 0.83 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:33:09.112671: step 59500, loss = 0.87 (680.3 examples/sec; 0.188 sec/batch)
2017-03-25 23:33:10.722704: step 59510, loss = 0.82 (795.0 examples/sec; 0.161 sec/batch)
2017-03-25 23:33:12.459160: step 59520, loss = 0.66 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:33:14.195505: step 59530, loss = 1.00 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:33:15.943761: step 59540, loss = 0.70 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 23:33:17.688889: step 59550, loss = 0.73 (733.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:33:19.440743: step 59560, loss = 0.68 (730.7 examples/sec; 0.175 sec/batch)
2017-03-25 23:33:21.185359: step 59570, loss = 0.71 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:33:22.909622: step 59580, loss = 0.70 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:33:24.650994: step 59590, loss = 0.92 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:33:26.556997: step 59600, loss = 0.61 (671.6 examples/sec; 0.191 sec/batch)
2017-03-25 23:33:28.166941: step 59610, loss = 0.79 (795.1 examples/sec; 0.161 sec/batch)
2017-03-25 23:33:29.898717: step 59620, loss = 0.85 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:33:31.639023: step 59630, loss = 0.63 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:33:33.367339: step 59640, loss = 0.80 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:33:35.107878: step 59650, loss = 0.69 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:33:36.842424: step 59660, loss = 0.67 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:33:38.583462: step 59670, loss = 0.67 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:33:40.312940: step 59680, loss = 0.76 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:33:42.041920: step 59690, loss = 0.64 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:33:43.942449: step 59700, loss = 0.77 (674.0 examples/sec; 0.190 sec/batch)
2017-03-25 23:33:45.553600: step 59710, loss = 0.62 (793.8 examples/sec; 0.161 sec/batch)
2017-03-25 23:33:47.281922: step 59720, loss = 0.80 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:33:49.010680: step 59730, loss = 0.61 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:33:50.737342: step 59740, loss = 0.71 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:33:52.475674: step 59750, loss = 0.67 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:33:54.203112: step 59760, loss = 0.83 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:33:55.955848: step 59770, loss = 0.64 (730.2 examples/sec; 0.175 sec/batch)
2017-03-25 23:33:57.685977: step 59780, loss = 0.67 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:33:59.411571: step 59790, loss = 0.69 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:01.315648: step 59800, loss = 0.77 (672.6 examples/sec; 0.190 sec/batch)
2017-03-25 23:34:02.939057: step 59810, loss = 0.60 (788.0 examples/sec; 0.162 sec/batch)
2017-03-25 23:34:04.670621: step 59820, loss = 0.79 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:06.401913: step 59830, loss = 0.78 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:08.137099: step 59840, loss = 0.87 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:34:09.863647: step 59850, loss = 0.60 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:11.597819: step 59860, loss = 0.71 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:13.328946: step 59870, loss = 0.84 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:15.056525: step 59880, loss = 0.66 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:16.793049: step 59890, loss = 0.69 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:34:18.667718: step 59900, loss = 0.66 (683.1 examples/sec; 0.187 sec/batch)
2017-03-25 23:34:20.277655: step 59910, loss = 0.76 (794.8 examples/sec; 0.161 sec/batch)
2017-03-25 23:34:22.003467: step 59920, loss = 0.61 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:23.726558: step 59930, loss = 0.80 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:34:25.467525: step 59940, loss = 0.78 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:34:27.204745: step 59950, loss = 0.68 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:34:28.919095: step 59960, loss = 0.71 (746.6 examples/sec; 0.171 sec/batch)
2017-03-25 23:34:30.644934: step 59970, loss = 0.72 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:32.377877: step 59980, loss = 0.67 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:34.109426: step 59990, loss = 0.91 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:36.005753: step 60000, loss = 0.81 (675.0 examples/sec; 0.190 sec/batch)
2017-03-25 23:34:37.611233: step 60010, loss = 0.71 (797.3 examples/sec; 0.161 sec/batch)
2017-03-25 23:34:39.335587: step 60020, loss = 0.60 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:34:41.069643: step 60030, loss = 0.72 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:42.817748: step 60040, loss = 0.77 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 23:34:44.549816: step 60050, loss = 0.77 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:46.282152: step 60060, loss = 0.77 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:48.007303: step 60070, loss = 0.72 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:34:49.728996: step 60080, loss = 0.83 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:34:51.480242: step 60090, loss = 0.72 (731.0 examples/sec; 0.175 sec/batch)
2017-03-25 23:34:53.383998: step 60100, loss = 0.69 (672.3 examples/sec; 0.190 sec/batch)
2017-03-25 23:34:54.972356: step 60110, loss = 0.89 (805.9 examples/sec; 0.159 sec/batch)
2017-03-25 23:34:56.697279: step 60120, loss = 0.78 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:34:58.429612: step 60130, loss = 0.75 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:35:00.167249: step 60140, loss = 0.69 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:35:01.901842: step 60150, loss = 0.59 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:35:03.638037: step 60160, loss = 0.60 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:35:05.376319: step 60170, loss = 0.79 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:35:07.122220: step 60180, loss = 0.71 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 23:35:08.839925: step 60190, loss = 0.55 (745.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:35:10.748026: step 60200, loss = 0.70 (670.8 examples/sec; 0.191 sec/batch)
2017-03-25 23:35:12.364256: step 60210, loss = 0.88 (792.0 examples/sec; 0.162 sec/batch)
2017-03-25 23:35:14.106761: step 60220, loss = 0.76 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:35:15.840871: step 60230, loss = 0.73 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:35:17.569607: step 60240, loss = 0.80 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:35:19.298555: step 60250, loss = 0.76 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:35:21.040985: step 60260, loss = 0.93 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:35:22.775391: step 60270, loss = 0.74 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:35:24.515565: step 60280, loss = 0.50 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:35:26.244773: step 60290, loss = 0.68 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:35:28.142076: step 60300, loss = 0.71 (675.0 examples/sec; 0.190 sec/batch)
2017-03-25 23:35:29.752658: step 60310, loss = 0.66 (794.3 examples/sec; 0.161 sec/batch)
2017-03-25 23:35:31.490156: step 60320, loss = 0.64 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:35:33.226893: step 60330, loss = 0.76 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:35:34.955732: step 60340, loss = 0.60 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:35:36.672570: step 60350, loss = 0.84 (745.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:35:38.397877: step 60360, loss = 0.78 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:35:40.122931: step 60370, loss = 0.85 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:35:41.851375: step 60380, loss = 0.68 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:35:43.597661: step 60390, loss = 0.82 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 23:35:45.485047: step 60400, loss = 0.70 (678.2 examples/sec; 0.189 sec/batch)
2017-03-25 23:35:47.100011: step 60410, loss = 0.60 (792.6 examples/sec; 0.161 sec/batch)
2017-03-25 23:35:48.824524: step 60420, loss = 0.64 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:35:50.547825: step 60430, loss = 0.78 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:35:52.291420: step 60440, loss = 0.74 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:35:54.012786: step 60450, loss = 0.76 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:35:55.757264: step 60460, loss = 0.72 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:35:57.505215: step 60470, loss = 0.63 (732.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:35:59.233525: step 60480, loss = 0.72 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:36:00.973650: step 60490, loss = 0.88 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:36:02.877886: step 60500, loss = 0.62 (672.2 examples/sec; 0.190 sec/batch)
2017-03-25 23:36:04.474549: step 60510, loss = 0.91 (801.7 examples/sec; 0.160 sec/batch)
2017-03-25 23:36:06.208619: step 60520, loss = 0.66 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:36:07.941454: step 60530, loss = 0.73 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:36:09.659685: step 60540, loss = 0.81 (745.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:36:11.397168: step 60550, loss = 0.70 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:36:13.127512: step 60560, loss = 0.63 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:36:14.861584: step 60570, loss = 0.65 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:36:16.590565: step 60580, loss = 0.81 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:36:18.311144: step 60590, loss = 0.65 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:36:20.200740: step 60600, loss = 0.84 (677.4 examples/sec; 0.189 sec/batch)
2017-03-25 23:36:21.803674: step 60610, loss = 0.76 (798.5 examples/sec; 0.160 sec/batch)
2017-03-25 23:36:23.523861: step 60620, loss = 0.79 (744.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:36:25.261950: step 60630, loss = 0.81 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:36:26.999212: step 60640, loss = 0.73 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:36:28.744196: step 60650, loss = 0.64 (733.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:36:30.478256: step 60660, loss = 0.80 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:36:32.207106: step 60670, loss = 0.70 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:36:33.942050: step 60680, loss = 0.62 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:36:35.679248: step 60690, loss = 0.64 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:36:37.612694: step 60700, loss = 0.73 (662.2 examples/sec; 0.193 sec/batch)
2017-03-25 23:36:39.162297: step 60710, loss = 0.84 (825.7 examples/sec; 0.155 sec/batch)
2017-03-25 23:36:40.906491: step 60720, loss = 0.71 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:36:42.644777: step 60730, loss = 0.90 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:36:44.371414: step 60740, loss = 0.67 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:36:46.096481: step 60750, loss = 0.60 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:36:47.838546: step 60760, loss = 0.81 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:36:49.573942: step 60770, loss = 0.71 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:36:51.303745: step 60780, loss = 0.86 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:36:53.041933: step 60790, loss = 0.80 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:36:54.941650: step 60800, loss = 0.70 (673.8 examples/sec; 0.190 sec/batch)
2017-03-25 23:36:56.546071: step 60810, loss = 0.71 (797.8 examples/sec; 0.160 sec/batch)
2017-03-25 23:36:58.293624: step 60820, loss = 0.72 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:37:00.026436: step 60830, loss = 0.66 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:01.761965: step 60840, loss = 0.96 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:37:03.500036: step 60850, loss = 0.85 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:37:05.222744: step 60860, loss = 0.64 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:37:06.949629: step 60870, loss = 0.70 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:08.705227: step 60880, loss = 0.76 (729.1 examples/sec; 0.176 sec/batch)
2017-03-25 23:37:10.437096: step 60890, loss = 0.65 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:12.348418: step 60900, loss = 0.64 (669.7 examples/sec; 0.191 sec/batch)
2017-03-25 23:37:13.958846: step 60910, loss = 0.77 (794.8 examples/sec; 0.161 sec/batch)
2017-03-25 23:37:15.688761: step 60920, loss = 0.63 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:17.433153: step 60930, loss = 0.63 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:37:19.159577: step 60940, loss = 0.71 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:20.886070: step 60950, loss = 0.81 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:22.611707: step 60960, loss = 0.60 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:24.338142: step 60970, loss = 0.71 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:26.068154: step 60980, loss = 0.82 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:27.804224: step 60990, loss = 0.69 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:37:29.674185: step 61000, loss = 0.65 (684.9 examples/sec; 0.187 sec/batch)
2017-03-25 23:37:31.321095: step 61010, loss = 0.69 (776.7 examples/sec; 0.165 sec/batch)
2017-03-25 23:37:33.065637: step 61020, loss = 0.76 (733.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:37:34.804272: step 61030, loss = 0.73 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:37:36.542090: step 61040, loss = 0.78 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:37:38.282447: step 61050, loss = 0.79 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:37:40.008030: step 61060, loss = 0.77 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:41.742269: step 61070, loss = 0.76 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:43.464752: step 61080, loss = 0.77 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:37:45.208316: step 61090, loss = 0.79 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:37:47.081445: step 61100, loss = 0.72 (683.3 examples/sec; 0.187 sec/batch)
2017-03-25 23:37:48.709235: step 61110, loss = 0.64 (786.3 examples/sec; 0.163 sec/batch)
2017-03-25 23:37:50.438361: step 61120, loss = 0.61 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:52.163892: step 61130, loss = 0.58 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:53.898099: step 61140, loss = 0.79 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:55.623893: step 61150, loss = 0.63 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:57.358268: step 61160, loss = 0.77 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:37:59.102340: step 61170, loss = 0.84 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:38:00.835621: step 61180, loss = 0.75 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:38:02.567233: step 61190, loss = 0.68 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:38:04.438853: step 61200, loss = 0.83 (683.9 examples/sec; 0.187 sec/batch)
2017-03-25 23:38:06.049842: step 61210, loss = 0.65 (794.5 examples/sec; 0.161 sec/batch)
2017-03-25 23:38:07.784503: step 61220, loss = 0.73 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:38:09.520573: step 61230, loss = 0.80 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:38:11.258137: step 61240, loss = 0.65 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:38:12.995007: step 61250, loss = 0.62 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:38:14.733108: step 61260, loss = 0.78 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:38:16.463425: step 61270, loss = 0.75 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:38:18.186652: step 61280, loss = 0.75 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:38:19.913661: step 61290, loss = 0.85 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:38:21.810181: step 61300, loss = 0.76 (674.9 examples/sec; 0.190 sec/batch)
2017-03-25 23:38:23.408126: step 61310, loss = 0.67 (801.0 examples/sec; 0.160 sec/batch)
2017-03-25 23:38:25.147895: step 61320, loss = 0.75 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:38:26.885706: step 61330, loss = 0.70 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:38:28.607744: step 61340, loss = 0.64 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:38:30.342180: step 61350, loss = 0.77 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:38:32.070247: step 61360, loss = 0.87 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:38:33.802833: step 61370, loss = 0.76 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:38:35.550631: step 61380, loss = 0.79 (732.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:38:37.292029: step 61390, loss = 0.72 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:38:39.192066: step 61400, loss = 0.79 (673.7 examples/sec; 0.190 sec/batch)
2017-03-25 23:38:40.790356: step 61410, loss = 0.68 (800.9 examples/sec; 0.160 sec/batch)
2017-03-25 23:38:42.522345: step 61420, loss = 0.76 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:38:44.257068: step 61430, loss = 0.67 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:38:45.990055: step 61440, loss = 0.62 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:38:47.718519: step 61450, loss = 0.85 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:38:49.455627: step 61460, loss = 0.70 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:38:51.188878: step 61470, loss = 0.75 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:38:52.928471: step 61480, loss = 0.73 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:38:54.663357: step 61490, loss = 0.85 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:38:56.558637: step 61500, loss = 0.80 (675.4 examples/sec; 0.190 sec/batch)
2017-03-25 23:38:58.180834: step 61510, loss = 0.70 (789.0 examples/sec; 0.162 sec/batch)
2017-03-25 23:38:59.923687: step 61520, loss = 0.75 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:39:01.658015: step 61530, loss = 0.78 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:39:03.381773: step 61540, loss = 0.73 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:39:05.125003: step 61550, loss = 0.51 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:39:06.850276: step 61560, loss = 0.70 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:39:08.582445: step 61570, loss = 0.74 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:39:10.330282: step 61580, loss = 0.82 (732.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:39:12.064641: step 61590, loss = 0.88 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:39:13.959770: step 61600, loss = 0.74 (675.4 examples/sec; 0.190 sec/batch)
2017-03-25 23:39:15.578775: step 61610, loss = 0.75 (790.6 examples/sec; 0.162 sec/batch)
2017-03-25 23:39:17.302549: step 61620, loss = 0.60 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:39:19.037260: step 61630, loss = 0.90 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:39:20.765162: step 61640, loss = 0.83 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:39:22.496400: step 61650, loss = 0.82 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:39:24.230505: step 61660, loss = 0.63 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:39:25.957031: step 61670, loss = 0.86 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:39:27.675510: step 61680, loss = 0.73 (744.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:39:29.416686: step 61690, loss = 0.89 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:39:31.297985: step 61700, loss = 0.62 (680.4 examples/sec; 0.188 sec/batch)
2017-03-25 23:39:32.911935: step 61710, loss = 0.85 (793.1 examples/sec; 0.161 sec/batch)
2017-03-25 23:39:34.653756: step 61720, loss = 0.77 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:39:36.389973: step 61730, loss = 0.66 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:39:38.110035: step 61740, loss = 0.73 (744.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:39:39.839631: step 61750, loss = 0.63 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:39:41.570090: step 61760, loss = 0.69 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:39:43.303296: step 61770, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:39:45.028233: step 61780, loss = 0.69 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:39:46.759807: step 61790, loss = 0.63 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:39:48.645108: step 61800, loss = 0.66 (678.9 examples/sec; 0.189 sec/batch)
2017-03-25 23:39:50.269665: step 61810, loss = 0.74 (787.9 examples/sec; 0.162 sec/batch)
2017-03-25 23:39:52.009176: step 61820, loss = 0.72 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:39:53.734158: step 61830, loss = 0.67 (742.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:39:55.477656: step 61840, loss = 0.90 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:39:57.214357: step 61850, loss = 0.77 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:39:58.949968: step 61860, loss = 0.87 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:40:00.674836: step 61870, loss = 0.74 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:40:02.414529: step 61880, loss = 0.83 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:40:04.137783: step 61890, loss = 0.79 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:40:06.019319: step 61900, loss = 0.74 (680.5 examples/sec; 0.188 sec/batch)
2017-03-25 23:40:07.640640: step 61910, loss = 0.61 (789.2 examples/sec; 0.162 sec/batch)
2017-03-25 23:40:09.380827: step 61920, loss = 0.67 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:40:11.110103: step 61930, loss = 0.65 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:40:12.835465: step 61940, loss = 0.77 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:40:14.587684: step 61950, loss = 0.70 (730.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:40:16.317821: step 61960, loss = 0.67 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:40:18.039731: step 61970, loss = 0.89 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:40:19.775088: step 61980, loss = 0.65 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:40:21.500035: step 61990, loss = 0.81 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:40:23.419944: step 62000, loss = 0.76 (666.7 examples/sec; 0.192 sec/batch)
2017-03-25 23:40:24.979720: step 62010, loss = 0.76 (820.6 examples/sec; 0.156 sec/batch)
2017-03-25 23:40:26.675652: step 62020, loss = 0.66 (754.8 examples/sec; 0.170 sec/batch)
2017-03-25 23:40:28.425673: step 62030, loss = 0.82 (731.4 examples/sec; 0.175 sec/batch)
2017-03-25 23:40:30.154078: step 62040, loss = 0.80 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:40:31.879555: step 62050, loss = 0.87 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:40:33.611870: step 62060, loss = 0.91 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:40:35.336510: step 62070, loss = 0.67 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:40:37.073454: step 62080, loss = 0.76 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:40:38.810801: step 62090, loss = 0.84 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:40:40.728540: step 62100, loss = 0.74 (667.5 examples/sec; 0.192 sec/batch)
2017-03-25 23:40:42.305270: step 62110, loss = 0.77 (811.8 examples/sec; 0.158 sec/batch)
2017-03-25 23:40:44.043479: step 62120, loss = 0.74 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:40:45.778042: step 62130, loss = 0.75 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:40:47.517262: step 62140, loss = 0.76 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:40:49.243312: step 62150, loss = 0.80 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:40:50.981431: step 62160, loss = 0.55 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:40:52.709098: step 62170, loss = 0.60 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:40:54.486186: step 62180, loss = 0.87 (720.4 examples/sec; 0.178 sec/batch)
2017-03-25 23:40:56.231566: step 62190, loss = 0.78 (733.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:40:58.116917: step 62200, loss = 0.70 (679.4 examples/sec; 0.188 sec/batch)
2017-03-25 23:40:59.720900: step 62210, loss = 0.84 (797.6 examples/sec; 0.160 sec/batch)
2017-03-25 23:41:01.460102: step 62220, loss = 0.56 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:41:03.200605: step 62230, loss = 0.65 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:41:04.933245: step 62240, loss = 0.74 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:41:06.683671: step 62250, loss = 0.79 (731.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:41:08.416987: step 62260, loss = 0.83 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:41:10.136774: step 62270, loss = 0.75 (744.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:41:11.873909: step 62280, loss = 0.64 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:41:13.603335: step 62290, loss = 0.56 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:41:15.509438: step 62300, loss = 0.77 (671.5 examples/sec; 0.191 sec/batch)
2017-03-25 23:41:17.107489: step 62310, loss = 0.65 (801.0 examples/sec; 0.160 sec/batch)
2017-03-25 23:41:18.847021: step 62320, loss = 0.75 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:41:20.572568: step 62330, loss = 0.69 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:41:22.312373: step 62340, loss = 0.65 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:41:24.044008: step 62350, loss = 0.66 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:41:25.783098: step 62360, loss = 0.71 (736.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:41:27.509370: step 62370, loss = 0.68 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:41:29.251985: step 62380, loss = 0.58 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:41:30.987597: step 62390, loss = 0.65 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:41:32.881452: step 62400, loss = 0.71 (675.9 examples/sec; 0.189 sec/batch)
2017-03-25 23:41:34.505833: step 62410, loss = 0.77 (788.0 examples/sec; 0.162 sec/batch)
2017-03-25 23:41:36.230973: step 62420, loss = 0.74 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:41:37.962436: step 62430, loss = 0.87 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:41:39.698772: step 62440, loss = 0.73 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:41:41.441758: step 62450, loss = 0.77 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:41:43.172647: step 62460, loss = 0.94 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:41:44.908272: step 62470, loss = 0.65 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:41:46.642963: step 62480, loss = 0.91 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:41:48.376431: step 62490, loss = 0.90 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:41:50.276070: step 62500, loss = 0.72 (673.8 examples/sec; 0.190 sec/batch)
2017-03-25 23:41:51.893621: step 62510, loss = 0.75 (791.3 examples/sec; 0.162 sec/batch)
2017-03-25 23:41:53.610984: step 62520, loss = 0.85 (745.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:41:55.349222: step 62530, loss = 0.69 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:41:57.076160: step 62540, loss = 0.74 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:41:58.812796: step 62550, loss = 0.83 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:42:00.529627: step 62560, loss = 0.68 (745.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:42:02.263006: step 62570, loss = 0.67 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:42:03.988088: step 62580, loss = 0.73 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:42:05.725552: step 62590, loss = 0.73 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:42:07.624138: step 62600, loss = 0.82 (674.2 examples/sec; 0.190 sec/batch)
2017-03-25 23:42:09.227030: step 62610, loss = 0.68 (798.6 examples/sec; 0.160 sec/batch)
2017-03-25 23:42:10.950897: step 62620, loss = 0.77 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:42:12.682227: step 62630, loss = 0.67 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:42:14.415415: step 62640, loss = 0.71 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:42:16.155402: step 62650, loss = 0.59 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:42:17.881619: step 62660, loss = 0.72 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:42:19.622086: step 62670, loss = 0.66 (735.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:42:21.359645: step 62680, loss = 0.68 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:42:23.077404: step 62690, loss = 0.75 (745.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:42:24.965017: step 62700, loss = 0.71 (678.6 examples/sec; 0.189 sec/batch)
2017-03-25 23:42:26.590814: step 62710, loss = 0.61 (786.6 examples/sec; 0.163 sec/batch)
2017-03-25 23:42:28.317983: step 62720, loss = 0.72 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:42:30.047041: step 62730, loss = 0.77 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:42:31.775216: step 62740, loss = 0.71 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:42:33.511371: step 62750, loss = 0.76 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:42:35.253830: step 62760, loss = 0.77 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:42:36.987570: step 62770, loss = 0.78 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:42:38.703376: step 62780, loss = 0.75 (746.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:42:40.435557: step 62790, loss = 0.84 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:42:42.311101: step 62800, loss = 0.72 (682.5 examples/sec; 0.188 sec/batch)
2017-03-25 23:42:43.947197: step 62810, loss = 0.71 (782.3 examples/sec; 0.164 sec/batch)
2017-03-25 23:42:45.694915: step 62820, loss = 0.72 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 23:42:47.432929: step 62830, loss = 0.69 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:42:49.163334: step 62840, loss = 0.63 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:42:50.899235: step 62850, loss = 0.60 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:42:52.639106: step 62860, loss = 0.72 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:42:54.370548: step 62870, loss = 0.69 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:42:56.109489: step 62880, loss = 0.77 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:42:57.835136: step 62890, loss = 0.76 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:42:59.716790: step 62900, loss = 0.78 (680.3 examples/sec; 0.188 sec/batch)
2017-03-25 23:43:01.350396: step 62910, loss = 0.73 (783.6 examples/sec; 0.163 sec/batch)
2017-03-25 23:43:03.086060: step 62920, loss = 0.72 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:43:04.811095: step 62930, loss = 0.75 (742.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:43:06.542675: step 62940, loss = 0.63 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:43:08.273857: step 62950, loss = 0.65 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:43:10.012872: step 62960, loss = 0.64 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:43:11.745884: step 62970, loss = 0.86 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:43:13.484361: step 62980, loss = 0.73 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:43:15.225384: step 62990, loss = 0.82 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:43:17.108499: step 63000, loss = 0.69 (679.7 examples/sec; 0.188 sec/batch)
2017-03-25 23:43:18.714675: step 63010, loss = 0.55 (796.9 examples/sec; 0.161 sec/batch)
2017-03-25 23:43:20.445373: step 63020, loss = 0.87 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:43:22.188722: step 63030, loss = 0.70 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:43:23.920508: step 63040, loss = 0.80 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:43:25.644915: step 63050, loss = 0.70 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:43:27.381664: step 63060, loss = 0.75 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:43:29.122104: step 63070, loss = 0.58 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:43:30.847769: step 63080, loss = 0.69 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:43:32.572010: step 63090, loss = 0.79 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:43:34.466661: step 63100, loss = 0.82 (675.7 examples/sec; 0.189 sec/batch)
2017-03-25 23:43:36.075768: step 63110, loss = 0.56 (795.3 examples/sec; 0.161 sec/batch)
2017-03-25 23:43:37.816956: step 63120, loss = 0.64 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:43:39.550297: step 63130, loss = 0.76 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:43:41.286692: step 63140, loss = 0.74 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:43:43.008887: step 63150, loss = 0.79 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:43:44.743519: step 63160, loss = 0.65 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:43:46.469686: step 63170, loss = 0.68 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:43:48.202689: step 63180, loss = 0.87 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:43:49.935843: step 63190, loss = 0.62 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:43:51.833562: step 63200, loss = 0.59 (674.5 examples/sec; 0.190 sec/batch)
2017-03-25 23:43:53.439787: step 63210, loss = 0.60 (796.9 examples/sec; 0.161 sec/batch)
2017-03-25 23:43:55.183672: step 63220, loss = 0.83 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:43:56.917004: step 63230, loss = 0.70 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:43:58.667171: step 63240, loss = 0.62 (731.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:44:00.384937: step 63250, loss = 0.58 (745.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:44:02.122131: step 63260, loss = 0.74 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:44:03.850847: step 63270, loss = 0.81 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:44:05.571371: step 63280, loss = 0.80 (744.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:44:07.314754: step 63290, loss = 0.77 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:44:09.187376: step 63300, loss = 0.75 (683.5 examples/sec; 0.187 sec/batch)
2017-03-25 23:44:10.819856: step 63310, loss = 0.75 (784.1 examples/sec; 0.163 sec/batch)
2017-03-25 23:44:12.549435: step 63320, loss = 0.81 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:44:14.272651: step 63330, loss = 0.71 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:44:15.994371: step 63340, loss = 0.74 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:44:17.725169: step 63350, loss = 0.64 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:44:19.448493: step 63360, loss = 0.69 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:44:21.179809: step 63370, loss = 0.71 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:44:22.911972: step 63380, loss = 0.64 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:44:24.651590: step 63390, loss = 0.77 (735.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:44:26.559423: step 63400, loss = 0.84 (671.0 examples/sec; 0.191 sec/batch)
2017-03-25 23:44:28.157290: step 63410, loss = 0.71 (801.0 examples/sec; 0.160 sec/batch)
2017-03-25 23:44:29.902074: step 63420, loss = 0.62 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:44:31.639360: step 63430, loss = 0.68 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:44:33.371193: step 63440, loss = 0.72 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:44:35.090385: step 63450, loss = 0.62 (744.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:44:36.827032: step 63460, loss = 0.70 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:44:38.568470: step 63470, loss = 0.95 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:44:40.301309: step 63480, loss = 0.66 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:44:42.022711: step 63490, loss = 0.76 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:44:43.906101: step 63500, loss = 0.74 (679.9 examples/sec; 0.188 sec/batch)
2017-03-25 23:44:45.538852: step 63510, loss = 0.72 (783.6 examples/sec; 0.163 sec/batch)
2017-03-25 23:44:47.286149: step 63520, loss = 0.64 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 23:44:49.016938: step 63530, loss = 0.82 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:44:50.740701: step 63540, loss = 0.68 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:44:52.464690: step 63550, loss = 0.63 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:44:54.206155: step 63560, loss = 0.90 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:44:55.928738: step 63570, loss = 0.85 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:44:57.671824: step 63580, loss = 0.83 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:44:59.408345: step 63590, loss = 0.80 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:45:01.302633: step 63600, loss = 0.72 (676.2 examples/sec; 0.189 sec/batch)
2017-03-25 23:45:02.943629: step 63610, loss = 0.67 (779.4 examples/sec; 0.164 sec/batch)
2017-03-25 23:45:04.683185: step 63620, loss = 0.72 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:45:06.430833: step 63630, loss = 0.67 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 23:45:08.167784: step 63640, loss = 0.81 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:45:09.908717: step 63650, loss = 0.69 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:45:11.640062: step 63660, loss = 0.77 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:45:13.378691: step 63670, loss = 0.58 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:45:15.119971: step 63680, loss = 0.82 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:45:16.858706: step 63690, loss = 0.76 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:45:18.757295: step 63700, loss = 0.61 (674.2 examples/sec; 0.190 sec/batch)
2017-03-25 23:45:20.375351: step 63710, loss = 0.93 (791.1 examples/sec; 0.162 sec/batch)
2017-03-25 23:45:22.119573: step 63720, loss = 0.62 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:45:23.865612: step 63730, loss = 0.66 (733.1 examples/sec; 0.175 sec/batch)
2017-03-25 23:45:25.591639: step 63740, loss = 0.77 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:45:27.328429: step 63750, loss = 0.73 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:45:29.056966: step 63760, loss = 0.62 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:45:30.805816: step 63770, loss = 0.71 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 23:45:32.534756: step 63780, loss = 0.61 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:45:34.270319: step 63790, loss = 0.84 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:45:36.159673: step 63800, loss = 0.68 (677.7 examples/sec; 0.189 sec/batch)
2017-03-25 23:45:37.763641: step 63810, loss = 0.76 (797.7 examples/sec; 0.160 sec/batch)
2017-03-25 23:45:39.491276: step 63820, loss = 0.61 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:45:41.232992: step 63830, loss = 0.83 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:45:42.967282: step 63840, loss = 0.72 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:45:44.699010: step 63850, loss = 0.80 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:45:46.433312: step 63860, loss = 0.70 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:45:48.162481: step 63870, loss = 0.82 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:45:49.900295: step 63880, loss = 0.70 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:45:51.636018: step 63890, loss = 0.74 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:45:53.534836: step 63900, loss = 0.93 (674.1 examples/sec; 0.190 sec/batch)
2017-03-25 23:45:55.145377: step 63910, loss = 0.68 (794.9 examples/sec; 0.161 sec/batch)
2017-03-25 23:45:56.880237: step 63920, loss = 0.61 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:45:58.618829: step 63930, loss = 0.68 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:46:00.343596: step 63940, loss = 0.76 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:46:02.082481: step 63950, loss = 0.71 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:46:03.828674: step 63960, loss = 0.66 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 23:46:05.557983: step 63970, loss = 0.75 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:07.281731: step 63980, loss = 0.71 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:46:09.000291: step 63990, loss = 0.80 (744.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:46:10.910470: step 64000, loss = 0.69 (670.2 examples/sec; 0.191 sec/batch)
2017-03-25 23:46:12.517291: step 64010, loss = 0.76 (796.5 examples/sec; 0.161 sec/batch)
2017-03-25 23:46:14.240720: step 64020, loss = 0.78 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:46:15.967702: step 64030, loss = 0.86 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:17.698265: step 64040, loss = 0.54 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:19.422030: step 64050, loss = 0.72 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:46:21.150764: step 64060, loss = 0.80 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:22.901567: step 64070, loss = 0.58 (731.1 examples/sec; 0.175 sec/batch)
2017-03-25 23:46:24.635505: step 64080, loss = 0.71 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:26.372750: step 64090, loss = 0.69 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:46:28.333841: step 64100, loss = 0.75 (652.7 examples/sec; 0.196 sec/batch)
2017-03-25 23:46:29.897597: step 64110, loss = 0.68 (818.5 examples/sec; 0.156 sec/batch)
2017-03-25 23:46:31.622881: step 64120, loss = 0.89 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:33.356804: step 64130, loss = 0.54 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:35.085936: step 64140, loss = 0.67 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:36.828410: step 64150, loss = 0.74 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:46:38.556233: step 64160, loss = 0.71 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:40.284821: step 64170, loss = 0.66 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:42.017519: step 64180, loss = 0.76 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:43.745477: step 64190, loss = 1.07 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:45.622389: step 64200, loss = 0.87 (682.5 examples/sec; 0.188 sec/batch)
2017-03-25 23:46:47.257058: step 64210, loss = 0.78 (782.3 examples/sec; 0.164 sec/batch)
2017-03-25 23:46:48.991160: step 64220, loss = 0.62 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:50.714472: step 64230, loss = 0.71 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:46:52.441111: step 64240, loss = 0.71 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:54.181304: step 64250, loss = 0.64 (735.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:46:55.915834: step 64260, loss = 0.73 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:46:57.651867: step 64270, loss = 0.73 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:46:59.378566: step 64280, loss = 0.56 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:47:01.109524: step 64290, loss = 0.80 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:47:03.020016: step 64300, loss = 0.63 (670.2 examples/sec; 0.191 sec/batch)
2017-03-25 23:47:04.628360: step 64310, loss = 0.63 (795.6 examples/sec; 0.161 sec/batch)
2017-03-25 23:47:06.354950: step 64320, loss = 0.66 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:47:08.091091: step 64330, loss = 0.81 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:47:09.823435: step 64340, loss = 0.81 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:47:11.544201: step 64350, loss = 0.80 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:47:13.273274: step 64360, loss = 0.74 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:47:15.012675: step 64370, loss = 0.86 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:47:16.738874: step 64380, loss = 0.64 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:47:18.473932: step 64390, loss = 0.82 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:47:20.383549: step 64400, loss = 0.73 (670.5 examples/sec; 0.191 sec/batch)
2017-03-25 23:47:21.978796: step 64410, loss = 0.79 (802.1 examples/sec; 0.160 sec/batch)
2017-03-25 23:47:23.708755: step 64420, loss = 0.72 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:47:25.440015: step 64430, loss = 0.87 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:47:27.176458: step 64440, loss = 0.66 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:47:28.919686: step 64450, loss = 0.71 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:47:30.661864: step 64460, loss = 0.68 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:47:32.386531: step 64470, loss = 0.67 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:47:34.119634: step 64480, loss = 0.83 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:47:35.837534: step 64490, loss = 0.80 (745.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:47:37.719587: step 64500, loss = 0.62 (680.1 examples/sec; 0.188 sec/batch)
2017-03-25 23:47:39.313264: step 64510, loss = 0.64 (803.2 examples/sec; 0.159 sec/batch)
2017-03-25 23:47:41.049460: step 64520, loss = 0.78 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:47:42.796565: step 64530, loss = 0.71 (732.6 examples/sec; 0.175 sec/batch)
2017-03-25 23:47:44.518147: step 64540, loss = 0.83 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:47:46.262075: step 64550, loss = 0.76 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:47:47.999195: step 64560, loss = 0.85 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:47:49.725940: step 64570, loss = 0.67 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:47:51.451391: step 64580, loss = 0.67 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:47:53.179163: step 64590, loss = 0.67 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:47:55.158505: step 64600, loss = 0.58 (646.7 examples/sec; 0.198 sec/batch)
2017-03-25 23:47:56.650020: step 64610, loss = 0.59 (858.2 examples/sec; 0.149 sec/batch)
2017-03-25 23:47:58.362723: step 64620, loss = 0.85 (747.4 examples/sec; 0.171 sec/batch)
2017-03-25 23:48:00.105212: step 64630, loss = 0.60 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:48:01.854591: step 64640, loss = 0.67 (731.8 examples/sec; 0.175 sec/batch)
2017-03-25 23:48:03.578065: step 64650, loss = 0.75 (742.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:48:05.309044: step 64660, loss = 0.74 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:48:07.055398: step 64670, loss = 0.61 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 23:48:08.786465: step 64680, loss = 0.79 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:48:10.521506: step 64690, loss = 0.72 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:48:12.478256: step 64700, loss = 0.70 (654.2 examples/sec; 0.196 sec/batch)
2017-03-25 23:48:14.042293: step 64710, loss = 0.76 (818.4 examples/sec; 0.156 sec/batch)
2017-03-25 23:48:15.779922: step 64720, loss = 0.71 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:48:17.512544: step 64730, loss = 0.64 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:48:19.246782: step 64740, loss = 0.67 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:48:20.994860: step 64750, loss = 0.74 (732.2 examples/sec; 0.175 sec/batch)
2017-03-25 23:48:22.738289: step 64760, loss = 0.83 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:48:24.461282: step 64770, loss = 0.75 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:48:26.192121: step 64780, loss = 0.65 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:48:27.929188: step 64790, loss = 0.66 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:48:29.855676: step 64800, loss = 0.80 (664.4 examples/sec; 0.193 sec/batch)
2017-03-25 23:48:31.443988: step 64810, loss = 0.68 (805.9 examples/sec; 0.159 sec/batch)
2017-03-25 23:48:33.184712: step 64820, loss = 0.68 (735.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:48:34.909742: step 64830, loss = 0.71 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:48:36.645091: step 64840, loss = 0.80 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:48:38.377532: step 64850, loss = 0.71 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:48:40.111981: step 64860, loss = 0.79 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:48:41.846867: step 64870, loss = 0.76 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:48:43.578182: step 64880, loss = 0.72 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:48:45.309234: step 64890, loss = 0.62 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:48:47.188769: step 64900, loss = 0.89 (681.0 examples/sec; 0.188 sec/batch)
2017-03-25 23:48:48.827186: step 64910, loss = 0.63 (781.2 examples/sec; 0.164 sec/batch)
2017-03-25 23:48:50.555970: step 64920, loss = 0.64 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:48:52.290380: step 64930, loss = 0.79 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:48:54.031931: step 64940, loss = 0.80 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:48:55.774972: step 64950, loss = 0.77 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:48:57.492374: step 64960, loss = 0.75 (745.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:48:59.234230: step 64970, loss = 0.61 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:49:00.982997: step 64980, loss = 0.63 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 23:49:02.722919: step 64990, loss = 0.83 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:49:04.611442: step 65000, loss = 0.70 (677.8 examples/sec; 0.189 sec/batch)
2017-03-25 23:49:06.250231: step 65010, loss = 0.57 (781.1 examples/sec; 0.164 sec/batch)
2017-03-25 23:49:07.992690: step 65020, loss = 0.70 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:49:09.741244: step 65030, loss = 0.77 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 23:49:11.475489: step 65040, loss = 0.69 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:49:13.191985: step 65050, loss = 0.75 (745.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:49:14.925230: step 65060, loss = 0.71 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:49:16.658198: step 65070, loss = 0.68 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:49:18.401228: step 65080, loss = 0.81 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:49:20.122003: step 65090, loss = 0.83 (743.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:49:22.029879: step 65100, loss = 0.63 (670.9 examples/sec; 0.191 sec/batch)
2017-03-25 23:49:23.644201: step 65110, loss = 0.65 (792.9 examples/sec; 0.161 sec/batch)
2017-03-25 23:49:25.377201: step 65120, loss = 0.77 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:49:27.110642: step 65130, loss = 0.58 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:49:28.855542: step 65140, loss = 0.72 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:49:30.583617: step 65150, loss = 0.54 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:49:32.321211: step 65160, loss = 0.79 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:49:34.049282: step 65170, loss = 0.82 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:49:35.796614: step 65180, loss = 0.66 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:49:37.524979: step 65190, loss = 0.64 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:49:39.416199: step 65200, loss = 0.76 (676.9 examples/sec; 0.189 sec/batch)
2017-03-25 23:49:41.030709: step 65210, loss = 0.73 (792.6 examples/sec; 0.161 sec/batch)
2017-03-25 23:49:42.755171: step 65220, loss = 0.73 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:49:44.497394: step 65230, loss = 0.76 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:49:46.229867: step 65240, loss = 0.86 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:49:47.961978: step 65250, loss = 0.72 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:49:49.702069: step 65260, loss = 0.76 (735.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:49:51.424400: step 65270, loss = 0.84 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:49:53.162351: step 65280, loss = 0.60 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:49:54.889008: step 65290, loss = 0.61 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:49:56.790704: step 65300, loss = 0.84 (672.9 examples/sec; 0.190 sec/batch)
2017-03-25 23:49:58.386219: step 65310, loss = 0.69 (802.2 examples/sec; 0.160 sec/batch)
2017-03-25 23:50:00.117719: step 65320, loss = 0.61 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:50:01.853689: step 65330, loss = 0.76 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:50:03.601403: step 65340, loss = 0.81 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 23:50:05.324016: step 65350, loss = 0.58 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:50:07.066421: step 65360, loss = 0.65 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:50:08.802006: step 65370, loss = 0.79 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:50:10.531512: step 65380, loss = 0.63 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:50:12.282294: step 65390, loss = 0.82 (731.1 examples/sec; 0.175 sec/batch)
2017-03-25 23:50:14.170595: step 65400, loss = 0.64 (677.9 examples/sec; 0.189 sec/batch)
2017-03-25 23:50:15.771855: step 65410, loss = 0.74 (799.6 examples/sec; 0.160 sec/batch)
2017-03-25 23:50:17.517885: step 65420, loss = 0.83 (733.0 examples/sec; 0.175 sec/batch)
2017-03-25 23:50:19.265271: step 65430, loss = 0.73 (732.5 examples/sec; 0.175 sec/batch)
2017-03-25 23:50:21.001405: step 65440, loss = 0.77 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:50:22.727778: step 65450, loss = 0.78 (741.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:50:24.443984: step 65460, loss = 0.82 (745.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:50:26.170414: step 65470, loss = 0.64 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:50:27.912191: step 65480, loss = 0.64 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:50:29.648617: step 65490, loss = 0.74 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:50:31.528523: step 65500, loss = 0.70 (681.1 examples/sec; 0.188 sec/batch)
2017-03-25 23:50:33.151640: step 65510, loss = 0.71 (788.4 examples/sec; 0.162 sec/batch)
2017-03-25 23:50:34.879030: step 65520, loss = 0.72 (741.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:50:36.614423: step 65530, loss = 0.65 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:50:38.362297: step 65540, loss = 0.71 (732.3 examples/sec; 0.175 sec/batch)
2017-03-25 23:50:40.091275: step 65550, loss = 0.75 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:50:41.830115: step 65560, loss = 0.88 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:50:43.554879: step 65570, loss = 0.70 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:50:45.295623: step 65580, loss = 0.79 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:50:47.027468: step 65590, loss = 0.73 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:50:48.926279: step 65600, loss = 0.64 (674.1 examples/sec; 0.190 sec/batch)
2017-03-25 23:50:50.526573: step 65610, loss = 0.62 (799.9 examples/sec; 0.160 sec/batch)
2017-03-25 23:50:52.262273: step 65620, loss = 0.80 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:50:54.074078: step 65630, loss = 0.70 (706.3 examples/sec; 0.181 sec/batch)
2017-03-25 23:50:55.796044: step 65640, loss = 0.74 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:50:57.518196: step 65650, loss = 0.70 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:50:59.246214: step 65660, loss = 0.70 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:51:00.989437: step 65670, loss = 0.72 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:51:02.722017: step 65680, loss = 0.65 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:51:04.454603: step 65690, loss = 0.70 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:51:06.349203: step 65700, loss = 0.68 (675.6 examples/sec; 0.189 sec/batch)
2017-03-25 23:51:07.962992: step 65710, loss = 0.77 (793.2 examples/sec; 0.161 sec/batch)
2017-03-25 23:51:09.697667: step 65720, loss = 0.69 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:51:11.433767: step 65730, loss = 0.75 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:51:13.176041: step 65740, loss = 0.59 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:51:14.898936: step 65750, loss = 0.61 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:51:16.617688: step 65760, loss = 0.69 (745.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:51:18.349411: step 65770, loss = 0.69 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:51:20.074780: step 65780, loss = 0.54 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:51:21.810758: step 65790, loss = 0.62 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:51:23.694682: step 65800, loss = 0.64 (679.4 examples/sec; 0.188 sec/batch)
2017-03-25 23:51:25.316910: step 65810, loss = 0.63 (789.0 examples/sec; 0.162 sec/batch)
2017-03-25 23:51:27.052780: step 65820, loss = 0.66 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:51:28.780623: step 65830, loss = 0.63 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:51:30.516848: step 65840, loss = 0.72 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:51:32.266119: step 65850, loss = 0.67 (731.6 examples/sec; 0.175 sec/batch)
2017-03-25 23:51:33.996922: step 65860, loss = 0.69 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:51:35.737945: step 65870, loss = 0.73 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:51:37.467057: step 65880, loss = 0.72 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:51:39.201523: step 65890, loss = 0.84 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:51:41.105765: step 65900, loss = 0.71 (672.5 examples/sec; 0.190 sec/batch)
2017-03-25 23:51:42.713464: step 65910, loss = 0.79 (795.7 examples/sec; 0.161 sec/batch)
2017-03-25 23:51:44.441730: step 65920, loss = 0.67 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:51:46.173991: step 65930, loss = 0.82 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:51:47.910180: step 65940, loss = 0.67 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:51:49.644035: step 65950, loss = 0.70 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:51:51.380783: step 65960, loss = 0.64 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:51:53.111631: step 65970, loss = 0.77 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:51:54.848732: step 65980, loss = 0.83 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:51:56.593469: step 65990, loss = 0.68 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:51:58.541964: step 66000, loss = 0.65 (656.9 examples/sec; 0.195 sec/batch)
2017-03-25 23:52:00.076556: step 66010, loss = 0.70 (834.5 examples/sec; 0.153 sec/batch)
2017-03-25 23:52:01.782142: step 66020, loss = 0.71 (750.2 examples/sec; 0.171 sec/batch)
2017-03-25 23:52:03.525639: step 66030, loss = 0.67 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:52:05.256567: step 66040, loss = 0.63 (739.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:52:06.978338: step 66050, loss = 0.73 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:52:08.696532: step 66060, loss = 0.65 (745.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:52:10.428257: step 66070, loss = 0.65 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:52:12.165465: step 66080, loss = 0.74 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:52:13.887501: step 66090, loss = 0.71 (743.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:52:15.784379: step 66100, loss = 0.78 (674.8 examples/sec; 0.190 sec/batch)
2017-03-25 23:52:17.391529: step 66110, loss = 0.69 (796.4 examples/sec; 0.161 sec/batch)
2017-03-25 23:52:19.133145: step 66120, loss = 0.64 (734.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:52:20.859383: step 66130, loss = 0.81 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:52:22.592547: step 66140, loss = 0.68 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:52:24.315753: step 66150, loss = 0.73 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:52:26.048621: step 66160, loss = 0.77 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:52:27.779933: step 66170, loss = 0.95 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:52:29.512213: step 66180, loss = 0.77 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:52:31.253278: step 66190, loss = 0.69 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:52:33.202996: step 66200, loss = 0.77 (657.0 examples/sec; 0.195 sec/batch)
2017-03-25 23:52:34.747288: step 66210, loss = 0.81 (828.1 examples/sec; 0.155 sec/batch)
2017-03-25 23:52:36.472269: step 66220, loss = 0.81 (742.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:52:38.190487: step 66230, loss = 0.65 (745.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:52:39.910675: step 66240, loss = 0.79 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:52:41.645478: step 66250, loss = 0.63 (737.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:52:43.356957: step 66260, loss = 0.73 (747.9 examples/sec; 0.171 sec/batch)
2017-03-25 23:52:45.091607: step 66270, loss = 0.81 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:52:46.810653: step 66280, loss = 0.78 (744.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:52:48.539069: step 66290, loss = 0.71 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:52:50.505562: step 66300, loss = 0.68 (650.9 examples/sec; 0.197 sec/batch)
2017-03-25 23:52:52.052921: step 66310, loss = 0.71 (827.2 examples/sec; 0.155 sec/batch)
2017-03-25 23:52:53.779994: step 66320, loss = 0.71 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:52:55.503007: step 66330, loss = 0.75 (742.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:52:57.239065: step 66340, loss = 0.59 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:52:58.973525: step 66350, loss = 0.69 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:53:00.715802: step 66360, loss = 0.72 (734.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:53:02.462779: step 66370, loss = 0.60 (732.7 examples/sec; 0.175 sec/batch)
2017-03-25 23:53:04.201226: step 66380, loss = 0.72 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:53:05.919050: step 66390, loss = 0.59 (745.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:53:07.816952: step 66400, loss = 0.74 (674.7 examples/sec; 0.190 sec/batch)
2017-03-25 23:53:09.429126: step 66410, loss = 0.83 (793.6 examples/sec; 0.161 sec/batch)
2017-03-25 23:53:11.178469: step 66420, loss = 0.71 (731.7 examples/sec; 0.175 sec/batch)
2017-03-25 23:53:12.899237: step 66430, loss = 0.68 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:53:14.625653: step 66440, loss = 0.68 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:53:16.362160: step 66450, loss = 0.52 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:53:18.098116: step 66460, loss = 0.72 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:53:19.829272: step 66470, loss = 0.83 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:53:21.545744: step 66480, loss = 0.67 (745.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:53:23.276278: step 66490, loss = 0.65 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:53:25.155136: step 66500, loss = 0.85 (681.7 examples/sec; 0.188 sec/batch)
2017-03-25 23:53:26.783717: step 66510, loss = 0.81 (785.4 examples/sec; 0.163 sec/batch)
2017-03-25 23:53:28.515926: step 66520, loss = 0.68 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:53:30.260389: step 66530, loss = 0.84 (733.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:53:31.987598: step 66540, loss = 0.89 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:53:33.741619: step 66550, loss = 0.84 (729.7 examples/sec; 0.175 sec/batch)
2017-03-25 23:53:35.469936: step 66560, loss = 0.67 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:53:37.216678: step 66570, loss = 0.72 (732.8 examples/sec; 0.175 sec/batch)
2017-03-25 23:53:38.950913: step 66580, loss = 0.61 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:53:40.692410: step 66590, loss = 0.65 (735.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:53:42.603414: step 66600, loss = 0.71 (669.8 examples/sec; 0.191 sec/batch)
2017-03-25 23:53:44.209849: step 66610, loss = 0.87 (796.8 examples/sec; 0.161 sec/batch)
2017-03-25 23:53:45.941605: step 66620, loss = 0.63 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:53:47.674004: step 66630, loss = 0.80 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:53:49.405394: step 66640, loss = 0.66 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:53:51.143611: step 66650, loss = 0.62 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:53:52.870171: step 66660, loss = 0.70 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:53:54.599824: step 66670, loss = 0.64 (740.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:53:56.325396: step 66680, loss = 0.71 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:53:58.050166: step 66690, loss = 0.89 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:54:00.007695: step 66700, loss = 0.69 (654.1 examples/sec; 0.196 sec/batch)
2017-03-25 23:54:01.527950: step 66710, loss = 0.65 (841.5 examples/sec; 0.152 sec/batch)
2017-03-25 23:54:03.248419: step 66720, loss = 0.84 (744.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:54:04.979500: step 66730, loss = 0.68 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:06.709894: step 66740, loss = 0.82 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:08.440481: step 66750, loss = 0.74 (739.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:10.169408: step 66760, loss = 0.65 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:11.905254: step 66770, loss = 0.67 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:54:13.641248: step 66780, loss = 0.69 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:54:15.378352: step 66790, loss = 0.69 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:54:17.263615: step 66800, loss = 0.55 (679.0 examples/sec; 0.189 sec/batch)
2017-03-25 23:54:18.880312: step 66810, loss = 0.81 (791.7 examples/sec; 0.162 sec/batch)
2017-03-25 23:54:20.605196: step 66820, loss = 0.68 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:54:22.332334: step 66830, loss = 0.72 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:24.065810: step 66840, loss = 0.69 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:25.801324: step 66850, loss = 0.74 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:54:27.535279: step 66860, loss = 0.66 (738.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:29.272826: step 66870, loss = 0.63 (736.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:54:31.002699: step 66880, loss = 0.78 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:32.735091: step 66890, loss = 0.71 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:34.628371: step 66900, loss = 0.60 (676.3 examples/sec; 0.189 sec/batch)
2017-03-25 23:54:36.230606: step 66910, loss = 0.62 (798.6 examples/sec; 0.160 sec/batch)
2017-03-25 23:54:37.959614: step 66920, loss = 0.82 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:39.697514: step 66930, loss = 0.74 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:54:41.429676: step 66940, loss = 0.72 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:43.167004: step 66950, loss = 0.97 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:54:44.893662: step 66960, loss = 0.66 (741.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:46.616261: step 66970, loss = 0.72 (743.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:54:48.346707: step 66980, loss = 0.64 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:50.078715: step 66990, loss = 0.74 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:51.953594: step 67000, loss = 0.72 (682.7 examples/sec; 0.187 sec/batch)
2017-03-25 23:54:53.570615: step 67010, loss = 0.72 (791.6 examples/sec; 0.162 sec/batch)
2017-03-25 23:54:55.298260: step 67020, loss = 0.71 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:57.031621: step 67030, loss = 0.88 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:54:58.769955: step 67040, loss = 0.58 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:55:00.497375: step 67050, loss = 0.77 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:55:02.221623: step 67060, loss = 0.69 (742.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:55:03.959367: step 67070, loss = 0.62 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:55:05.691211: step 67080, loss = 0.70 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:55:07.421225: step 67090, loss = 0.81 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:55:09.384813: step 67100, loss = 0.66 (652.2 examples/sec; 0.196 sec/batch)
2017-03-25 23:55:10.930453: step 67110, loss = 0.63 (827.6 examples/sec; 0.155 sec/batch)
2017-03-25 23:55:12.652847: step 67120, loss = 0.69 (743.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:55:14.379254: step 67130, loss = 0.70 (741.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:55:16.116014: step 67140, loss = 0.68 (737.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:55:17.844961: step 67150, loss = 0.73 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:55:19.587441: step 67160, loss = 0.61 (734.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:55:21.320614: step 67170, loss = 0.71 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:55:23.046772: step 67180, loss = 0.85 (741.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:55:24.784616: step 67190, loss = 0.78 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:55:26.653628: step 67200, loss = 0.63 (684.9 examples/sec; 0.187 sec/batch)
2017-03-25 23:55:28.282952: step 67210, loss = 0.67 (785.6 examples/sec; 0.163 sec/batch)
2017-03-25 23:55:30.016676: step 67220, loss = 0.62 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:55:31.732137: step 67230, loss = 0.73 (746.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:55:33.462554: step 67240, loss = 0.68 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:55:35.187860: step 67250, loss = 0.72 (741.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:55:36.924991: step 67260, loss = 0.68 (736.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:55:38.668193: step 67270, loss = 0.88 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:55:40.397940: step 67280, loss = 0.85 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:55:42.130851: step 67290, loss = 0.74 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:55:44.018418: step 67300, loss = 0.63 (678.1 examples/sec; 0.189 sec/batch)
2017-03-25 23:55:45.645163: step 67310, loss = 0.66 (786.9 examples/sec; 0.163 sec/batch)
2017-03-25 23:55:47.387110: step 67320, loss = 0.79 (734.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:55:49.110417: step 67330, loss = 0.68 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:55:50.851620: step 67340, loss = 0.64 (735.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:55:52.573303: step 67350, loss = 0.89 (743.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:55:54.320920: step 67360, loss = 0.71 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 23:55:56.049239: step 67370, loss = 0.70 (740.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:55:57.792380: step 67380, loss = 0.71 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:55:59.525820: step 67390, loss = 0.78 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:56:01.422829: step 67400, loss = 0.74 (675.1 examples/sec; 0.190 sec/batch)
2017-03-25 23:56:03.036803: step 67410, loss = 0.63 (792.6 examples/sec; 0.161 sec/batch)
2017-03-25 23:56:04.793795: step 67420, loss = 0.72 (728.5 examples/sec; 0.176 sec/batch)
2017-03-25 23:56:06.522366: step 67430, loss = 0.67 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:56:08.242197: step 67440, loss = 0.71 (744.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:56:09.976396: step 67450, loss = 0.76 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:56:11.712355: step 67460, loss = 0.74 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:56:13.438102: step 67470, loss = 0.70 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:56:15.165063: step 67480, loss = 0.81 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:56:16.897769: step 67490, loss = 0.82 (738.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:56:18.821822: step 67500, loss = 0.82 (665.3 examples/sec; 0.192 sec/batch)
2017-03-25 23:56:20.421460: step 67510, loss = 0.73 (800.2 examples/sec; 0.160 sec/batch)
2017-03-25 23:56:22.160858: step 67520, loss = 0.74 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:56:23.884089: step 67530, loss = 0.78 (742.8 examples/sec; 0.172 sec/batch)
2017-03-25 23:56:25.617234: step 67540, loss = 0.75 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:56:27.337962: step 67550, loss = 0.64 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:56:29.081639: step 67560, loss = 0.70 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:56:30.804368: step 67570, loss = 0.74 (743.0 examples/sec; 0.172 sec/batch)
2017-03-25 23:56:32.529494: step 67580, loss = 0.58 (742.1 examples/sec; 0.172 sec/batch)
2017-03-25 23:56:34.262375: step 67590, loss = 0.75 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:56:36.161914: step 67600, loss = 0.71 (674.0 examples/sec; 0.190 sec/batch)
2017-03-25 23:56:37.769466: step 67610, loss = 0.58 (796.0 examples/sec; 0.161 sec/batch)
2017-03-25 23:56:39.500542: step 67620, loss = 0.69 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:56:41.238344: step 67630, loss = 0.78 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:56:42.973592: step 67640, loss = 0.71 (737.7 examples/sec; 0.174 sec/batch)
2017-03-25 23:56:44.708125: step 67650, loss = 0.71 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:56:46.450660: step 67660, loss = 0.64 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:56:48.182388: step 67670, loss = 0.74 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:56:49.918082: step 67680, loss = 0.71 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:56:51.647630: step 67690, loss = 0.71 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:56:53.549061: step 67700, loss = 0.74 (673.4 examples/sec; 0.190 sec/batch)
2017-03-25 23:56:55.138134: step 67710, loss = 0.63 (805.2 examples/sec; 0.159 sec/batch)
2017-03-25 23:56:56.877524: step 67720, loss = 0.73 (735.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:56:58.596509: step 67730, loss = 0.76 (744.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:57:00.345422: step 67740, loss = 0.76 (731.9 examples/sec; 0.175 sec/batch)
2017-03-25 23:57:02.076660: step 67750, loss = 0.69 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:03.806058: step 67760, loss = 0.67 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:05.549479: step 67770, loss = 0.68 (734.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:57:07.297112: step 67780, loss = 0.65 (732.4 examples/sec; 0.175 sec/batch)
2017-03-25 23:57:09.026502: step 67790, loss = 0.72 (740.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:10.912115: step 67800, loss = 0.52 (679.1 examples/sec; 0.188 sec/batch)
2017-03-25 23:57:12.523870: step 67810, loss = 0.90 (793.8 examples/sec; 0.161 sec/batch)
2017-03-25 23:57:14.264684: step 67820, loss = 0.76 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:57:15.992287: step 67830, loss = 0.71 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:17.723844: step 67840, loss = 0.72 (739.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:19.453053: step 67850, loss = 0.70 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:21.187456: step 67860, loss = 0.73 (738.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:22.928221: step 67870, loss = 0.53 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:57:24.653634: step 67880, loss = 0.90 (741.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:26.374353: step 67890, loss = 0.78 (743.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:57:28.280477: step 67900, loss = 0.72 (671.5 examples/sec; 0.191 sec/batch)
2017-03-25 23:57:29.899076: step 67910, loss = 0.78 (790.8 examples/sec; 0.162 sec/batch)
2017-03-25 23:57:31.634330: step 67920, loss = 0.63 (737.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:57:33.362888: step 67930, loss = 0.67 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:35.089866: step 67940, loss = 0.57 (741.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:36.824347: step 67950, loss = 0.56 (738.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:38.550104: step 67960, loss = 0.63 (741.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:40.277380: step 67970, loss = 0.69 (741.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:42.009352: step 67980, loss = 0.86 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:43.738466: step 67990, loss = 0.69 (740.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:45.643437: step 68000, loss = 0.68 (672.4 examples/sec; 0.190 sec/batch)
2017-03-25 23:57:47.233247: step 68010, loss = 0.81 (804.5 examples/sec; 0.159 sec/batch)
2017-03-25 23:57:48.964624: step 68020, loss = 0.82 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:50.692586: step 68030, loss = 0.78 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:52.430354: step 68040, loss = 0.79 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:57:54.166846: step 68050, loss = 0.68 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:57:55.903435: step 68060, loss = 0.76 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:57:57.633551: step 68070, loss = 0.69 (739.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:57:59.376461: step 68080, loss = 0.74 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:58:01.098184: step 68090, loss = 0.63 (743.4 examples/sec; 0.172 sec/batch)
2017-03-25 23:58:03.016788: step 68100, loss = 0.70 (667.2 examples/sec; 0.192 sec/batch)
2017-03-25 23:58:04.606920: step 68110, loss = 0.70 (805.0 examples/sec; 0.159 sec/batch)
2017-03-25 23:58:06.330804: step 68120, loss = 0.70 (742.5 examples/sec; 0.172 sec/batch)
2017-03-25 23:58:08.067406: step 68130, loss = 0.93 (737.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:58:09.799658: step 68140, loss = 0.61 (738.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:58:11.542863: step 68150, loss = 0.71 (734.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:58:13.276392: step 68160, loss = 0.68 (738.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:58:15.004578: step 68170, loss = 0.78 (740.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:58:16.747995: step 68180, loss = 0.69 (734.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:58:18.456478: step 68190, loss = 0.72 (749.2 examples/sec; 0.171 sec/batch)
2017-03-25 23:58:20.353929: step 68200, loss = 0.72 (674.6 examples/sec; 0.190 sec/batch)
2017-03-25 23:58:21.981931: step 68210, loss = 0.77 (786.2 examples/sec; 0.163 sec/batch)
2017-03-25 23:58:23.710445: step 68220, loss = 0.72 (740.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:58:25.441670: step 68230, loss = 0.80 (739.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:58:27.182427: step 68240, loss = 0.72 (735.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:58:28.917194: step 68250, loss = 0.78 (737.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:58:30.653775: step 68260, loss = 0.80 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:58:32.386969: step 68270, loss = 0.82 (738.5 examples/sec; 0.173 sec/batch)
2017-03-25 23:58:34.120801: step 68280, loss = 0.60 (738.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:58:35.857972: step 68290, loss = 0.69 (736.8 examples/sec; 0.174 sec/batch)
2017-03-25 23:58:37.758555: step 68300, loss = 0.74 (673.7 examples/sec; 0.190 sec/batch)
2017-03-25 23:58:39.363348: step 68310, loss = 0.82 (797.3 examples/sec; 0.161 sec/batch)
2017-03-25 23:58:41.093797: step 68320, loss = 0.69 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:58:42.838657: step 68330, loss = 0.68 (733.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:58:44.570992: step 68340, loss = 0.66 (738.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:58:46.289413: step 68350, loss = 0.77 (744.9 examples/sec; 0.172 sec/batch)
2017-03-25 23:58:48.012780: step 68360, loss = 0.73 (742.7 examples/sec; 0.172 sec/batch)
2017-03-25 23:58:49.753860: step 68370, loss = 0.77 (735.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:58:51.484219: step 68380, loss = 0.75 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:58:53.228077: step 68390, loss = 0.80 (734.0 examples/sec; 0.174 sec/batch)
2017-03-25 23:58:55.114167: step 68400, loss = 0.63 (678.7 examples/sec; 0.189 sec/batch)
2017-03-25 23:58:56.701356: step 68410, loss = 0.77 (806.5 examples/sec; 0.159 sec/batch)
2017-03-25 23:58:58.431324: step 68420, loss = 0.82 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:59:00.174217: step 68430, loss = 0.78 (734.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:59:01.911962: step 68440, loss = 0.81 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:59:03.639980: step 68450, loss = 0.71 (740.8 examples/sec; 0.173 sec/batch)
2017-03-25 23:59:05.368706: step 68460, loss = 0.76 (740.4 examples/sec; 0.173 sec/batch)
2017-03-25 23:59:07.099092: step 68470, loss = 0.68 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:59:08.826765: step 68480, loss = 0.67 (740.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:59:10.570827: step 68490, loss = 0.62 (733.9 examples/sec; 0.174 sec/batch)
2017-03-25 23:59:12.447644: step 68500, loss = 0.81 (682.3 examples/sec; 0.188 sec/batch)
2017-03-25 23:59:14.085740: step 68510, loss = 0.78 (781.0 examples/sec; 0.164 sec/batch)
2017-03-25 23:59:15.817582: step 68520, loss = 0.61 (739.1 examples/sec; 0.173 sec/batch)
2017-03-25 23:59:17.553485: step 68530, loss = 0.72 (737.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:59:19.282729: step 68540, loss = 0.71 (740.2 examples/sec; 0.173 sec/batch)
2017-03-25 23:59:21.021021: step 68550, loss = 0.77 (736.4 examples/sec; 0.174 sec/batch)
2017-03-25 23:59:22.754038: step 68560, loss = 0.63 (738.6 examples/sec; 0.173 sec/batch)
2017-03-25 23:59:24.485326: step 68570, loss = 0.77 (739.3 examples/sec; 0.173 sec/batch)
2017-03-25 23:59:26.221504: step 68580, loss = 0.80 (737.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:59:27.942757: step 68590, loss = 0.84 (743.6 examples/sec; 0.172 sec/batch)
2017-03-25 23:59:29.853930: step 68600, loss = 0.58 (669.7 examples/sec; 0.191 sec/batch)
2017-03-25 23:59:31.446187: step 68610, loss = 0.78 (803.9 examples/sec; 0.159 sec/batch)
2017-03-25 23:59:33.181812: step 68620, loss = 0.75 (737.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:59:34.920414: step 68630, loss = 0.70 (736.2 examples/sec; 0.174 sec/batch)
2017-03-25 23:59:36.659258: step 68640, loss = 0.75 (736.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:59:38.391215: step 68650, loss = 0.73 (739.0 examples/sec; 0.173 sec/batch)
2017-03-25 23:59:40.129749: step 68660, loss = 0.74 (736.3 examples/sec; 0.174 sec/batch)
2017-03-25 23:59:41.854132: step 68670, loss = 0.83 (742.3 examples/sec; 0.172 sec/batch)
2017-03-25 23:59:43.596891: step 68680, loss = 0.66 (734.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:59:45.321529: step 68690, loss = 0.71 (742.2 examples/sec; 0.172 sec/batch)
2017-03-25 23:59:47.211538: step 68700, loss = 0.75 (677.2 examples/sec; 0.189 sec/batch)
2017-03-25 23:59:48.819406: step 68710, loss = 0.71 (796.1 examples/sec; 0.161 sec/batch)
2017-03-25 23:59:50.549790: step 68720, loss = 0.66 (739.7 examples/sec; 0.173 sec/batch)
2017-03-25 23:59:52.287420: step 68730, loss = 0.79 (736.6 examples/sec; 0.174 sec/batch)
2017-03-25 23:59:54.018048: step 68740, loss = 0.57 (739.9 examples/sec; 0.173 sec/batch)
2017-03-25 23:59:55.755402: step 68750, loss = 0.70 (736.5 examples/sec; 0.174 sec/batch)
2017-03-25 23:59:57.491959: step 68760, loss = 0.68 (737.1 examples/sec; 0.174 sec/batch)
2017-03-25 23:59:59.233297: step 68770, loss = 0.70 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:00:00.970577: step 68780, loss = 0.74 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:00:02.717280: step 68790, loss = 0.63 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:00:04.620588: step 68800, loss = 0.69 (672.5 examples/sec; 0.190 sec/batch)
2017-03-26 00:00:06.214531: step 68810, loss = 0.78 (803.0 examples/sec; 0.159 sec/batch)
2017-03-26 00:00:07.939249: step 68820, loss = 0.82 (742.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:00:09.673441: step 68830, loss = 0.70 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:00:11.424837: step 68840, loss = 0.60 (730.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:00:13.154242: step 68850, loss = 0.70 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:00:14.890377: step 68860, loss = 0.72 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:00:16.623278: step 68870, loss = 0.86 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:00:18.347051: step 68880, loss = 0.73 (742.6 examples/sec; 0.172 sec/batch)
2017-03-26 00:00:20.084334: step 68890, loss = 0.70 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:00:21.962041: step 68900, loss = 0.55 (681.6 examples/sec; 0.188 sec/batch)
2017-03-26 00:00:23.586363: step 68910, loss = 0.68 (788.0 examples/sec; 0.162 sec/batch)
2017-03-26 00:00:25.318662: step 68920, loss = 0.78 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:00:27.052401: step 68930, loss = 0.88 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:00:28.780119: step 68940, loss = 0.58 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:00:30.509896: step 68950, loss = 0.70 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:00:32.239439: step 68960, loss = 0.77 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:00:33.966284: step 68970, loss = 0.75 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:00:35.702988: step 68980, loss = 0.70 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:00:37.444677: step 68990, loss = 0.85 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:00:39.324640: step 69000, loss = 0.77 (681.2 examples/sec; 0.188 sec/batch)
2017-03-26 00:00:40.945453: step 69010, loss = 0.68 (789.3 examples/sec; 0.162 sec/batch)
2017-03-26 00:00:42.676756: step 69020, loss = 0.76 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:00:44.412161: step 69030, loss = 0.63 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:00:46.141337: step 69040, loss = 0.72 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:00:47.869572: step 69050, loss = 0.77 (740.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:00:49.605162: step 69060, loss = 0.65 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:00:51.349939: step 69070, loss = 0.77 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:00:53.078147: step 69080, loss = 0.58 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:00:54.833983: step 69090, loss = 0.74 (729.0 examples/sec; 0.176 sec/batch)
2017-03-26 00:00:56.759919: step 69100, loss = 0.85 (664.9 examples/sec; 0.193 sec/batch)
2017-03-26 00:00:58.361438: step 69110, loss = 0.65 (798.9 examples/sec; 0.160 sec/batch)
2017-03-26 00:01:00.088147: step 69120, loss = 0.68 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:01.831567: step 69130, loss = 0.84 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:01:03.563342: step 69140, loss = 0.88 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:05.304128: step 69150, loss = 0.63 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:01:07.050483: step 69160, loss = 0.62 (733.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:01:08.790125: step 69170, loss = 0.68 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:01:10.528246: step 69180, loss = 0.89 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:01:12.260026: step 69190, loss = 0.80 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:14.180022: step 69200, loss = 0.85 (667.0 examples/sec; 0.192 sec/batch)
2017-03-26 00:01:15.743411: step 69210, loss = 0.71 (818.2 examples/sec; 0.156 sec/batch)
2017-03-26 00:01:17.475492: step 69220, loss = 0.62 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:19.206580: step 69230, loss = 0.54 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:20.935127: step 69240, loss = 0.71 (740.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:22.660947: step 69250, loss = 0.74 (741.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:24.394499: step 69260, loss = 0.60 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:26.120153: step 69270, loss = 0.75 (741.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:27.858428: step 69280, loss = 0.81 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:01:29.592060: step 69290, loss = 0.79 (738.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:31.557196: step 69300, loss = 0.77 (651.4 examples/sec; 0.197 sec/batch)
2017-03-26 00:01:33.069240: step 69310, loss = 0.82 (846.5 examples/sec; 0.151 sec/batch)
2017-03-26 00:01:34.765445: step 69320, loss = 0.60 (754.6 examples/sec; 0.170 sec/batch)
2017-03-26 00:01:36.519360: step 69330, loss = 0.76 (729.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:01:38.254220: step 69340, loss = 0.77 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:39.989312: step 69350, loss = 0.72 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:01:41.723476: step 69360, loss = 0.59 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:43.453060: step 69370, loss = 0.72 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:45.182564: step 69380, loss = 0.74 (740.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:46.904028: step 69390, loss = 0.81 (743.2 examples/sec; 0.172 sec/batch)
2017-03-26 00:01:48.800800: step 69400, loss = 0.75 (674.8 examples/sec; 0.190 sec/batch)
2017-03-26 00:01:50.422092: step 69410, loss = 0.67 (789.5 examples/sec; 0.162 sec/batch)
2017-03-26 00:01:52.162841: step 69420, loss = 0.84 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:01:53.891316: step 69430, loss = 0.69 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:55.623950: step 69440, loss = 0.71 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:01:57.360187: step 69450, loss = 0.77 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:01:59.098369: step 69460, loss = 0.75 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:02:00.829143: step 69470, loss = 0.74 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:02.562668: step 69480, loss = 0.59 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:04.301006: step 69490, loss = 0.70 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:02:06.237452: step 69500, loss = 0.91 (661.0 examples/sec; 0.194 sec/batch)
2017-03-26 00:02:07.800988: step 69510, loss = 0.66 (818.7 examples/sec; 0.156 sec/batch)
2017-03-26 00:02:09.532033: step 69520, loss = 0.71 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:11.267992: step 69530, loss = 0.74 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:02:13.003892: step 69540, loss = 0.69 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:02:14.731291: step 69550, loss = 0.66 (741.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:16.461102: step 69560, loss = 0.65 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:18.195257: step 69570, loss = 0.75 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:19.921674: step 69580, loss = 0.72 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:21.656085: step 69590, loss = 0.89 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:23.557348: step 69600, loss = 0.77 (673.2 examples/sec; 0.190 sec/batch)
2017-03-26 00:02:25.163666: step 69610, loss = 0.65 (796.8 examples/sec; 0.161 sec/batch)
2017-03-26 00:02:26.894030: step 69620, loss = 0.67 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:28.625810: step 69630, loss = 0.88 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:30.360309: step 69640, loss = 0.75 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:32.088058: step 69650, loss = 0.69 (740.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:33.834127: step 69660, loss = 0.63 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:02:35.563630: step 69670, loss = 0.59 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:37.301840: step 69680, loss = 0.69 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:02:39.036573: step 69690, loss = 0.59 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:41.016320: step 69700, loss = 0.74 (646.5 examples/sec; 0.198 sec/batch)
2017-03-26 00:02:42.525924: step 69710, loss = 0.64 (847.9 examples/sec; 0.151 sec/batch)
2017-03-26 00:02:44.262026: step 69720, loss = 0.73 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:02:45.991506: step 69730, loss = 0.66 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:47.717119: step 69740, loss = 0.78 (741.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:02:49.456756: step 69750, loss = 0.81 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:02:51.177356: step 69760, loss = 0.73 (744.5 examples/sec; 0.172 sec/batch)
2017-03-26 00:02:52.914100: step 69770, loss = 0.66 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:02:54.651280: step 69780, loss = 0.69 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:02:56.392311: step 69790, loss = 0.58 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:02:58.290861: step 69800, loss = 0.66 (674.2 examples/sec; 0.190 sec/batch)
2017-03-26 00:02:59.901713: step 69810, loss = 0.70 (794.6 examples/sec; 0.161 sec/batch)
2017-03-26 00:03:01.639138: step 69820, loss = 0.64 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:03:03.379092: step 69830, loss = 0.62 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:03:05.107066: step 69840, loss = 0.77 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:03:06.840342: step 69850, loss = 0.74 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:03:08.578648: step 69860, loss = 0.58 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:03:10.308631: step 69870, loss = 0.80 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:03:12.039165: step 69880, loss = 0.77 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:03:13.761748: step 69890, loss = 0.86 (743.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:03:15.665161: step 69900, loss = 0.72 (672.5 examples/sec; 0.190 sec/batch)
2017-03-26 00:03:17.270897: step 69910, loss = 0.69 (797.1 examples/sec; 0.161 sec/batch)
2017-03-26 00:03:18.995182: step 69920, loss = 0.71 (742.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:03:20.740462: step 69930, loss = 0.53 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:03:22.454392: step 69940, loss = 0.71 (746.8 examples/sec; 0.171 sec/batch)
2017-03-26 00:03:24.189937: step 69950, loss = 0.69 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:03:25.922362: step 69960, loss = 0.75 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:03:27.642693: step 69970, loss = 0.70 (744.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:03:29.382386: step 69980, loss = 0.77 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:03:31.105147: step 69990, loss = 0.63 (743.0 examples/sec; 0.172 sec/batch)
2017-03-26 00:03:33.004078: step 70000, loss = 0.80 (674.0 examples/sec; 0.190 sec/batch)
2017-03-26 00:03:34.615481: step 70010, loss = 0.71 (794.3 examples/sec; 0.161 sec/batch)
2017-03-26 00:03:36.346578: step 70020, loss = 0.83 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:03:38.075678: step 70030, loss = 0.66 (740.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:03:39.818554: step 70040, loss = 0.68 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:03:41.551220: step 70050, loss = 0.63 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:03:43.288271: step 70060, loss = 0.59 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:03:45.013271: step 70070, loss = 0.75 (742.0 examples/sec; 0.172 sec/batch)
2017-03-26 00:03:46.739637: step 70080, loss = 0.71 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:03:48.475597: step 70090, loss = 0.72 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:03:50.368861: step 70100, loss = 0.62 (676.3 examples/sec; 0.189 sec/batch)
2017-03-26 00:03:51.971340: step 70110, loss = 0.65 (798.3 examples/sec; 0.160 sec/batch)
2017-03-26 00:03:53.698053: step 70120, loss = 0.76 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:03:55.438622: step 70130, loss = 0.69 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:03:57.159869: step 70140, loss = 0.59 (743.6 examples/sec; 0.172 sec/batch)
2017-03-26 00:03:58.892035: step 70150, loss = 0.69 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:00.639486: step 70160, loss = 0.91 (732.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:04:02.369110: step 70170, loss = 0.73 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:04.107880: step 70180, loss = 0.75 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:04:05.843796: step 70190, loss = 0.72 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:04:07.744416: step 70200, loss = 0.87 (673.9 examples/sec; 0.190 sec/batch)
2017-03-26 00:04:09.344652: step 70210, loss = 0.61 (799.3 examples/sec; 0.160 sec/batch)
2017-03-26 00:04:11.068463: step 70220, loss = 0.72 (742.5 examples/sec; 0.172 sec/batch)
2017-03-26 00:04:12.804593: step 70230, loss = 0.73 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:04:14.534041: step 70240, loss = 0.71 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:16.260788: step 70250, loss = 0.82 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:18.002543: step 70260, loss = 0.83 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:04:19.737376: step 70270, loss = 0.61 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:21.474064: step 70280, loss = 0.78 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:04:23.193837: step 70290, loss = 0.70 (744.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:04:25.105085: step 70300, loss = 0.58 (669.7 examples/sec; 0.191 sec/batch)
2017-03-26 00:04:26.703598: step 70310, loss = 0.74 (800.7 examples/sec; 0.160 sec/batch)
2017-03-26 00:04:28.425387: step 70320, loss = 0.76 (743.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:04:30.155305: step 70330, loss = 0.77 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:31.894959: step 70340, loss = 0.86 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:04:33.625213: step 70350, loss = 0.89 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:35.348254: step 70360, loss = 0.72 (742.9 examples/sec; 0.172 sec/batch)
2017-03-26 00:04:37.081104: step 70370, loss = 0.53 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:38.807677: step 70380, loss = 0.65 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:40.534454: step 70390, loss = 0.64 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:42.472559: step 70400, loss = 0.71 (660.4 examples/sec; 0.194 sec/batch)
2017-03-26 00:04:44.047524: step 70410, loss = 0.77 (812.7 examples/sec; 0.157 sec/batch)
2017-03-26 00:04:45.760340: step 70420, loss = 0.67 (747.3 examples/sec; 0.171 sec/batch)
2017-03-26 00:04:47.504072: step 70430, loss = 0.76 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:04:49.236904: step 70440, loss = 0.64 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:50.966794: step 70450, loss = 0.71 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:52.697308: step 70460, loss = 0.69 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:54.430230: step 70470, loss = 0.79 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:56.158806: step 70480, loss = 0.69 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:04:57.897965: step 70490, loss = 0.73 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:04:59.811150: step 70500, loss = 0.69 (669.4 examples/sec; 0.191 sec/batch)
2017-03-26 00:05:01.405205: step 70510, loss = 0.81 (802.4 examples/sec; 0.160 sec/batch)
2017-03-26 00:05:03.140107: step 70520, loss = 0.75 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:05:04.885537: step 70530, loss = 0.65 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:05:06.617482: step 70540, loss = 0.81 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:05:08.340866: step 70550, loss = 0.68 (742.7 examples/sec; 0.172 sec/batch)
2017-03-26 00:05:10.077712: step 70560, loss = 0.73 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:05:11.812030: step 70570, loss = 0.77 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:05:13.541658: step 70580, loss = 0.81 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:05:15.266739: step 70590, loss = 0.61 (742.2 examples/sec; 0.172 sec/batch)
2017-03-26 00:05:17.146780: step 70600, loss = 0.80 (680.9 examples/sec; 0.188 sec/batch)
2017-03-26 00:05:18.771223: step 70610, loss = 0.74 (787.6 examples/sec; 0.163 sec/batch)
2017-03-26 00:05:20.509673: step 70620, loss = 0.72 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:05:22.239491: step 70630, loss = 0.88 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:05:23.982829: step 70640, loss = 0.64 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:05:25.727607: step 70650, loss = 0.70 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:05:27.450199: step 70660, loss = 0.87 (743.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:05:29.180506: step 70670, loss = 0.74 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:05:30.927954: step 70680, loss = 0.72 (732.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:05:32.665460: step 70690, loss = 0.55 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:05:34.560505: step 70700, loss = 0.64 (675.3 examples/sec; 0.190 sec/batch)
2017-03-26 00:05:36.156170: step 70710, loss = 0.64 (802.2 examples/sec; 0.160 sec/batch)
2017-03-26 00:05:37.889708: step 70720, loss = 0.64 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:05:39.624758: step 70730, loss = 0.81 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:05:41.355161: step 70740, loss = 0.71 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:05:43.101244: step 70750, loss = 0.56 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:05:44.819223: step 70760, loss = 0.75 (745.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:05:46.575670: step 70770, loss = 0.67 (728.7 examples/sec; 0.176 sec/batch)
2017-03-26 00:05:48.304541: step 70780, loss = 0.72 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:05:50.038960: step 70790, loss = 0.63 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:05:51.926481: step 70800, loss = 0.79 (678.1 examples/sec; 0.189 sec/batch)
2017-03-26 00:05:53.548554: step 70810, loss = 0.72 (789.1 examples/sec; 0.162 sec/batch)
2017-03-26 00:05:55.284010: step 70820, loss = 0.71 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:05:57.023312: step 70830, loss = 0.64 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:05:58.753992: step 70840, loss = 0.82 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:06:00.493716: step 70850, loss = 0.69 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:06:02.234406: step 70860, loss = 0.80 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:06:03.961425: step 70870, loss = 0.71 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:06:05.691222: step 70880, loss = 0.66 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:06:07.430492: step 70890, loss = 0.77 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:06:09.333822: step 70900, loss = 0.72 (672.9 examples/sec; 0.190 sec/batch)
2017-03-26 00:06:10.944666: step 70910, loss = 0.75 (794.1 examples/sec; 0.161 sec/batch)
2017-03-26 00:06:12.687358: step 70920, loss = 0.67 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:06:14.433625: step 70930, loss = 0.86 (733.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:06:16.174170: step 70940, loss = 0.59 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:06:17.903203: step 70950, loss = 0.72 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:06:19.631958: step 70960, loss = 0.74 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:06:21.356417: step 70970, loss = 0.65 (742.5 examples/sec; 0.172 sec/batch)
2017-03-26 00:06:23.095475: step 70980, loss = 0.64 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:06:24.827156: step 70990, loss = 0.59 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:06:26.799454: step 71000, loss = 0.71 (649.0 examples/sec; 0.197 sec/batch)
2017-03-26 00:06:28.329474: step 71010, loss = 0.84 (836.6 examples/sec; 0.153 sec/batch)
2017-03-26 00:06:30.056852: step 71020, loss = 0.56 (741.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:06:31.803233: step 71030, loss = 0.60 (732.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:06:33.533653: step 71040, loss = 0.72 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:06:35.254532: step 71050, loss = 0.77 (743.8 examples/sec; 0.172 sec/batch)
2017-03-26 00:06:36.983191: step 71060, loss = 0.75 (740.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:06:38.729439: step 71070, loss = 0.79 (733.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:06:40.454563: step 71080, loss = 0.62 (742.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:06:42.185826: step 71090, loss = 0.68 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:06:44.063476: step 71100, loss = 0.61 (681.7 examples/sec; 0.188 sec/batch)
2017-03-26 00:06:45.689936: step 71110, loss = 0.74 (787.0 examples/sec; 0.163 sec/batch)
2017-03-26 00:06:47.430019: step 71120, loss = 0.63 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:06:49.161213: step 71130, loss = 0.66 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:06:50.890755: step 71140, loss = 0.74 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:06:52.622938: step 71150, loss = 0.83 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:06:54.370109: step 71160, loss = 0.62 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:06:56.121030: step 71170, loss = 0.71 (731.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:06:57.851767: step 71180, loss = 0.57 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:06:59.582253: step 71190, loss = 0.62 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:01.492047: step 71200, loss = 0.72 (670.2 examples/sec; 0.191 sec/batch)
2017-03-26 00:07:03.089962: step 71210, loss = 0.71 (801.0 examples/sec; 0.160 sec/batch)
2017-03-26 00:07:04.815991: step 71220, loss = 0.78 (741.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:06.537637: step 71230, loss = 0.74 (743.5 examples/sec; 0.172 sec/batch)
2017-03-26 00:07:08.265287: step 71240, loss = 0.70 (740.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:09.992245: step 71250, loss = 0.76 (741.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:11.730105: step 71260, loss = 0.82 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:07:13.470289: step 71270, loss = 0.69 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:07:15.203436: step 71280, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:16.935596: step 71290, loss = 0.73 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:18.832898: step 71300, loss = 0.73 (675.0 examples/sec; 0.190 sec/batch)
2017-03-26 00:07:20.423078: step 71310, loss = 0.70 (804.5 examples/sec; 0.159 sec/batch)
2017-03-26 00:07:22.149968: step 71320, loss = 0.79 (741.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:23.882619: step 71330, loss = 0.77 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:25.607300: step 71340, loss = 0.68 (742.2 examples/sec; 0.172 sec/batch)
2017-03-26 00:07:27.339131: step 71350, loss = 0.81 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:29.082955: step 71360, loss = 0.77 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:07:30.816141: step 71370, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:32.537361: step 71380, loss = 0.80 (743.7 examples/sec; 0.172 sec/batch)
2017-03-26 00:07:34.264810: step 71390, loss = 0.72 (741.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:36.197017: step 71400, loss = 0.65 (662.7 examples/sec; 0.193 sec/batch)
2017-03-26 00:07:37.774506: step 71410, loss = 0.78 (811.3 examples/sec; 0.158 sec/batch)
2017-03-26 00:07:39.521605: step 71420, loss = 0.71 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:07:41.249545: step 71430, loss = 0.83 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:42.981069: step 71440, loss = 0.82 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:44.718111: step 71450, loss = 0.85 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:07:46.446098: step 71460, loss = 0.74 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:48.173318: step 71470, loss = 0.77 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:49.918523: step 71480, loss = 0.81 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:07:51.650909: step 71490, loss = 0.73 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:07:53.545980: step 71500, loss = 0.64 (675.4 examples/sec; 0.190 sec/batch)
2017-03-26 00:07:55.175493: step 71510, loss = 0.70 (785.5 examples/sec; 0.163 sec/batch)
2017-03-26 00:07:56.916514: step 71520, loss = 0.67 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:07:58.655874: step 71530, loss = 0.77 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:08:00.398571: step 71540, loss = 0.78 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:08:02.147699: step 71550, loss = 0.81 (731.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:08:03.891038: step 71560, loss = 0.67 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:08:05.630117: step 71570, loss = 0.69 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:08:07.376053: step 71580, loss = 0.64 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:08:09.121554: step 71590, loss = 0.78 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:08:11.007641: step 71600, loss = 0.69 (678.7 examples/sec; 0.189 sec/batch)
2017-03-26 00:08:12.628500: step 71610, loss = 0.56 (789.7 examples/sec; 0.162 sec/batch)
2017-03-26 00:08:14.355229: step 71620, loss = 0.85 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:08:16.097582: step 71630, loss = 0.75 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:08:17.825900: step 71640, loss = 0.57 (740.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:08:19.560797: step 71650, loss = 0.76 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:08:21.296247: step 71660, loss = 0.87 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:08:23.045630: step 71670, loss = 0.80 (731.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:08:24.769896: step 71680, loss = 0.73 (742.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:08:26.502575: step 71690, loss = 0.77 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:08:28.388979: step 71700, loss = 0.75 (678.8 examples/sec; 0.189 sec/batch)
2017-03-26 00:08:30.009446: step 71710, loss = 0.57 (789.5 examples/sec; 0.162 sec/batch)
2017-03-26 00:08:31.740076: step 71720, loss = 0.78 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:08:33.470927: step 71730, loss = 0.79 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:08:35.204496: step 71740, loss = 0.84 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:08:36.931041: step 71750, loss = 0.65 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:08:38.662858: step 71760, loss = 0.68 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:08:40.394290: step 71770, loss = 0.63 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:08:42.132696: step 71780, loss = 0.74 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:08:43.860557: step 71790, loss = 0.68 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:08:45.745603: step 71800, loss = 0.63 (679.3 examples/sec; 0.188 sec/batch)
2017-03-26 00:08:47.369104: step 71810, loss = 0.86 (788.1 examples/sec; 0.162 sec/batch)
2017-03-26 00:08:49.101428: step 71820, loss = 0.86 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:08:50.830191: step 71830, loss = 0.89 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:08:52.553217: step 71840, loss = 0.61 (742.9 examples/sec; 0.172 sec/batch)
2017-03-26 00:08:54.291174: step 71850, loss = 0.63 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:08:56.026820: step 71860, loss = 0.60 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:08:57.759253: step 71870, loss = 0.55 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:08:59.487642: step 71880, loss = 0.70 (740.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:09:01.220338: step 71890, loss = 0.69 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:09:03.125323: step 71900, loss = 0.71 (672.0 examples/sec; 0.190 sec/batch)
2017-03-26 00:09:04.750127: step 71910, loss = 0.72 (787.7 examples/sec; 0.162 sec/batch)
2017-03-26 00:09:06.496837: step 71920, loss = 0.67 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:09:08.233805: step 71930, loss = 0.81 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:09:09.960804: step 71940, loss = 0.62 (741.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:09:11.697511: step 71950, loss = 0.68 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:09:13.432460: step 71960, loss = 0.65 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:09:15.173332: step 71970, loss = 0.74 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:09:16.900679: step 71980, loss = 0.90 (741.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:09:18.645244: step 71990, loss = 0.82 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:09:20.534398: step 72000, loss = 0.79 (677.6 examples/sec; 0.189 sec/batch)
2017-03-26 00:09:22.157019: step 72010, loss = 0.78 (788.9 examples/sec; 0.162 sec/batch)
2017-03-26 00:09:23.889582: step 72020, loss = 0.72 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:09:25.631413: step 72030, loss = 0.69 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:09:27.363143: step 72040, loss = 0.68 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:09:29.088152: step 72050, loss = 0.75 (742.0 examples/sec; 0.172 sec/batch)
2017-03-26 00:09:30.839189: step 72060, loss = 0.67 (731.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:09:32.562694: step 72070, loss = 0.63 (742.7 examples/sec; 0.172 sec/batch)
2017-03-26 00:09:34.306134: step 72080, loss = 0.81 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:09:36.036885: step 72090, loss = 0.61 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:09:37.932623: step 72100, loss = 0.60 (675.4 examples/sec; 0.190 sec/batch)
2017-03-26 00:09:39.555363: step 72110, loss = 0.71 (788.6 examples/sec; 0.162 sec/batch)
2017-03-26 00:09:41.284039: step 72120, loss = 0.66 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:09:43.014287: step 72130, loss = 0.68 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:09:44.749754: step 72140, loss = 0.74 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:09:46.490023: step 72150, loss = 0.75 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:09:48.221128: step 72160, loss = 0.86 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:09:49.934735: step 72170, loss = 0.75 (746.8 examples/sec; 0.171 sec/batch)
2017-03-26 00:09:51.661144: step 72180, loss = 0.50 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:09:53.410178: step 72190, loss = 0.79 (732.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:09:55.319878: step 72200, loss = 0.70 (670.3 examples/sec; 0.191 sec/batch)
2017-03-26 00:09:56.947107: step 72210, loss = 0.71 (786.3 examples/sec; 0.163 sec/batch)
2017-03-26 00:09:58.700526: step 72220, loss = 0.67 (730.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:10:00.454025: step 72230, loss = 0.68 (730.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:10:02.190277: step 72240, loss = 0.65 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:10:03.934321: step 72250, loss = 0.62 (733.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:10:05.665561: step 72260, loss = 0.67 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:10:07.402864: step 72270, loss = 0.69 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:10:09.141296: step 72280, loss = 0.76 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:10:10.882508: step 72290, loss = 0.87 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:10:12.802187: step 72300, loss = 0.75 (666.8 examples/sec; 0.192 sec/batch)
2017-03-26 00:10:14.394465: step 72310, loss = 0.76 (803.9 examples/sec; 0.159 sec/batch)
2017-03-26 00:10:16.143347: step 72320, loss = 0.83 (731.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:10:17.877711: step 72330, loss = 0.76 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:10:19.615815: step 72340, loss = 0.83 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:10:21.349756: step 72350, loss = 0.69 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:10:23.080014: step 72360, loss = 0.85 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:10:24.805007: step 72370, loss = 0.82 (742.0 examples/sec; 0.172 sec/batch)
2017-03-26 00:10:26.553289: step 72380, loss = 0.64 (732.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:10:28.279765: step 72390, loss = 0.64 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:10:30.189661: step 72400, loss = 0.75 (670.4 examples/sec; 0.191 sec/batch)
2017-03-26 00:10:31.800142: step 72410, loss = 0.71 (794.5 examples/sec; 0.161 sec/batch)
2017-03-26 00:10:33.534092: step 72420, loss = 0.78 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:10:35.269257: step 72430, loss = 0.79 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:10:37.007046: step 72440, loss = 0.67 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:10:38.725100: step 72450, loss = 0.60 (745.0 examples/sec; 0.172 sec/batch)
2017-03-26 00:10:40.449109: step 72460, loss = 0.66 (742.6 examples/sec; 0.172 sec/batch)
2017-03-26 00:10:42.181191: step 72470, loss = 0.62 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:10:43.909753: step 72480, loss = 0.67 (740.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:10:45.648918: step 72490, loss = 0.67 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:10:47.541649: step 72500, loss = 0.77 (676.3 examples/sec; 0.189 sec/batch)
2017-03-26 00:10:49.155155: step 72510, loss = 0.61 (793.3 examples/sec; 0.161 sec/batch)
2017-03-26 00:10:50.881981: step 72520, loss = 0.59 (741.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:10:52.620740: step 72530, loss = 0.80 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:10:54.391677: step 72540, loss = 0.74 (722.8 examples/sec; 0.177 sec/batch)
2017-03-26 00:10:56.133676: step 72550, loss = 0.72 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:10:57.872906: step 72560, loss = 0.77 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:10:59.604799: step 72570, loss = 0.67 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:11:01.335123: step 72580, loss = 0.68 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:11:03.072024: step 72590, loss = 0.78 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:11:05.038648: step 72600, loss = 0.58 (651.1 examples/sec; 0.197 sec/batch)
2017-03-26 00:11:06.569180: step 72610, loss = 0.58 (835.8 examples/sec; 0.153 sec/batch)
2017-03-26 00:11:08.300602: step 72620, loss = 0.76 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:11:10.037035: step 72630, loss = 0.70 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:11:11.772579: step 72640, loss = 0.65 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:11:13.501418: step 72650, loss = 0.52 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:11:15.234247: step 72660, loss = 0.58 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:11:16.976299: step 72670, loss = 0.72 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:11:18.694062: step 72680, loss = 0.74 (745.2 examples/sec; 0.172 sec/batch)
2017-03-26 00:11:20.439590: step 72690, loss = 0.72 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:11:22.366982: step 72700, loss = 0.77 (664.3 examples/sec; 0.193 sec/batch)
2017-03-26 00:11:23.955201: step 72710, loss = 0.68 (805.9 examples/sec; 0.159 sec/batch)
2017-03-26 00:11:25.684488: step 72720, loss = 0.74 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:11:27.409132: step 72730, loss = 0.57 (742.2 examples/sec; 0.172 sec/batch)
2017-03-26 00:11:29.144472: step 72740, loss = 0.75 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:11:30.879231: step 72750, loss = 0.75 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:11:32.620755: step 72760, loss = 0.80 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:11:34.346371: step 72770, loss = 0.68 (741.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:11:36.075765: step 72780, loss = 0.60 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:11:37.814565: step 72790, loss = 0.55 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:11:39.708013: step 72800, loss = 0.73 (676.0 examples/sec; 0.189 sec/batch)
2017-03-26 00:11:41.314438: step 72810, loss = 0.70 (796.8 examples/sec; 0.161 sec/batch)
2017-03-26 00:11:43.050773: step 72820, loss = 0.65 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:11:44.785708: step 72830, loss = 0.59 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:11:46.504533: step 72840, loss = 0.77 (744.7 examples/sec; 0.172 sec/batch)
2017-03-26 00:11:48.233403: step 72850, loss = 0.78 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:11:49.977347: step 72860, loss = 0.64 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:11:51.706610: step 72870, loss = 0.73 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:11:53.426233: step 72880, loss = 0.76 (744.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:11:55.153584: step 72890, loss = 0.88 (741.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:11:57.088617: step 72900, loss = 0.67 (661.9 examples/sec; 0.193 sec/batch)
2017-03-26 00:11:58.675397: step 72910, loss = 0.78 (806.1 examples/sec; 0.159 sec/batch)
2017-03-26 00:12:00.438349: step 72920, loss = 0.77 (726.0 examples/sec; 0.176 sec/batch)
2017-03-26 00:12:02.177585: step 72930, loss = 0.73 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:12:03.901240: step 72940, loss = 0.81 (742.6 examples/sec; 0.172 sec/batch)
2017-03-26 00:12:05.635473: step 72950, loss = 0.65 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:12:07.372173: step 72960, loss = 0.81 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:12:09.097862: step 72970, loss = 0.78 (741.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:12:10.844292: step 72980, loss = 0.84 (732.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:12:12.566347: step 72990, loss = 0.91 (743.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:12:14.502895: step 73000, loss = 0.83 (661.0 examples/sec; 0.194 sec/batch)
2017-03-26 00:12:16.077444: step 73010, loss = 0.55 (812.9 examples/sec; 0.157 sec/batch)
2017-03-26 00:12:17.804120: step 73020, loss = 0.65 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:12:19.542951: step 73030, loss = 0.77 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:12:21.286937: step 73040, loss = 0.71 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:12:23.016635: step 73050, loss = 0.81 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:12:24.751876: step 73060, loss = 0.60 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:12:26.497752: step 73070, loss = 0.78 (733.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:12:28.231600: step 73080, loss = 0.70 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:12:29.956432: step 73090, loss = 0.70 (742.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:12:31.866012: step 73100, loss = 0.71 (670.3 examples/sec; 0.191 sec/batch)
2017-03-26 00:12:33.469456: step 73110, loss = 0.65 (798.3 examples/sec; 0.160 sec/batch)
2017-03-26 00:12:35.212967: step 73120, loss = 0.62 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:12:36.947023: step 73130, loss = 0.60 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:12:38.674338: step 73140, loss = 0.72 (741.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:12:40.411893: step 73150, loss = 0.68 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:12:42.141146: step 73160, loss = 0.68 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:12:43.875308: step 73170, loss = 0.52 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:12:45.615714: step 73180, loss = 0.60 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:12:47.360761: step 73190, loss = 0.65 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:12:49.258367: step 73200, loss = 0.74 (674.4 examples/sec; 0.190 sec/batch)
2017-03-26 00:12:50.876357: step 73210, loss = 0.79 (791.1 examples/sec; 0.162 sec/batch)
2017-03-26 00:12:52.604207: step 73220, loss = 0.71 (740.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:12:54.334210: step 73230, loss = 0.66 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:12:56.061985: step 73240, loss = 0.69 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:12:57.788733: step 73250, loss = 0.78 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:12:59.509012: step 73260, loss = 0.79 (744.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:13:01.246166: step 73270, loss = 0.62 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:13:02.988143: step 73280, loss = 0.75 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:13:04.717981: step 73290, loss = 0.73 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:13:06.622550: step 73300, loss = 0.79 (672.5 examples/sec; 0.190 sec/batch)
2017-03-26 00:13:08.217220: step 73310, loss = 0.63 (802.0 examples/sec; 0.160 sec/batch)
2017-03-26 00:13:09.965148: step 73320, loss = 0.69 (732.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:13:11.700718: step 73330, loss = 0.91 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:13:13.438913: step 73340, loss = 0.73 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:13:15.158421: step 73350, loss = 0.78 (744.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:13:16.892708: step 73360, loss = 0.86 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:13:18.620602: step 73370, loss = 0.55 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:13:20.349043: step 73380, loss = 0.53 (740.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:13:22.077719: step 73390, loss = 0.78 (740.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:13:23.957116: step 73400, loss = 0.75 (681.3 examples/sec; 0.188 sec/batch)
2017-03-26 00:13:25.581472: step 73410, loss = 0.67 (787.8 examples/sec; 0.162 sec/batch)
2017-03-26 00:13:27.310856: step 73420, loss = 0.73 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:13:29.030794: step 73430, loss = 0.78 (744.2 examples/sec; 0.172 sec/batch)
2017-03-26 00:13:30.760012: step 73440, loss = 0.92 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:13:32.495535: step 73450, loss = 0.66 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:13:34.220779: step 73460, loss = 0.75 (741.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:13:35.958594: step 73470, loss = 0.76 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:13:37.691689: step 73480, loss = 0.62 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:13:39.428947: step 73490, loss = 0.80 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:13:41.299442: step 73500, loss = 0.72 (684.3 examples/sec; 0.187 sec/batch)
2017-03-26 00:13:42.926727: step 73510, loss = 0.74 (786.6 examples/sec; 0.163 sec/batch)
2017-03-26 00:13:44.658182: step 73520, loss = 0.65 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:13:46.386339: step 73530, loss = 0.64 (740.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:13:48.110664: step 73540, loss = 0.81 (742.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:13:49.845046: step 73550, loss = 0.80 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:13:51.567965: step 73560, loss = 0.75 (742.9 examples/sec; 0.172 sec/batch)
2017-03-26 00:13:53.287030: step 73570, loss = 0.79 (744.6 examples/sec; 0.172 sec/batch)
2017-03-26 00:13:55.023137: step 73580, loss = 0.89 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:13:56.745346: step 73590, loss = 0.59 (743.2 examples/sec; 0.172 sec/batch)
2017-03-26 00:13:58.648605: step 73600, loss = 0.66 (672.8 examples/sec; 0.190 sec/batch)
2017-03-26 00:14:00.254080: step 73610, loss = 0.76 (796.9 examples/sec; 0.161 sec/batch)
2017-03-26 00:14:02.003253: step 73620, loss = 0.87 (731.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:14:03.726930: step 73630, loss = 0.66 (742.6 examples/sec; 0.172 sec/batch)
2017-03-26 00:14:05.461409: step 73640, loss = 0.71 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:14:07.189629: step 73650, loss = 0.79 (740.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:14:08.928554: step 73660, loss = 0.68 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:14:10.668506: step 73670, loss = 0.85 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:14:12.396610: step 73680, loss = 0.76 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:14:14.122475: step 73690, loss = 0.85 (741.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:14:16.115531: step 73700, loss = 0.68 (642.6 examples/sec; 0.199 sec/batch)
2017-03-26 00:14:17.647657: step 73710, loss = 0.73 (834.6 examples/sec; 0.153 sec/batch)
2017-03-26 00:14:19.374927: step 73720, loss = 0.78 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:14:21.108270: step 73730, loss = 0.75 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:14:22.831062: step 73740, loss = 0.82 (743.0 examples/sec; 0.172 sec/batch)
2017-03-26 00:14:24.561520: step 73750, loss = 0.66 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:14:26.305282: step 73760, loss = 0.67 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:14:28.032553: step 73770, loss = 0.60 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:14:29.778493: step 73780, loss = 0.66 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:14:31.506684: step 73790, loss = 0.56 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:14:33.381523: step 73800, loss = 0.75 (682.7 examples/sec; 0.187 sec/batch)
2017-03-26 00:14:34.994338: step 73810, loss = 0.65 (793.6 examples/sec; 0.161 sec/batch)
2017-03-26 00:14:36.733946: step 73820, loss = 0.58 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:14:38.466626: step 73830, loss = 0.55 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:14:40.204865: step 73840, loss = 0.65 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:14:41.959265: step 73850, loss = 0.79 (729.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:14:43.711528: step 73860, loss = 0.82 (730.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:14:45.449534: step 73870, loss = 0.65 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:14:47.181507: step 73880, loss = 0.73 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:14:48.908893: step 73890, loss = 0.63 (741.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:14:50.817397: step 73900, loss = 0.61 (671.2 examples/sec; 0.191 sec/batch)
2017-03-26 00:14:52.420911: step 73910, loss = 0.68 (797.6 examples/sec; 0.160 sec/batch)
2017-03-26 00:14:54.171077: step 73920, loss = 0.67 (731.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:14:55.899914: step 73930, loss = 0.72 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:14:57.642446: step 73940, loss = 0.65 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:14:59.377281: step 73950, loss = 0.78 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:15:01.118824: step 73960, loss = 0.84 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:15:02.873916: step 73970, loss = 0.63 (729.3 examples/sec; 0.176 sec/batch)
2017-03-26 00:15:04.596178: step 73980, loss = 0.69 (743.2 examples/sec; 0.172 sec/batch)
2017-03-26 00:15:06.330651: step 73990, loss = 0.61 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:15:08.239441: step 74000, loss = 0.84 (670.9 examples/sec; 0.191 sec/batch)
2017-03-26 00:15:09.846491: step 74010, loss = 0.66 (796.0 examples/sec; 0.161 sec/batch)
2017-03-26 00:15:11.594393: step 74020, loss = 0.89 (732.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:15:13.333696: step 74030, loss = 0.67 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:15:15.072978: step 74040, loss = 0.70 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:15:16.817811: step 74050, loss = 0.62 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:15:18.532965: step 74060, loss = 0.87 (746.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:15:20.275577: step 74070, loss = 0.68 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:15:22.004033: step 74080, loss = 0.83 (740.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:15:23.737996: step 74090, loss = 0.78 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:15:25.712859: step 74100, loss = 0.72 (648.4 examples/sec; 0.197 sec/batch)
2017-03-26 00:15:27.188776: step 74110, loss = 0.76 (866.7 examples/sec; 0.148 sec/batch)
2017-03-26 00:15:28.898331: step 74120, loss = 0.75 (748.7 examples/sec; 0.171 sec/batch)
2017-03-26 00:15:30.629317: step 74130, loss = 0.69 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:15:32.357138: step 74140, loss = 0.65 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:15:34.082302: step 74150, loss = 0.70 (742.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:15:35.820329: step 74160, loss = 0.83 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:15:37.560726: step 74170, loss = 0.73 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:15:39.289224: step 74180, loss = 0.79 (740.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:15:41.035584: step 74190, loss = 0.73 (733.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:15:42.944610: step 74200, loss = 0.82 (670.5 examples/sec; 0.191 sec/batch)
2017-03-26 00:15:44.555616: step 74210, loss = 0.73 (794.6 examples/sec; 0.161 sec/batch)
2017-03-26 00:15:46.299700: step 74220, loss = 0.81 (733.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:15:48.047838: step 74230, loss = 0.73 (732.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:15:49.792955: step 74240, loss = 0.85 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:15:51.546417: step 74250, loss = 0.82 (730.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:15:53.289403: step 74260, loss = 0.73 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:15:55.020136: step 74270, loss = 0.76 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:15:56.746593: step 74280, loss = 1.03 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:15:58.492672: step 74290, loss = 0.78 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:16:00.502581: step 74300, loss = 0.74 (636.8 examples/sec; 0.201 sec/batch)
2017-03-26 00:16:01.982476: step 74310, loss = 0.56 (864.9 examples/sec; 0.148 sec/batch)
2017-03-26 00:16:03.687283: step 74320, loss = 0.81 (750.8 examples/sec; 0.170 sec/batch)
2017-03-26 00:16:05.373384: step 74330, loss = 0.64 (759.1 examples/sec; 0.169 sec/batch)
2017-03-26 00:16:07.064398: step 74340, loss = 0.71 (757.0 examples/sec; 0.169 sec/batch)
2017-03-26 00:16:08.759581: step 74350, loss = 0.81 (755.1 examples/sec; 0.170 sec/batch)
2017-03-26 00:16:10.458987: step 74360, loss = 0.67 (753.2 examples/sec; 0.170 sec/batch)
2017-03-26 00:16:12.153318: step 74370, loss = 0.60 (755.5 examples/sec; 0.169 sec/batch)
2017-03-26 00:16:13.839569: step 74380, loss = 0.74 (759.1 examples/sec; 0.169 sec/batch)
2017-03-26 00:16:15.572675: step 74390, loss = 0.70 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:16:17.485053: step 74400, loss = 0.72 (669.3 examples/sec; 0.191 sec/batch)
2017-03-26 00:16:19.082710: step 74410, loss = 0.77 (801.2 examples/sec; 0.160 sec/batch)
2017-03-26 00:16:20.819515: step 74420, loss = 0.80 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:16:22.549187: step 74430, loss = 0.98 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:16:24.288758: step 74440, loss = 0.72 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:16:26.023897: step 74450, loss = 0.71 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:16:27.763585: step 74460, loss = 0.74 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:16:29.511550: step 74470, loss = 0.79 (732.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:16:31.245487: step 74480, loss = 0.65 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:16:32.977475: step 74490, loss = 0.66 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:16:34.875312: step 74500, loss = 0.69 (674.4 examples/sec; 0.190 sec/batch)
2017-03-26 00:16:36.479625: step 74510, loss = 0.80 (797.9 examples/sec; 0.160 sec/batch)
2017-03-26 00:16:38.201568: step 74520, loss = 0.78 (743.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:16:39.925669: step 74530, loss = 0.69 (742.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:16:41.676587: step 74540, loss = 0.80 (731.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:16:43.401136: step 74550, loss = 0.84 (742.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:16:45.137112: step 74560, loss = 0.76 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:16:46.871334: step 74570, loss = 0.60 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:16:48.608642: step 74580, loss = 0.66 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:16:50.353795: step 74590, loss = 0.87 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:16:52.264758: step 74600, loss = 0.65 (670.3 examples/sec; 0.191 sec/batch)
2017-03-26 00:16:53.843629: step 74610, loss = 0.75 (810.0 examples/sec; 0.158 sec/batch)
2017-03-26 00:16:55.579219: step 74620, loss = 0.76 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:16:57.319280: step 74630, loss = 0.76 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:16:59.052724: step 74640, loss = 0.70 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:17:00.798071: step 74650, loss = 0.75 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:17:02.534012: step 74660, loss = 0.77 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:17:04.254842: step 74670, loss = 0.70 (743.8 examples/sec; 0.172 sec/batch)
2017-03-26 00:17:05.984910: step 74680, loss = 0.61 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:17:07.706927: step 74690, loss = 0.61 (743.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:17:09.664377: step 74700, loss = 0.68 (654.0 examples/sec; 0.196 sec/batch)
2017-03-26 00:17:11.206661: step 74710, loss = 0.75 (829.9 examples/sec; 0.154 sec/batch)
2017-03-26 00:17:12.939152: step 74720, loss = 0.68 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:17:14.677780: step 74730, loss = 0.84 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:17:16.406269: step 74740, loss = 0.67 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:17:18.140712: step 74750, loss = 0.59 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:17:19.881799: step 74760, loss = 0.76 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:17:21.608036: step 74770, loss = 0.70 (741.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:17:23.329925: step 74780, loss = 0.64 (743.5 examples/sec; 0.172 sec/batch)
2017-03-26 00:17:25.059827: step 74790, loss = 0.78 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:17:26.972780: step 74800, loss = 0.81 (669.3 examples/sec; 0.191 sec/batch)
2017-03-26 00:17:28.576015: step 74810, loss = 0.60 (798.3 examples/sec; 0.160 sec/batch)
2017-03-26 00:17:30.297751: step 74820, loss = 0.72 (743.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:17:32.041706: step 74830, loss = 0.67 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:17:33.774643: step 74840, loss = 0.59 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:17:35.508809: step 74850, loss = 0.64 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:17:37.246485: step 74860, loss = 0.71 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:17:38.981862: step 74870, loss = 0.92 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:17:40.724359: step 74880, loss = 0.79 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:17:42.461541: step 74890, loss = 0.81 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:17:44.368363: step 74900, loss = 0.72 (671.6 examples/sec; 0.191 sec/batch)
2017-03-26 00:17:45.959582: step 74910, loss = 0.80 (804.0 examples/sec; 0.159 sec/batch)
2017-03-26 00:17:47.683312: step 74920, loss = 0.71 (742.6 examples/sec; 0.172 sec/batch)
2017-03-26 00:17:49.419165: step 74930, loss = 0.79 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:17:51.159828: step 74940, loss = 0.82 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:17:52.896490: step 74950, loss = 0.73 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:17:54.620620: step 74960, loss = 0.71 (742.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:17:56.341561: step 74970, loss = 0.67 (743.9 examples/sec; 0.172 sec/batch)
2017-03-26 00:17:58.072732: step 74980, loss = 0.79 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:17:59.806631: step 74990, loss = 0.60 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:18:01.699068: step 75000, loss = 0.79 (676.4 examples/sec; 0.189 sec/batch)
2017-03-26 00:18:03.315813: step 75010, loss = 0.73 (791.7 examples/sec; 0.162 sec/batch)
2017-03-26 00:18:05.047522: step 75020, loss = 0.71 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:18:06.788325: step 75030, loss = 0.70 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:18:08.519222: step 75040, loss = 0.70 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:18:10.258613: step 75050, loss = 0.75 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:18:11.993679: step 75060, loss = 0.67 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:18:13.721868: step 75070, loss = 0.75 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:18:15.482409: step 75080, loss = 0.79 (727.0 examples/sec; 0.176 sec/batch)
2017-03-26 00:18:17.224820: step 75090, loss = 0.73 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:18:19.115298: step 75100, loss = 0.71 (677.3 examples/sec; 0.189 sec/batch)
2017-03-26 00:18:20.760121: step 75110, loss = 0.73 (777.8 examples/sec; 0.165 sec/batch)
2017-03-26 00:18:22.495827: step 75120, loss = 0.75 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:18:24.234139: step 75130, loss = 0.67 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:18:25.961491: step 75140, loss = 0.73 (741.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:18:27.704299: step 75150, loss = 0.79 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:18:29.429639: step 75160, loss = 0.76 (741.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:18:31.165180: step 75170, loss = 0.85 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:18:32.906588: step 75180, loss = 0.86 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:18:34.629255: step 75190, loss = 0.63 (743.0 examples/sec; 0.172 sec/batch)
2017-03-26 00:18:36.501609: step 75200, loss = 0.81 (683.6 examples/sec; 0.187 sec/batch)
2017-03-26 00:18:38.126771: step 75210, loss = 0.76 (787.6 examples/sec; 0.163 sec/batch)
2017-03-26 00:18:39.859792: step 75220, loss = 0.67 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:18:41.585752: step 75230, loss = 0.61 (741.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:18:43.337343: step 75240, loss = 0.59 (730.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:18:45.065428: step 75250, loss = 0.66 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:18:46.805028: step 75260, loss = 0.89 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:18:48.530178: step 75270, loss = 0.80 (742.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:18:50.269075: step 75280, loss = 0.75 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:18:52.004907: step 75290, loss = 0.73 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:18:53.992828: step 75300, loss = 0.65 (644.2 examples/sec; 0.199 sec/batch)
2017-03-26 00:18:55.521531: step 75310, loss = 0.77 (836.8 examples/sec; 0.153 sec/batch)
2017-03-26 00:18:57.243127: step 75320, loss = 0.73 (743.5 examples/sec; 0.172 sec/batch)
2017-03-26 00:18:58.973080: step 75330, loss = 0.84 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:19:00.693938: step 75340, loss = 0.69 (743.8 examples/sec; 0.172 sec/batch)
2017-03-26 00:19:02.448561: step 75350, loss = 0.77 (729.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:19:04.171789: step 75360, loss = 0.83 (742.8 examples/sec; 0.172 sec/batch)
2017-03-26 00:19:05.900071: step 75370, loss = 0.70 (740.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:19:07.633299: step 75380, loss = 0.67 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:19:09.374347: step 75390, loss = 0.80 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:19:11.271474: step 75400, loss = 0.59 (674.7 examples/sec; 0.190 sec/batch)
2017-03-26 00:19:12.891474: step 75410, loss = 0.84 (790.1 examples/sec; 0.162 sec/batch)
2017-03-26 00:19:14.639531: step 75420, loss = 0.71 (732.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:19:16.377711: step 75430, loss = 0.70 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:19:18.134695: step 75440, loss = 0.68 (728.4 examples/sec; 0.176 sec/batch)
2017-03-26 00:19:19.881931: step 75450, loss = 0.68 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:19:21.625123: step 75460, loss = 0.81 (734.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:19:23.354864: step 75470, loss = 0.82 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:19:25.091044: step 75480, loss = 0.73 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:19:26.833359: step 75490, loss = 0.85 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:19:28.755227: step 75500, loss = 0.70 (666.3 examples/sec; 0.192 sec/batch)
2017-03-26 00:19:30.351686: step 75510, loss = 0.79 (801.3 examples/sec; 0.160 sec/batch)
2017-03-26 00:19:32.084143: step 75520, loss = 0.64 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:19:33.825042: step 75530, loss = 0.71 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:19:35.568705: step 75540, loss = 0.77 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:19:37.311479: step 75550, loss = 0.88 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:19:39.053510: step 75560, loss = 0.74 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:19:40.780963: step 75570, loss = 0.74 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:19:42.513615: step 75580, loss = 0.69 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:19:44.258589: step 75590, loss = 0.61 (733.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:19:46.246138: step 75600, loss = 0.71 (644.0 examples/sec; 0.199 sec/batch)
2017-03-26 00:19:47.768094: step 75610, loss = 0.64 (841.0 examples/sec; 0.152 sec/batch)
2017-03-26 00:19:49.499423: step 75620, loss = 0.78 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:19:51.223911: step 75630, loss = 0.76 (742.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:19:52.959683: step 75640, loss = 0.60 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:19:54.698205: step 75650, loss = 0.62 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:19:56.428996: step 75660, loss = 0.68 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:19:58.160839: step 75670, loss = 0.64 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:19:59.892145: step 75680, loss = 0.62 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:20:01.638797: step 75690, loss = 0.67 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:20:03.610068: step 75700, loss = 0.72 (649.3 examples/sec; 0.197 sec/batch)
2017-03-26 00:20:05.135577: step 75710, loss = 0.65 (839.1 examples/sec; 0.153 sec/batch)
2017-03-26 00:20:06.884869: step 75720, loss = 0.86 (731.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:20:08.611063: step 75730, loss = 0.85 (741.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:20:10.356147: step 75740, loss = 0.73 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:20:12.074827: step 75750, loss = 0.75 (744.8 examples/sec; 0.172 sec/batch)
2017-03-26 00:20:13.805290: step 75760, loss = 0.64 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:20:15.540461: step 75770, loss = 0.48 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:20:17.276051: step 75780, loss = 0.64 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:20:19.006669: step 75790, loss = 0.73 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:20:20.900366: step 75800, loss = 0.72 (676.3 examples/sec; 0.189 sec/batch)
2017-03-26 00:20:22.509044: step 75810, loss = 0.81 (795.1 examples/sec; 0.161 sec/batch)
2017-03-26 00:20:24.255700: step 75820, loss = 0.66 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:20:25.971694: step 75830, loss = 0.69 (745.9 examples/sec; 0.172 sec/batch)
2017-03-26 00:20:27.708093: step 75840, loss = 0.75 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:20:29.453146: step 75850, loss = 0.80 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:20:31.185686: step 75860, loss = 0.66 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:20:32.919396: step 75870, loss = 0.82 (738.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:20:34.652266: step 75880, loss = 0.70 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:20:36.387377: step 75890, loss = 0.62 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:20:38.270599: step 75900, loss = 0.85 (679.7 examples/sec; 0.188 sec/batch)
2017-03-26 00:20:39.890002: step 75910, loss = 0.82 (790.4 examples/sec; 0.162 sec/batch)
2017-03-26 00:20:41.626497: step 75920, loss = 0.70 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:20:43.348099: step 75930, loss = 0.71 (743.2 examples/sec; 0.172 sec/batch)
2017-03-26 00:20:45.083501: step 75940, loss = 0.81 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:20:46.817147: step 75950, loss = 0.60 (738.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:20:48.560604: step 75960, loss = 0.76 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:20:50.282635: step 75970, loss = 0.68 (743.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:20:52.009137: step 75980, loss = 0.55 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:20:53.889615: step 75990, loss = 0.71 (680.7 examples/sec; 0.188 sec/batch)
2017-03-26 00:20:55.703775: step 76000, loss = 0.81 (705.7 examples/sec; 0.181 sec/batch)
2017-03-26 00:20:57.325270: step 76010, loss = 0.66 (789.1 examples/sec; 0.162 sec/batch)
2017-03-26 00:20:59.057627: step 76020, loss = 0.79 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:21:00.793800: step 76030, loss = 0.81 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:21:02.535690: step 76040, loss = 0.92 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:21:04.275888: step 76050, loss = 0.87 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:21:06.017678: step 76060, loss = 0.56 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:21:07.769892: step 76070, loss = 0.73 (730.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:21:09.509237: step 76080, loss = 0.76 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:21:11.235726: step 76090, loss = 0.73 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:21:13.122151: step 76100, loss = 0.57 (678.5 examples/sec; 0.189 sec/batch)
2017-03-26 00:21:14.761274: step 76110, loss = 0.64 (780.8 examples/sec; 0.164 sec/batch)
2017-03-26 00:21:16.495161: step 76120, loss = 0.57 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:21:18.242315: step 76130, loss = 0.75 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:21:19.961068: step 76140, loss = 0.70 (744.7 examples/sec; 0.172 sec/batch)
2017-03-26 00:21:21.709473: step 76150, loss = 0.72 (732.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:21:23.436427: step 76160, loss = 0.67 (741.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:21:25.183929: step 76170, loss = 0.66 (732.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:21:26.920057: step 76180, loss = 0.60 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:21:28.647078: step 76190, loss = 0.93 (741.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:21:30.557000: step 76200, loss = 0.65 (670.6 examples/sec; 0.191 sec/batch)
2017-03-26 00:21:32.155675: step 76210, loss = 0.80 (800.1 examples/sec; 0.160 sec/batch)
2017-03-26 00:21:33.890389: step 76220, loss = 0.75 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:21:35.625737: step 76230, loss = 0.65 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:21:37.355460: step 76240, loss = 0.70 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:21:39.082255: step 76250, loss = 0.78 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:21:40.813130: step 76260, loss = 0.67 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:21:42.537291: step 76270, loss = 0.64 (742.5 examples/sec; 0.172 sec/batch)
2017-03-26 00:21:44.262893: step 76280, loss = 0.72 (741.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:21:45.994339: step 76290, loss = 0.74 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:21:47.881431: step 76300, loss = 0.66 (678.3 examples/sec; 0.189 sec/batch)
2017-03-26 00:21:49.502662: step 76310, loss = 0.73 (789.5 examples/sec; 0.162 sec/batch)
2017-03-26 00:21:51.229065: step 76320, loss = 0.73 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:21:52.963025: step 76330, loss = 0.78 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:21:54.711484: step 76340, loss = 0.68 (731.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:21:56.460363: step 76350, loss = 0.61 (731.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:21:58.202660: step 76360, loss = 0.70 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:21:59.937916: step 76370, loss = 0.73 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:22:01.681676: step 76380, loss = 0.62 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:22:03.405182: step 76390, loss = 0.69 (742.7 examples/sec; 0.172 sec/batch)
2017-03-26 00:22:05.280249: step 76400, loss = 0.71 (682.6 examples/sec; 0.188 sec/batch)
2017-03-26 00:22:06.904508: step 76410, loss = 0.67 (788.1 examples/sec; 0.162 sec/batch)
2017-03-26 00:22:08.631730: step 76420, loss = 0.72 (741.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:22:10.382117: step 76430, loss = 0.53 (731.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:22:12.115863: step 76440, loss = 0.78 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:22:13.866750: step 76450, loss = 0.91 (731.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:22:15.597953: step 76460, loss = 0.69 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:22:17.322754: step 76470, loss = 0.69 (742.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:22:19.061462: step 76480, loss = 0.66 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:22:20.789534: step 76490, loss = 0.65 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:22:22.770261: step 76500, loss = 0.65 (646.2 examples/sec; 0.198 sec/batch)
2017-03-26 00:22:24.259399: step 76510, loss = 0.88 (859.6 examples/sec; 0.149 sec/batch)
2017-03-26 00:22:25.956631: step 76520, loss = 0.54 (754.2 examples/sec; 0.170 sec/batch)
2017-03-26 00:22:27.688401: step 76530, loss = 0.68 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:22:29.420350: step 76540, loss = 0.78 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:22:31.143663: step 76550, loss = 0.76 (742.8 examples/sec; 0.172 sec/batch)
2017-03-26 00:22:32.887401: step 76560, loss = 0.64 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:22:34.609869: step 76570, loss = 0.74 (743.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:22:36.343096: step 76580, loss = 0.58 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:22:38.064630: step 76590, loss = 0.77 (743.5 examples/sec; 0.172 sec/batch)
2017-03-26 00:22:39.978004: step 76600, loss = 0.64 (669.3 examples/sec; 0.191 sec/batch)
2017-03-26 00:22:41.590752: step 76610, loss = 0.69 (793.2 examples/sec; 0.161 sec/batch)
2017-03-26 00:22:43.324852: step 76620, loss = 0.66 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:22:45.045194: step 76630, loss = 0.65 (744.0 examples/sec; 0.172 sec/batch)
2017-03-26 00:22:46.791737: step 76640, loss = 0.69 (732.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:22:48.515803: step 76650, loss = 0.66 (742.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:22:50.242693: step 76660, loss = 0.69 (741.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:22:51.984174: step 76670, loss = 0.69 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:22:53.702503: step 76680, loss = 0.67 (744.9 examples/sec; 0.172 sec/batch)
2017-03-26 00:22:55.433929: step 76690, loss = 0.62 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:22:57.315385: step 76700, loss = 0.81 (680.3 examples/sec; 0.188 sec/batch)
2017-03-26 00:22:58.941417: step 76710, loss = 0.59 (787.2 examples/sec; 0.163 sec/batch)
2017-03-26 00:23:00.659594: step 76720, loss = 0.84 (745.0 examples/sec; 0.172 sec/batch)
2017-03-26 00:23:02.396914: step 76730, loss = 0.70 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:23:04.131313: step 76740, loss = 0.66 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:23:05.864742: step 76750, loss = 0.84 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:23:07.610671: step 76760, loss = 0.75 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:23:09.348561: step 76770, loss = 0.77 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:23:11.081370: step 76780, loss = 0.75 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:23:12.815082: step 76790, loss = 0.82 (738.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:23:14.702806: step 76800, loss = 0.73 (678.1 examples/sec; 0.189 sec/batch)
2017-03-26 00:23:16.330241: step 76810, loss = 0.79 (786.8 examples/sec; 0.163 sec/batch)
2017-03-26 00:23:18.064888: step 76820, loss = 0.72 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:23:19.793187: step 76830, loss = 0.68 (740.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:23:21.531986: step 76840, loss = 0.78 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:23:23.274933: step 76850, loss = 0.82 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:23:25.011552: step 76860, loss = 0.65 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:23:26.741144: step 76870, loss = 0.70 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:23:28.454197: step 76880, loss = 0.70 (747.2 examples/sec; 0.171 sec/batch)
2017-03-26 00:23:30.185839: step 76890, loss = 0.67 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:23:32.073898: step 76900, loss = 0.71 (678.2 examples/sec; 0.189 sec/batch)
2017-03-26 00:23:33.700987: step 76910, loss = 0.66 (786.3 examples/sec; 0.163 sec/batch)
2017-03-26 00:23:35.430852: step 76920, loss = 0.64 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:23:37.176841: step 76930, loss = 0.88 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:23:38.900747: step 76940, loss = 0.80 (742.5 examples/sec; 0.172 sec/batch)
2017-03-26 00:23:40.628647: step 76950, loss = 0.77 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:23:42.357550: step 76960, loss = 0.77 (740.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:23:44.097436: step 76970, loss = 0.65 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:23:45.835119: step 76980, loss = 0.63 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:23:47.576945: step 76990, loss = 0.52 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:23:49.465932: step 77000, loss = 0.62 (678.1 examples/sec; 0.189 sec/batch)
2017-03-26 00:23:51.075426: step 77010, loss = 0.75 (794.7 examples/sec; 0.161 sec/batch)
2017-03-26 00:23:52.811256: step 77020, loss = 0.69 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:23:54.546763: step 77030, loss = 0.67 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:23:56.277475: step 77040, loss = 0.89 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:23:58.010506: step 77050, loss = 0.61 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:23:59.744917: step 77060, loss = 0.83 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:24:01.482223: step 77070, loss = 0.76 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:03.222718: step 77080, loss = 0.70 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:04.954557: step 77090, loss = 0.57 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:24:06.824183: step 77100, loss = 0.63 (684.6 examples/sec; 0.187 sec/batch)
2017-03-26 00:24:08.465604: step 77110, loss = 0.71 (779.8 examples/sec; 0.164 sec/batch)
2017-03-26 00:24:10.203408: step 77120, loss = 0.78 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:11.927380: step 77130, loss = 0.64 (742.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:24:13.670956: step 77140, loss = 0.58 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:15.397712: step 77150, loss = 0.70 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:24:17.131042: step 77160, loss = 0.71 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:24:18.864142: step 77170, loss = 0.76 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:24:20.600566: step 77180, loss = 0.67 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:22.343033: step 77190, loss = 0.77 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:24.221046: step 77200, loss = 0.71 (681.6 examples/sec; 0.188 sec/batch)
2017-03-26 00:24:25.843991: step 77210, loss = 0.73 (788.9 examples/sec; 0.162 sec/batch)
2017-03-26 00:24:27.583097: step 77220, loss = 0.79 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:29.324516: step 77230, loss = 0.56 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:31.063457: step 77240, loss = 0.69 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:32.792364: step 77250, loss = 0.61 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:24:34.519300: step 77260, loss = 0.77 (741.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:24:36.261061: step 77270, loss = 0.66 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:37.982358: step 77280, loss = 0.76 (743.6 examples/sec; 0.172 sec/batch)
2017-03-26 00:24:39.725128: step 77290, loss = 0.62 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:41.605314: step 77300, loss = 0.79 (680.8 examples/sec; 0.188 sec/batch)
2017-03-26 00:24:43.237580: step 77310, loss = 0.60 (784.2 examples/sec; 0.163 sec/batch)
2017-03-26 00:24:44.982481: step 77320, loss = 0.83 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:46.717733: step 77330, loss = 0.69 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:48.447802: step 77340, loss = 0.57 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:24:50.186572: step 77350, loss = 0.57 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:51.917391: step 77360, loss = 0.60 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:24:53.644910: step 77370, loss = 0.68 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:24:55.378379: step 77380, loss = 0.89 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:24:57.116439: step 77390, loss = 0.69 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:24:58.997152: step 77400, loss = 0.70 (680.6 examples/sec; 0.188 sec/batch)
2017-03-26 00:25:00.623540: step 77410, loss = 0.66 (787.0 examples/sec; 0.163 sec/batch)
2017-03-26 00:25:02.371857: step 77420, loss = 0.65 (732.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:25:04.099336: step 77430, loss = 0.79 (741.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:25:05.833116: step 77440, loss = 0.61 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:25:07.566147: step 77450, loss = 0.79 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:25:09.285411: step 77460, loss = 0.56 (744.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:25:11.024630: step 77470, loss = 0.77 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:25:12.763370: step 77480, loss = 0.83 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:25:14.507708: step 77490, loss = 0.74 (733.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:25:16.440944: step 77500, loss = 0.70 (662.1 examples/sec; 0.193 sec/batch)
2017-03-26 00:25:17.983879: step 77510, loss = 0.75 (829.6 examples/sec; 0.154 sec/batch)
2017-03-26 00:25:19.721807: step 77520, loss = 0.79 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:25:21.451741: step 77530, loss = 0.64 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:25:23.185787: step 77540, loss = 0.62 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:25:24.913551: step 77550, loss = 0.64 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:25:26.652156: step 77560, loss = 0.73 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:25:28.372978: step 77570, loss = 0.65 (743.8 examples/sec; 0.172 sec/batch)
2017-03-26 00:25:30.104771: step 77580, loss = 0.80 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:25:31.831284: step 77590, loss = 0.63 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:25:33.716133: step 77600, loss = 0.60 (679.1 examples/sec; 0.188 sec/batch)
2017-03-26 00:25:35.337553: step 77610, loss = 0.65 (789.4 examples/sec; 0.162 sec/batch)
2017-03-26 00:25:37.055842: step 77620, loss = 0.74 (744.9 examples/sec; 0.172 sec/batch)
2017-03-26 00:25:38.782068: step 77630, loss = 0.70 (741.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:25:40.510847: step 77640, loss = 0.72 (740.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:25:42.237577: step 77650, loss = 0.78 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:25:43.962450: step 77660, loss = 0.82 (742.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:25:45.696310: step 77670, loss = 0.75 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:25:47.442099: step 77680, loss = 0.88 (733.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:25:49.166397: step 77690, loss = 0.67 (742.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:25:51.056038: step 77700, loss = 0.97 (677.6 examples/sec; 0.189 sec/batch)
2017-03-26 00:25:52.677696: step 77710, loss = 0.67 (789.2 examples/sec; 0.162 sec/batch)
2017-03-26 00:25:54.400827: step 77720, loss = 0.65 (742.7 examples/sec; 0.172 sec/batch)
2017-03-26 00:25:56.135932: step 77730, loss = 0.66 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:25:57.861861: step 77740, loss = 0.76 (741.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:25:59.598657: step 77750, loss = 0.66 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:26:01.345778: step 77760, loss = 0.57 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:26:03.071747: step 77770, loss = 0.69 (741.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:26:04.804410: step 77780, loss = 0.63 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:26:06.548263: step 77790, loss = 0.65 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:26:08.445811: step 77800, loss = 0.82 (674.6 examples/sec; 0.190 sec/batch)
2017-03-26 00:26:10.034615: step 77810, loss = 0.59 (805.6 examples/sec; 0.159 sec/batch)
2017-03-26 00:26:11.784715: step 77820, loss = 0.69 (731.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:26:13.517060: step 77830, loss = 0.61 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:26:15.258410: step 77840, loss = 0.80 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:26:16.993535: step 77850, loss = 0.59 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:26:18.724888: step 77860, loss = 0.64 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:26:20.461633: step 77870, loss = 0.64 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:26:22.185418: step 77880, loss = 0.67 (742.7 examples/sec; 0.172 sec/batch)
2017-03-26 00:26:23.923898: step 77890, loss = 0.77 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:26:25.813002: step 77900, loss = 0.61 (678.0 examples/sec; 0.189 sec/batch)
2017-03-26 00:26:27.422493: step 77910, loss = 0.71 (794.6 examples/sec; 0.161 sec/batch)
2017-03-26 00:26:29.156565: step 77920, loss = 0.72 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:26:30.891047: step 77930, loss = 0.70 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:26:32.629328: step 77940, loss = 0.79 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:26:34.350311: step 77950, loss = 0.77 (743.8 examples/sec; 0.172 sec/batch)
2017-03-26 00:26:36.083180: step 77960, loss = 0.74 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:26:37.825998: step 77970, loss = 0.67 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:26:39.557690: step 77980, loss = 0.74 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:26:41.304930: step 77990, loss = 0.59 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:26:43.220218: step 78000, loss = 0.68 (668.3 examples/sec; 0.192 sec/batch)
2017-03-26 00:26:44.818273: step 78010, loss = 0.63 (801.0 examples/sec; 0.160 sec/batch)
2017-03-26 00:26:46.572534: step 78020, loss = 0.70 (729.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:26:48.317084: step 78030, loss = 0.68 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:26:50.052644: step 78040, loss = 0.73 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:26:51.781769: step 78050, loss = 0.63 (740.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:26:53.514616: step 78060, loss = 0.76 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:26:55.249630: step 78070, loss = 0.68 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:26:56.972899: step 78080, loss = 0.58 (742.8 examples/sec; 0.172 sec/batch)
2017-03-26 00:26:58.716372: step 78090, loss = 0.81 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:27:00.620726: step 78100, loss = 0.70 (672.8 examples/sec; 0.190 sec/batch)
2017-03-26 00:27:02.230183: step 78110, loss = 0.66 (794.4 examples/sec; 0.161 sec/batch)
2017-03-26 00:27:03.958367: step 78120, loss = 0.66 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:27:05.692589: step 78130, loss = 0.52 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:27:07.438843: step 78140, loss = 0.63 (732.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:27:09.175511: step 78150, loss = 0.66 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:27:10.914959: step 78160, loss = 0.60 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:27:12.653894: step 78170, loss = 1.00 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:27:14.397870: step 78180, loss = 0.68 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:27:16.130609: step 78190, loss = 0.76 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:27:18.017631: step 78200, loss = 0.75 (678.6 examples/sec; 0.189 sec/batch)
2017-03-26 00:27:19.629746: step 78210, loss = 0.59 (793.7 examples/sec; 0.161 sec/batch)
2017-03-26 00:27:21.370179: step 78220, loss = 0.75 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:27:23.092860: step 78230, loss = 0.79 (743.0 examples/sec; 0.172 sec/batch)
2017-03-26 00:27:24.826749: step 78240, loss = 0.74 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:27:26.559438: step 78250, loss = 0.65 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:27:28.293905: step 78260, loss = 0.78 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:27:30.029498: step 78270, loss = 0.74 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:27:31.765177: step 78280, loss = 0.64 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:27:33.499595: step 78290, loss = 0.75 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:27:35.475392: step 78300, loss = 0.83 (648.0 examples/sec; 0.198 sec/batch)
2017-03-26 00:27:36.985219: step 78310, loss = 0.75 (847.5 examples/sec; 0.151 sec/batch)
2017-03-26 00:27:38.687076: step 78320, loss = 0.65 (752.1 examples/sec; 0.170 sec/batch)
2017-03-26 00:27:40.421212: step 78330, loss = 0.60 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:27:42.145365: step 78340, loss = 0.61 (742.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:27:43.878691: step 78350, loss = 0.71 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:27:45.602633: step 78360, loss = 0.80 (742.7 examples/sec; 0.172 sec/batch)
2017-03-26 00:27:47.339830: step 78370, loss = 0.66 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:27:49.081464: step 78380, loss = 0.84 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:27:50.808052: step 78390, loss = 0.64 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:27:52.693697: step 78400, loss = 0.74 (678.8 examples/sec; 0.189 sec/batch)
2017-03-26 00:27:54.312570: step 78410, loss = 0.68 (790.7 examples/sec; 0.162 sec/batch)
2017-03-26 00:27:56.048808: step 78420, loss = 0.62 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:27:57.778068: step 78430, loss = 0.73 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:27:59.510520: step 78440, loss = 0.74 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:01.242731: step 78450, loss = 0.55 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:02.971898: step 78460, loss = 0.53 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:04.712437: step 78470, loss = 0.68 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:28:06.444198: step 78480, loss = 0.66 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:08.185468: step 78490, loss = 0.70 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:28:10.073509: step 78500, loss = 0.68 (678.2 examples/sec; 0.189 sec/batch)
2017-03-26 00:28:11.692215: step 78510, loss = 0.78 (790.4 examples/sec; 0.162 sec/batch)
2017-03-26 00:28:13.428546: step 78520, loss = 0.82 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:28:15.163160: step 78530, loss = 0.59 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:16.893886: step 78540, loss = 0.54 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:18.627420: step 78550, loss = 0.83 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:20.367790: step 78560, loss = 0.75 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:28:22.094959: step 78570, loss = 0.76 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:23.834463: step 78580, loss = 0.78 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:28:25.576482: step 78590, loss = 0.68 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:28:27.455326: step 78600, loss = 0.63 (681.2 examples/sec; 0.188 sec/batch)
2017-03-26 00:28:29.081856: step 78610, loss = 0.71 (787.0 examples/sec; 0.163 sec/batch)
2017-03-26 00:28:30.823672: step 78620, loss = 0.70 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:28:32.564390: step 78630, loss = 0.76 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:28:34.314336: step 78640, loss = 0.68 (731.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:28:36.035323: step 78650, loss = 0.71 (743.8 examples/sec; 0.172 sec/batch)
2017-03-26 00:28:37.767810: step 78660, loss = 0.84 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:39.509461: step 78670, loss = 0.73 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:28:41.235902: step 78680, loss = 0.75 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:42.970704: step 78690, loss = 0.83 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:44.844164: step 78700, loss = 0.75 (683.3 examples/sec; 0.187 sec/batch)
2017-03-26 00:28:46.451703: step 78710, loss = 0.70 (796.0 examples/sec; 0.161 sec/batch)
2017-03-26 00:28:48.186550: step 78720, loss = 0.64 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:49.912827: step 78730, loss = 0.57 (741.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:51.643679: step 78740, loss = 0.75 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:53.386781: step 78750, loss = 0.80 (734.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:28:55.118554: step 78760, loss = 0.81 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:56.851188: step 78770, loss = 0.78 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:28:58.588547: step 78780, loss = 0.87 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:29:00.313228: step 78790, loss = 0.72 (742.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:29:02.228803: step 78800, loss = 0.64 (668.5 examples/sec; 0.191 sec/batch)
2017-03-26 00:29:03.816386: step 78810, loss = 0.73 (805.9 examples/sec; 0.159 sec/batch)
2017-03-26 00:29:05.545859: step 78820, loss = 0.76 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:07.276130: step 78830, loss = 0.75 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:09.016526: step 78840, loss = 0.82 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:29:10.743619: step 78850, loss = 0.71 (741.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:12.475219: step 78860, loss = 0.69 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:14.200332: step 78870, loss = 0.72 (742.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:15.932558: step 78880, loss = 0.62 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:17.666574: step 78890, loss = 0.71 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:19.562453: step 78900, loss = 0.81 (675.2 examples/sec; 0.190 sec/batch)
2017-03-26 00:29:21.171539: step 78910, loss = 0.56 (795.5 examples/sec; 0.161 sec/batch)
2017-03-26 00:29:22.910307: step 78920, loss = 0.75 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:29:24.643195: step 78930, loss = 0.72 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:26.375730: step 78940, loss = 0.76 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:28.111829: step 78950, loss = 0.73 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:29:29.861013: step 78960, loss = 0.77 (731.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:29:31.597465: step 78970, loss = 0.74 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:29:33.326691: step 78980, loss = 0.66 (740.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:35.058486: step 78990, loss = 0.78 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:36.951724: step 79000, loss = 0.73 (676.1 examples/sec; 0.189 sec/batch)
2017-03-26 00:29:38.576213: step 79010, loss = 0.63 (787.9 examples/sec; 0.162 sec/batch)
2017-03-26 00:29:40.315553: step 79020, loss = 0.64 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:29:42.048114: step 79030, loss = 0.67 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:43.778335: step 79040, loss = 0.68 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:45.511034: step 79050, loss = 0.85 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:47.248790: step 79060, loss = 0.76 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:29:48.990425: step 79070, loss = 0.75 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:29:50.728829: step 79080, loss = 0.71 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:29:52.461322: step 79090, loss = 0.68 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:29:54.353852: step 79100, loss = 0.64 (676.3 examples/sec; 0.189 sec/batch)
2017-03-26 00:29:55.974324: step 79110, loss = 0.73 (789.9 examples/sec; 0.162 sec/batch)
2017-03-26 00:29:57.713830: step 79120, loss = 0.63 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:29:59.457107: step 79130, loss = 0.68 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:30:01.191425: step 79140, loss = 0.69 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:30:02.932799: step 79150, loss = 0.79 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:30:04.676590: step 79160, loss = 0.69 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:30:06.419657: step 79170, loss = 0.64 (734.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:30:08.169134: step 79180, loss = 0.62 (731.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:30:09.894109: step 79190, loss = 0.65 (742.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:30:11.794298: step 79200, loss = 0.67 (673.5 examples/sec; 0.190 sec/batch)
2017-03-26 00:30:13.402331: step 79210, loss = 0.74 (795.9 examples/sec; 0.161 sec/batch)
2017-03-26 00:30:15.153396: step 79220, loss = 0.66 (731.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:30:16.894024: step 79230, loss = 0.68 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:30:18.613534: step 79240, loss = 0.68 (744.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:30:20.354298: step 79250, loss = 0.66 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:30:22.100358: step 79260, loss = 0.76 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:30:23.826456: step 79270, loss = 0.65 (741.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:30:25.551951: step 79280, loss = 0.68 (742.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:30:27.290373: step 79290, loss = 0.68 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:30:29.186788: step 79300, loss = 0.68 (675.3 examples/sec; 0.190 sec/batch)
2017-03-26 00:30:30.811287: step 79310, loss = 0.70 (787.5 examples/sec; 0.163 sec/batch)
2017-03-26 00:30:32.536379: step 79320, loss = 0.75 (742.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:30:34.270872: step 79330, loss = 0.71 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:30:36.013844: step 79340, loss = 0.74 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:30:37.756439: step 79350, loss = 0.63 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:30:39.479946: step 79360, loss = 0.64 (742.7 examples/sec; 0.172 sec/batch)
2017-03-26 00:30:41.219446: step 79370, loss = 0.76 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:30:42.958891: step 79380, loss = 0.82 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:30:44.684241: step 79390, loss = 0.61 (741.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:30:46.682666: step 79400, loss = 0.72 (640.5 examples/sec; 0.200 sec/batch)
2017-03-26 00:30:48.211448: step 79410, loss = 0.67 (837.3 examples/sec; 0.153 sec/batch)
2017-03-26 00:30:49.949479: step 79420, loss = 0.74 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:30:51.677579: step 79430, loss = 0.67 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:30:53.409300: step 79440, loss = 0.58 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:30:55.160754: step 79450, loss = 0.72 (730.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:30:56.901955: step 79460, loss = 0.70 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:30:58.633325: step 79470, loss = 0.65 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:31:00.374421: step 79480, loss = 0.69 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:02.137313: step 79490, loss = 0.71 (726.1 examples/sec; 0.176 sec/batch)
2017-03-26 00:31:04.090464: step 79500, loss = 0.74 (655.4 examples/sec; 0.195 sec/batch)
2017-03-26 00:31:05.642592: step 79510, loss = 0.81 (824.7 examples/sec; 0.155 sec/batch)
2017-03-26 00:31:07.377613: step 79520, loss = 0.70 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:09.120355: step 79530, loss = 0.71 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:10.864786: step 79540, loss = 0.70 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:12.597462: step 79550, loss = 0.71 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:31:14.340124: step 79560, loss = 0.72 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:16.081368: step 79570, loss = 0.79 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:17.816192: step 79580, loss = 0.83 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:31:19.557323: step 79590, loss = 0.74 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:21.521956: step 79600, loss = 0.66 (651.5 examples/sec; 0.196 sec/batch)
2017-03-26 00:31:23.082224: step 79610, loss = 0.53 (820.4 examples/sec; 0.156 sec/batch)
2017-03-26 00:31:24.823812: step 79620, loss = 0.61 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:26.544540: step 79630, loss = 0.64 (743.9 examples/sec; 0.172 sec/batch)
2017-03-26 00:31:28.272552: step 79640, loss = 0.78 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:31:30.013385: step 79650, loss = 0.69 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:31.748527: step 79660, loss = 0.76 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:33.490181: step 79670, loss = 0.84 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:35.221904: step 79680, loss = 0.74 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:31:36.953328: step 79690, loss = 0.83 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:31:38.868131: step 79700, loss = 0.62 (668.7 examples/sec; 0.191 sec/batch)
2017-03-26 00:31:40.469932: step 79710, loss = 0.95 (798.7 examples/sec; 0.160 sec/batch)
2017-03-26 00:31:42.222584: step 79720, loss = 0.73 (730.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:31:43.961184: step 79730, loss = 0.74 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:45.701286: step 79740, loss = 0.70 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:47.432274: step 79750, loss = 0.76 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:31:49.170223: step 79760, loss = 0.60 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:50.911545: step 79770, loss = 0.70 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:52.640933: step 79780, loss = 0.76 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:31:54.379295: step 79790, loss = 0.77 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:31:56.279276: step 79800, loss = 0.80 (673.7 examples/sec; 0.190 sec/batch)
2017-03-26 00:31:57.890687: step 79810, loss = 0.72 (794.3 examples/sec; 0.161 sec/batch)
2017-03-26 00:31:59.626875: step 79820, loss = 0.79 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:32:01.369511: step 79830, loss = 0.59 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:32:03.109659: step 79840, loss = 0.76 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:32:04.851982: step 79850, loss = 0.66 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:32:06.586526: step 79860, loss = 0.63 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:32:08.318966: step 79870, loss = 0.66 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:32:10.049261: step 79880, loss = 0.66 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:32:11.779231: step 79890, loss = 0.74 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:32:13.699017: step 79900, loss = 0.70 (666.8 examples/sec; 0.192 sec/batch)
2017-03-26 00:32:15.270154: step 79910, loss = 0.61 (814.2 examples/sec; 0.157 sec/batch)
2017-03-26 00:32:17.018260: step 79920, loss = 0.80 (732.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:32:18.749686: step 79930, loss = 0.73 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:32:20.492866: step 79940, loss = 0.63 (734.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:32:22.216306: step 79950, loss = 0.70 (742.7 examples/sec; 0.172 sec/batch)
2017-03-26 00:32:23.942730: step 79960, loss = 0.77 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:32:25.672533: step 79970, loss = 0.65 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:32:27.401463: step 79980, loss = 0.75 (740.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:32:29.140379: step 79990, loss = 0.82 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:32:31.050319: step 80000, loss = 0.71 (670.2 examples/sec; 0.191 sec/batch)
2017-03-26 00:32:32.647173: step 80010, loss = 0.74 (801.6 examples/sec; 0.160 sec/batch)
2017-03-26 00:32:34.377957: step 80020, loss = 0.89 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:32:36.119996: step 80030, loss = 0.61 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:32:37.856773: step 80040, loss = 0.88 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:32:39.591404: step 80050, loss = 0.70 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:32:41.318784: step 80060, loss = 0.74 (741.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:32:43.060810: step 80070, loss = 0.58 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:32:44.801282: step 80080, loss = 0.65 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:32:46.538772: step 80090, loss = 0.63 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:32:48.495905: step 80100, loss = 0.74 (654.0 examples/sec; 0.196 sec/batch)
2017-03-26 00:32:50.029782: step 80110, loss = 0.73 (834.5 examples/sec; 0.153 sec/batch)
2017-03-26 00:32:51.749309: step 80120, loss = 0.67 (744.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:32:53.493275: step 80130, loss = 0.64 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:32:55.235611: step 80140, loss = 0.69 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:32:56.981085: step 80150, loss = 0.66 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:32:58.726154: step 80160, loss = 0.57 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:33:00.474334: step 80170, loss = 0.74 (732.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:33:02.226102: step 80180, loss = 0.69 (730.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:33:03.977457: step 80190, loss = 0.58 (730.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:33:05.891263: step 80200, loss = 0.81 (668.8 examples/sec; 0.191 sec/batch)
2017-03-26 00:33:07.515743: step 80210, loss = 0.75 (788.0 examples/sec; 0.162 sec/batch)
2017-03-26 00:33:09.254432: step 80220, loss = 0.74 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:33:10.998000: step 80230, loss = 0.62 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:33:12.757983: step 80240, loss = 0.68 (727.3 examples/sec; 0.176 sec/batch)
2017-03-26 00:33:14.493293: step 80250, loss = 0.69 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:33:16.235813: step 80260, loss = 0.72 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:33:17.970676: step 80270, loss = 0.70 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:33:19.703566: step 80280, loss = 0.60 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:33:21.436413: step 80290, loss = 0.73 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:33:23.338201: step 80300, loss = 0.64 (673.4 examples/sec; 0.190 sec/batch)
2017-03-26 00:33:24.952674: step 80310, loss = 0.67 (792.3 examples/sec; 0.162 sec/batch)
2017-03-26 00:33:26.686010: step 80320, loss = 0.84 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:33:28.441121: step 80330, loss = 0.82 (729.3 examples/sec; 0.176 sec/batch)
2017-03-26 00:33:30.184267: step 80340, loss = 0.73 (734.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:33:31.913997: step 80350, loss = 0.69 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:33:33.660741: step 80360, loss = 0.73 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:33:35.407845: step 80370, loss = 0.74 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:33:37.156724: step 80380, loss = 0.64 (731.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:33:38.903704: step 80390, loss = 0.75 (732.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:33:40.801482: step 80400, loss = 0.60 (674.8 examples/sec; 0.190 sec/batch)
2017-03-26 00:33:42.423704: step 80410, loss = 0.72 (788.9 examples/sec; 0.162 sec/batch)
2017-03-26 00:33:44.168509: step 80420, loss = 0.72 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:33:45.910527: step 80430, loss = 0.78 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:33:47.649091: step 80440, loss = 0.64 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:33:49.387680: step 80450, loss = 0.79 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:33:51.134978: step 80460, loss = 0.89 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:33:52.865432: step 80470, loss = 0.80 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:33:54.611106: step 80480, loss = 0.66 (733.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:33:56.368329: step 80490, loss = 0.67 (728.4 examples/sec; 0.176 sec/batch)
2017-03-26 00:33:58.276583: step 80500, loss = 0.83 (670.8 examples/sec; 0.191 sec/batch)
2017-03-26 00:33:59.889126: step 80510, loss = 0.68 (793.7 examples/sec; 0.161 sec/batch)
2017-03-26 00:34:01.626864: step 80520, loss = 0.68 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:34:03.387272: step 80530, loss = 0.73 (727.1 examples/sec; 0.176 sec/batch)
2017-03-26 00:34:05.150767: step 80540, loss = 0.77 (725.8 examples/sec; 0.176 sec/batch)
2017-03-26 00:34:06.889128: step 80550, loss = 0.64 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:34:08.636597: step 80560, loss = 0.72 (732.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:34:10.388408: step 80570, loss = 0.67 (730.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:34:12.134522: step 80580, loss = 0.74 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:34:13.874777: step 80590, loss = 0.73 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:34:15.785180: step 80600, loss = 0.60 (670.0 examples/sec; 0.191 sec/batch)
2017-03-26 00:34:17.419395: step 80610, loss = 0.63 (783.2 examples/sec; 0.163 sec/batch)
2017-03-26 00:34:19.164060: step 80620, loss = 0.65 (733.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:34:20.899986: step 80630, loss = 0.91 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:34:22.654429: step 80640, loss = 0.88 (729.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:34:24.393668: step 80650, loss = 0.77 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:34:26.135477: step 80660, loss = 0.71 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:34:27.865969: step 80670, loss = 0.56 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:34:29.611693: step 80680, loss = 0.57 (733.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:34:31.361526: step 80690, loss = 0.79 (731.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:34:33.233866: step 80700, loss = 0.70 (683.6 examples/sec; 0.187 sec/batch)
2017-03-26 00:34:34.861271: step 80710, loss = 0.63 (786.6 examples/sec; 0.163 sec/batch)
2017-03-26 00:34:36.590686: step 80720, loss = 0.76 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:34:38.334443: step 80730, loss = 0.83 (734.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:34:40.064608: step 80740, loss = 0.59 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:34:41.798989: step 80750, loss = 0.66 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:34:43.558264: step 80760, loss = 0.74 (727.6 examples/sec; 0.176 sec/batch)
2017-03-26 00:34:45.299199: step 80770, loss = 0.62 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:34:47.026393: step 80780, loss = 0.54 (741.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:34:48.768114: step 80790, loss = 0.76 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:34:50.668917: step 80800, loss = 0.83 (673.4 examples/sec; 0.190 sec/batch)
2017-03-26 00:34:52.283130: step 80810, loss = 0.77 (793.0 examples/sec; 0.161 sec/batch)
2017-03-26 00:34:54.025679: step 80820, loss = 0.65 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:34:55.781848: step 80830, loss = 0.73 (728.9 examples/sec; 0.176 sec/batch)
2017-03-26 00:34:57.516696: step 80840, loss = 0.80 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:34:59.260818: step 80850, loss = 0.66 (733.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:01.014405: step 80860, loss = 0.79 (729.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:35:02.756657: step 80870, loss = 0.72 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:04.499343: step 80880, loss = 0.57 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:06.232653: step 80890, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:35:08.143595: step 80900, loss = 0.60 (669.8 examples/sec; 0.191 sec/batch)
2017-03-26 00:35:09.763901: step 80910, loss = 0.75 (790.0 examples/sec; 0.162 sec/batch)
2017-03-26 00:35:11.503625: step 80920, loss = 0.84 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:13.246279: step 80930, loss = 0.84 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:14.987138: step 80940, loss = 0.80 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:16.725565: step 80950, loss = 0.86 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:18.453311: step 80960, loss = 0.71 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:35:20.192739: step 80970, loss = 0.65 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:21.941066: step 80980, loss = 0.75 (732.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:35:23.699679: step 80990, loss = 0.74 (727.8 examples/sec; 0.176 sec/batch)
2017-03-26 00:35:25.626230: step 81000, loss = 0.53 (664.4 examples/sec; 0.193 sec/batch)
2017-03-26 00:35:27.222622: step 81010, loss = 0.69 (801.8 examples/sec; 0.160 sec/batch)
2017-03-26 00:35:28.967029: step 81020, loss = 0.61 (733.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:30.720371: step 81030, loss = 0.80 (730.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:35:32.457592: step 81040, loss = 0.60 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:34.189497: step 81050, loss = 0.64 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:35:35.926032: step 81060, loss = 0.74 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:37.673443: step 81070, loss = 0.68 (732.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:35:39.418940: step 81080, loss = 0.74 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:35:41.161587: step 81090, loss = 0.69 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:43.060698: step 81100, loss = 0.81 (674.3 examples/sec; 0.190 sec/batch)
2017-03-26 00:35:44.672596: step 81110, loss = 0.79 (793.7 examples/sec; 0.161 sec/batch)
2017-03-26 00:35:46.410948: step 81120, loss = 0.79 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:48.149161: step 81130, loss = 0.77 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:49.889350: step 81140, loss = 0.76 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:51.628323: step 81150, loss = 0.82 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:53.373095: step 81160, loss = 0.84 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:55.121571: step 81170, loss = 0.70 (732.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:35:56.861682: step 81180, loss = 0.54 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:35:58.591900: step 81190, loss = 0.85 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:36:00.479560: step 81200, loss = 0.76 (678.1 examples/sec; 0.189 sec/batch)
2017-03-26 00:36:02.110938: step 81210, loss = 0.61 (784.6 examples/sec; 0.163 sec/batch)
2017-03-26 00:36:03.851623: step 81220, loss = 0.73 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:36:05.601736: step 81230, loss = 0.77 (731.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:36:07.344881: step 81240, loss = 0.67 (734.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:36:09.080978: step 81250, loss = 0.70 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:36:10.823474: step 81260, loss = 0.76 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:36:12.576819: step 81270, loss = 0.68 (730.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:36:14.316220: step 81280, loss = 0.76 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:36:16.064869: step 81290, loss = 0.73 (732.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:36:17.970356: step 81300, loss = 0.71 (671.7 examples/sec; 0.191 sec/batch)
2017-03-26 00:36:19.589114: step 81310, loss = 0.87 (790.7 examples/sec; 0.162 sec/batch)
2017-03-26 00:36:21.334448: step 81320, loss = 0.59 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:36:23.068648: step 81330, loss = 0.68 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:36:24.812958: step 81340, loss = 0.72 (733.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:36:26.566916: step 81350, loss = 0.75 (729.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:36:28.317457: step 81360, loss = 0.63 (731.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:36:30.064982: step 81370, loss = 0.75 (732.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:36:31.796613: step 81380, loss = 0.56 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:36:33.545942: step 81390, loss = 0.64 (731.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:36:35.455403: step 81400, loss = 0.71 (670.3 examples/sec; 0.191 sec/batch)
2017-03-26 00:36:37.073323: step 81410, loss = 0.64 (791.1 examples/sec; 0.162 sec/batch)
2017-03-26 00:36:38.811865: step 81420, loss = 0.79 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:36:40.542191: step 81430, loss = 0.83 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:36:42.289000: step 81440, loss = 0.63 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:36:44.028619: step 81450, loss = 0.83 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:36:45.757697: step 81460, loss = 0.75 (740.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:36:47.499561: step 81470, loss = 0.73 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:36:49.234258: step 81480, loss = 0.63 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:36:50.984890: step 81490, loss = 0.84 (731.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:36:52.925774: step 81500, loss = 0.60 (659.5 examples/sec; 0.194 sec/batch)
2017-03-26 00:36:54.463616: step 81510, loss = 0.69 (832.3 examples/sec; 0.154 sec/batch)
2017-03-26 00:36:56.198905: step 81520, loss = 0.78 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:36:57.932949: step 81530, loss = 0.57 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:36:59.688700: step 81540, loss = 0.71 (729.0 examples/sec; 0.176 sec/batch)
2017-03-26 00:37:01.433941: step 81550, loss = 0.67 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:03.177282: step 81560, loss = 0.76 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:04.927150: step 81570, loss = 0.69 (731.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:37:06.678080: step 81580, loss = 0.61 (731.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:37:08.424941: step 81590, loss = 0.69 (732.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:37:10.320939: step 81600, loss = 0.78 (675.1 examples/sec; 0.190 sec/batch)
2017-03-26 00:37:11.942236: step 81610, loss = 0.58 (789.5 examples/sec; 0.162 sec/batch)
2017-03-26 00:37:13.688194: step 81620, loss = 0.71 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:37:15.432761: step 81630, loss = 0.69 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:17.171158: step 81640, loss = 0.66 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:18.916485: step 81650, loss = 0.75 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:37:20.659129: step 81660, loss = 0.70 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:22.400196: step 81670, loss = 0.64 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:24.144850: step 81680, loss = 0.76 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:25.894588: step 81690, loss = 0.69 (731.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:37:27.799848: step 81700, loss = 0.63 (672.4 examples/sec; 0.190 sec/batch)
2017-03-26 00:37:29.428502: step 81710, loss = 0.67 (785.1 examples/sec; 0.163 sec/batch)
2017-03-26 00:37:31.185712: step 81720, loss = 0.64 (728.4 examples/sec; 0.176 sec/batch)
2017-03-26 00:37:32.923695: step 81730, loss = 0.70 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:34.661976: step 81740, loss = 0.68 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:36.406581: step 81750, loss = 0.77 (733.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:38.145907: step 81760, loss = 0.83 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:39.885882: step 81770, loss = 0.82 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:41.642800: step 81780, loss = 0.71 (728.5 examples/sec; 0.176 sec/batch)
2017-03-26 00:37:43.368394: step 81790, loss = 0.64 (741.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:37:45.259276: step 81800, loss = 0.73 (677.3 examples/sec; 0.189 sec/batch)
2017-03-26 00:37:46.883234: step 81810, loss = 0.74 (787.7 examples/sec; 0.162 sec/batch)
2017-03-26 00:37:48.626468: step 81820, loss = 0.65 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:50.364763: step 81830, loss = 0.52 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:52.103047: step 81840, loss = 0.72 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:37:53.849183: step 81850, loss = 0.66 (733.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:37:55.595358: step 81860, loss = 0.62 (732.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:37:57.329890: step 81870, loss = 0.63 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:37:59.070726: step 81880, loss = 0.62 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:38:00.811708: step 81890, loss = 0.65 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:38:02.714053: step 81900, loss = 0.68 (672.9 examples/sec; 0.190 sec/batch)
2017-03-26 00:38:04.348920: step 81910, loss = 0.81 (783.1 examples/sec; 0.163 sec/batch)
2017-03-26 00:38:06.090192: step 81920, loss = 0.56 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:38:07.836736: step 81930, loss = 0.75 (732.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:38:09.573196: step 81940, loss = 0.76 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:38:11.317145: step 81950, loss = 0.73 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:38:13.051797: step 81960, loss = 0.63 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:38:14.797271: step 81970, loss = 0.79 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:38:16.537869: step 81980, loss = 0.60 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:38:18.286560: step 81990, loss = 0.61 (732.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:38:20.191428: step 82000, loss = 0.83 (672.2 examples/sec; 0.190 sec/batch)
2017-03-26 00:38:21.813251: step 82010, loss = 0.66 (788.9 examples/sec; 0.162 sec/batch)
2017-03-26 00:38:23.553446: step 82020, loss = 0.69 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:38:25.304239: step 82030, loss = 0.63 (731.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:38:27.047053: step 82040, loss = 0.74 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:38:28.795843: step 82050, loss = 0.72 (731.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:38:30.523362: step 82060, loss = 0.59 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:38:32.269718: step 82070, loss = 0.53 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:38:34.005753: step 82080, loss = 0.72 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:38:35.736432: step 82090, loss = 0.64 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:38:37.708161: step 82100, loss = 0.74 (649.7 examples/sec; 0.197 sec/batch)
2017-03-26 00:38:39.259831: step 82110, loss = 0.65 (824.1 examples/sec; 0.155 sec/batch)
2017-03-26 00:38:41.013472: step 82120, loss = 0.74 (729.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:38:42.763228: step 82130, loss = 0.81 (731.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:38:44.505677: step 82140, loss = 0.73 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:38:46.244001: step 82150, loss = 0.65 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:38:47.985608: step 82160, loss = 0.63 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:38:49.717502: step 82170, loss = 0.85 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:38:51.449968: step 82180, loss = 0.74 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:38:53.190157: step 82190, loss = 0.65 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:38:55.116707: step 82200, loss = 0.76 (664.4 examples/sec; 0.193 sec/batch)
2017-03-26 00:38:56.724703: step 82210, loss = 0.63 (796.0 examples/sec; 0.161 sec/batch)
2017-03-26 00:38:58.463441: step 82220, loss = 0.67 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:39:00.194077: step 82230, loss = 0.71 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:39:01.943633: step 82240, loss = 0.74 (731.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:39:03.680518: step 82250, loss = 0.70 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:39:05.424542: step 82260, loss = 0.68 (733.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:39:07.171018: step 82270, loss = 0.70 (732.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:39:08.916477: step 82280, loss = 0.59 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:39:10.667116: step 82290, loss = 0.68 (731.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:39:12.631233: step 82300, loss = 0.65 (651.7 examples/sec; 0.196 sec/batch)
2017-03-26 00:39:14.189059: step 82310, loss = 0.67 (821.3 examples/sec; 0.156 sec/batch)
2017-03-26 00:39:15.930348: step 82320, loss = 0.56 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:39:17.675797: step 82330, loss = 0.75 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:39:19.427837: step 82340, loss = 0.75 (730.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:39:21.162048: step 82350, loss = 0.79 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:39:22.895135: step 82360, loss = 0.85 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:39:24.635845: step 82370, loss = 0.68 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:39:26.383753: step 82380, loss = 0.77 (732.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:39:28.119341: step 82390, loss = 0.64 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:39:30.012238: step 82400, loss = 0.81 (676.5 examples/sec; 0.189 sec/batch)
2017-03-26 00:39:31.625980: step 82410, loss = 0.74 (792.8 examples/sec; 0.161 sec/batch)
2017-03-26 00:39:33.372752: step 82420, loss = 0.66 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:39:35.103290: step 82430, loss = 0.74 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:39:36.839645: step 82440, loss = 0.90 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:39:38.575200: step 82450, loss = 0.78 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:39:40.309290: step 82460, loss = 0.66 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:39:42.062467: step 82470, loss = 0.72 (730.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:39:43.795906: step 82480, loss = 0.62 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:39:45.530447: step 82490, loss = 0.71 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:39:47.426056: step 82500, loss = 0.62 (675.2 examples/sec; 0.190 sec/batch)
2017-03-26 00:39:49.042883: step 82510, loss = 0.78 (791.7 examples/sec; 0.162 sec/batch)
2017-03-26 00:39:50.787291: step 82520, loss = 0.61 (733.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:39:52.520630: step 82530, loss = 0.67 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:39:54.257991: step 82540, loss = 0.61 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:39:56.002881: step 82550, loss = 0.70 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:39:57.733006: step 82560, loss = 0.60 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:39:59.478839: step 82570, loss = 0.56 (733.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:40:01.234287: step 82580, loss = 0.84 (729.2 examples/sec; 0.176 sec/batch)
2017-03-26 00:40:02.984925: step 82590, loss = 0.73 (731.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:40:04.904416: step 82600, loss = 0.64 (667.0 examples/sec; 0.192 sec/batch)
2017-03-26 00:40:06.526195: step 82610, loss = 0.69 (789.0 examples/sec; 0.162 sec/batch)
2017-03-26 00:40:08.276328: step 82620, loss = 0.70 (731.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:40:10.010951: step 82630, loss = 0.93 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:40:11.768922: step 82640, loss = 0.66 (728.1 examples/sec; 0.176 sec/batch)
2017-03-26 00:40:13.507610: step 82650, loss = 0.72 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:40:15.245517: step 82660, loss = 0.57 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:40:16.993407: step 82670, loss = 0.62 (732.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:40:18.727505: step 82680, loss = 0.70 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:40:20.468633: step 82690, loss = 0.87 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:40:22.371017: step 82700, loss = 0.59 (672.8 examples/sec; 0.190 sec/batch)
2017-03-26 00:40:23.986665: step 82710, loss = 0.84 (792.3 examples/sec; 0.162 sec/batch)
2017-03-26 00:40:25.744467: step 82720, loss = 0.89 (728.2 examples/sec; 0.176 sec/batch)
2017-03-26 00:40:27.500053: step 82730, loss = 0.72 (729.2 examples/sec; 0.176 sec/batch)
2017-03-26 00:40:29.231804: step 82740, loss = 0.74 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:40:30.973265: step 82750, loss = 0.69 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:40:32.713747: step 82760, loss = 0.65 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:40:34.452241: step 82770, loss = 0.70 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:40:36.191660: step 82780, loss = 0.72 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:40:37.937833: step 82790, loss = 0.79 (733.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:40:39.831196: step 82800, loss = 0.80 (676.0 examples/sec; 0.189 sec/batch)
2017-03-26 00:40:41.456357: step 82810, loss = 0.70 (787.6 examples/sec; 0.163 sec/batch)
2017-03-26 00:40:43.201045: step 82820, loss = 0.67 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:40:44.946127: step 82830, loss = 0.69 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:40:46.694861: step 82840, loss = 0.68 (732.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:40:48.438212: step 82850, loss = 0.66 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:40:50.175425: step 82860, loss = 0.79 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:40:51.913739: step 82870, loss = 0.80 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:40:53.660316: step 82880, loss = 0.76 (732.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:40:55.427549: step 82890, loss = 0.70 (724.3 examples/sec; 0.177 sec/batch)
2017-03-26 00:40:57.372510: step 82900, loss = 0.70 (658.1 examples/sec; 0.194 sec/batch)
2017-03-26 00:40:58.977733: step 82910, loss = 0.80 (797.4 examples/sec; 0.161 sec/batch)
2017-03-26 00:41:00.730880: step 82920, loss = 0.70 (730.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:41:02.471433: step 82930, loss = 0.74 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:41:04.211820: step 82940, loss = 0.74 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:41:05.955863: step 82950, loss = 0.61 (733.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:41:07.697411: step 82960, loss = 0.92 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:41:09.447877: step 82970, loss = 0.60 (731.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:41:11.193282: step 82980, loss = 0.69 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:41:12.950100: step 82990, loss = 0.85 (728.6 examples/sec; 0.176 sec/batch)
2017-03-26 00:41:14.860658: step 83000, loss = 0.74 (670.1 examples/sec; 0.191 sec/batch)
2017-03-26 00:41:16.465068: step 83010, loss = 0.80 (797.6 examples/sec; 0.160 sec/batch)
2017-03-26 00:41:18.196702: step 83020, loss = 0.81 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:41:19.943805: step 83030, loss = 0.69 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:41:21.695676: step 83040, loss = 0.84 (730.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:41:23.440854: step 83050, loss = 0.61 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:41:25.171807: step 83060, loss = 0.73 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:41:26.905340: step 83070, loss = 0.70 (738.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:41:28.660458: step 83080, loss = 0.71 (729.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:41:30.388364: step 83090, loss = 0.85 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:41:32.276120: step 83100, loss = 0.76 (678.4 examples/sec; 0.189 sec/batch)
2017-03-26 00:41:33.914639: step 83110, loss = 0.58 (780.7 examples/sec; 0.164 sec/batch)
2017-03-26 00:41:35.655871: step 83120, loss = 0.65 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:41:37.388211: step 83130, loss = 0.65 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:41:39.124251: step 83140, loss = 0.75 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:41:40.875149: step 83150, loss = 0.66 (731.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:41:42.601689: step 83160, loss = 0.90 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:41:44.333660: step 83170, loss = 0.75 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:41:46.074917: step 83180, loss = 0.74 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:41:47.812146: step 83190, loss = 0.73 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:41:49.780031: step 83200, loss = 0.81 (650.4 examples/sec; 0.197 sec/batch)
2017-03-26 00:41:51.313462: step 83210, loss = 0.70 (834.7 examples/sec; 0.153 sec/batch)
2017-03-26 00:41:53.053827: step 83220, loss = 0.69 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:41:54.798982: step 83230, loss = 0.62 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:41:56.536829: step 83240, loss = 0.84 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:41:58.269253: step 83250, loss = 0.58 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:41:59.992731: step 83260, loss = 0.65 (742.6 examples/sec; 0.172 sec/batch)
2017-03-26 00:42:01.742148: step 83270, loss = 0.73 (731.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:42:03.473336: step 83280, loss = 0.71 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:42:05.207699: step 83290, loss = 0.88 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:42:07.122531: step 83300, loss = 0.71 (668.5 examples/sec; 0.191 sec/batch)
2017-03-26 00:42:08.715385: step 83310, loss = 0.85 (803.6 examples/sec; 0.159 sec/batch)
2017-03-26 00:42:10.452742: step 83320, loss = 0.73 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:42:12.193137: step 83330, loss = 0.67 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:42:13.928354: step 83340, loss = 0.67 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:42:15.674623: step 83350, loss = 0.64 (733.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:42:17.433324: step 83360, loss = 0.79 (727.8 examples/sec; 0.176 sec/batch)
2017-03-26 00:42:19.173158: step 83370, loss = 0.66 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:42:20.914258: step 83380, loss = 0.70 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:42:22.642292: step 83390, loss = 0.83 (740.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:42:24.525589: step 83400, loss = 0.71 (679.7 examples/sec; 0.188 sec/batch)
2017-03-26 00:42:26.142865: step 83410, loss = 0.68 (791.6 examples/sec; 0.162 sec/batch)
2017-03-26 00:42:27.871797: step 83420, loss = 0.59 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:42:29.597454: step 83430, loss = 0.82 (741.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:42:31.343549: step 83440, loss = 0.79 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:42:33.076581: step 83450, loss = 0.90 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:42:34.810773: step 83460, loss = 0.80 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:42:36.549698: step 83470, loss = 0.84 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:42:38.287424: step 83480, loss = 0.73 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:42:40.034203: step 83490, loss = 0.76 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:42:41.935283: step 83500, loss = 0.73 (673.3 examples/sec; 0.190 sec/batch)
2017-03-26 00:42:43.541778: step 83510, loss = 0.80 (796.8 examples/sec; 0.161 sec/batch)
2017-03-26 00:42:45.284731: step 83520, loss = 0.76 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:42:47.024474: step 83530, loss = 0.72 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:42:48.780068: step 83540, loss = 0.55 (728.8 examples/sec; 0.176 sec/batch)
2017-03-26 00:42:50.517095: step 83550, loss = 0.60 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:42:52.258690: step 83560, loss = 0.62 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:42:54.005474: step 83570, loss = 0.62 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:42:55.750852: step 83580, loss = 0.68 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:42:57.486874: step 83590, loss = 0.56 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:42:59.374795: step 83600, loss = 0.78 (678.3 examples/sec; 0.189 sec/batch)
2017-03-26 00:43:01.014728: step 83610, loss = 0.58 (780.1 examples/sec; 0.164 sec/batch)
2017-03-26 00:43:02.760139: step 83620, loss = 0.62 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:43:04.510960: step 83630, loss = 0.92 (731.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:43:06.256965: step 83640, loss = 0.82 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:43:07.984915: step 83650, loss = 0.75 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:43:09.723497: step 83660, loss = 0.58 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:43:11.457944: step 83670, loss = 0.67 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:43:13.202583: step 83680, loss = 0.62 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:43:14.941389: step 83690, loss = 0.59 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:43:16.835184: step 83700, loss = 0.68 (675.8 examples/sec; 0.189 sec/batch)
2017-03-26 00:43:18.470778: step 83710, loss = 0.67 (782.6 examples/sec; 0.164 sec/batch)
2017-03-26 00:43:20.216076: step 83720, loss = 0.66 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:43:21.966519: step 83730, loss = 0.60 (731.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:43:23.692242: step 83740, loss = 0.70 (741.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:43:25.430187: step 83750, loss = 0.59 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:43:27.171573: step 83760, loss = 0.78 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:43:28.922841: step 83770, loss = 0.71 (730.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:43:30.659595: step 83780, loss = 0.82 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:43:32.412546: step 83790, loss = 0.62 (730.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:43:34.321429: step 83800, loss = 0.81 (670.8 examples/sec; 0.191 sec/batch)
2017-03-26 00:43:35.940891: step 83810, loss = 0.75 (790.0 examples/sec; 0.162 sec/batch)
2017-03-26 00:43:37.677091: step 83820, loss = 0.70 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:43:39.413480: step 83830, loss = 0.78 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:43:41.163962: step 83840, loss = 0.87 (731.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:43:42.914138: step 83850, loss = 0.81 (731.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:43:44.665533: step 83860, loss = 0.79 (730.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:43:46.421088: step 83870, loss = 0.78 (729.1 examples/sec; 0.176 sec/batch)
2017-03-26 00:43:48.169619: step 83880, loss = 0.67 (732.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:43:49.907022: step 83890, loss = 0.76 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:43:51.871539: step 83900, loss = 0.73 (651.6 examples/sec; 0.196 sec/batch)
2017-03-26 00:43:53.431420: step 83910, loss = 0.71 (820.6 examples/sec; 0.156 sec/batch)
2017-03-26 00:43:55.175001: step 83920, loss = 0.65 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:43:56.918538: step 83930, loss = 0.84 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:43:58.654714: step 83940, loss = 0.82 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:00.396921: step 83950, loss = 0.67 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:02.161148: step 83960, loss = 0.65 (725.5 examples/sec; 0.176 sec/batch)
2017-03-26 00:44:03.901000: step 83970, loss = 0.68 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:05.646819: step 83980, loss = 0.67 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:44:07.391292: step 83990, loss = 0.67 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:09.307100: step 84000, loss = 0.58 (668.2 examples/sec; 0.192 sec/batch)
2017-03-26 00:44:10.911487: step 84010, loss = 0.65 (797.5 examples/sec; 0.160 sec/batch)
2017-03-26 00:44:12.656201: step 84020, loss = 0.69 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:14.400873: step 84030, loss = 0.69 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:16.142250: step 84040, loss = 0.74 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:17.882080: step 84050, loss = 0.59 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:19.629565: step 84060, loss = 0.62 (732.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:44:21.379801: step 84070, loss = 0.74 (731.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:44:23.120354: step 84080, loss = 0.75 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:24.865537: step 84090, loss = 0.70 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:44:26.758337: step 84100, loss = 0.66 (676.2 examples/sec; 0.189 sec/batch)
2017-03-26 00:44:28.378510: step 84110, loss = 0.75 (790.1 examples/sec; 0.162 sec/batch)
2017-03-26 00:44:30.140126: step 84120, loss = 0.78 (726.5 examples/sec; 0.176 sec/batch)
2017-03-26 00:44:31.880250: step 84130, loss = 0.70 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:33.625936: step 84140, loss = 0.76 (733.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:44:35.367651: step 84150, loss = 0.58 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:37.112464: step 84160, loss = 0.60 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:38.850565: step 84170, loss = 0.78 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:40.593259: step 84180, loss = 0.55 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:42.323511: step 84190, loss = 0.60 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:44:44.244995: step 84200, loss = 0.65 (666.1 examples/sec; 0.192 sec/batch)
2017-03-26 00:44:45.859391: step 84210, loss = 0.68 (792.9 examples/sec; 0.161 sec/batch)
2017-03-26 00:44:47.601285: step 84220, loss = 0.89 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:49.358304: step 84230, loss = 0.68 (728.5 examples/sec; 0.176 sec/batch)
2017-03-26 00:44:51.104616: step 84240, loss = 0.64 (733.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:44:52.844989: step 84250, loss = 0.64 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:54.594161: step 84260, loss = 0.53 (731.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:44:56.334848: step 84270, loss = 0.69 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:44:58.092533: step 84280, loss = 0.58 (728.2 examples/sec; 0.176 sec/batch)
2017-03-26 00:44:59.823140: step 84290, loss = 0.68 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:45:01.728922: step 84300, loss = 0.74 (671.6 examples/sec; 0.191 sec/batch)
2017-03-26 00:45:03.361888: step 84310, loss = 0.66 (783.8 examples/sec; 0.163 sec/batch)
2017-03-26 00:45:05.098596: step 84320, loss = 0.76 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:45:06.839775: step 84330, loss = 0.93 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:45:08.571598: step 84340, loss = 0.57 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:45:10.303112: step 84350, loss = 0.82 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:45:12.039020: step 84360, loss = 0.68 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:45:13.771430: step 84370, loss = 0.83 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:45:15.510446: step 84380, loss = 0.60 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:45:17.245442: step 84390, loss = 0.67 (737.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:45:19.134280: step 84400, loss = 0.69 (677.7 examples/sec; 0.189 sec/batch)
2017-03-26 00:45:20.756923: step 84410, loss = 0.65 (788.8 examples/sec; 0.162 sec/batch)
2017-03-26 00:45:22.501305: step 84420, loss = 0.61 (733.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:45:24.242260: step 84430, loss = 0.61 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:45:25.977189: step 84440, loss = 0.75 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:45:27.707075: step 84450, loss = 0.69 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:45:29.447578: step 84460, loss = 0.71 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:45:31.186579: step 84470, loss = 0.70 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:45:32.924784: step 84480, loss = 0.80 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:45:34.657351: step 84490, loss = 0.67 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:45:36.568342: step 84500, loss = 0.67 (670.0 examples/sec; 0.191 sec/batch)
2017-03-26 00:45:38.188205: step 84510, loss = 0.83 (789.9 examples/sec; 0.162 sec/batch)
2017-03-26 00:45:39.919803: step 84520, loss = 0.74 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:45:41.653273: step 84530, loss = 0.72 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:45:43.381572: step 84540, loss = 0.78 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:45:45.126297: step 84550, loss = 0.63 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:45:46.862240: step 84560, loss = 0.63 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:45:48.600128: step 84570, loss = 0.73 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:45:50.334880: step 84580, loss = 0.71 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:45:52.076291: step 84590, loss = 0.73 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:45:53.983969: step 84600, loss = 0.63 (671.2 examples/sec; 0.191 sec/batch)
2017-03-26 00:45:55.591678: step 84610, loss = 0.67 (795.9 examples/sec; 0.161 sec/batch)
2017-03-26 00:45:57.314367: step 84620, loss = 0.74 (743.0 examples/sec; 0.172 sec/batch)
2017-03-26 00:45:59.053905: step 84630, loss = 0.64 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:46:00.791526: step 84640, loss = 0.71 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:46:02.538729: step 84650, loss = 0.63 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:46:04.260253: step 84660, loss = 0.67 (743.5 examples/sec; 0.172 sec/batch)
2017-03-26 00:46:05.997387: step 84670, loss = 0.68 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:46:07.737477: step 84680, loss = 0.64 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:46:09.462050: step 84690, loss = 0.58 (742.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:46:11.353434: step 84700, loss = 0.69 (676.7 examples/sec; 0.189 sec/batch)
2017-03-26 00:46:12.969076: step 84710, loss = 0.65 (792.3 examples/sec; 0.162 sec/batch)
2017-03-26 00:46:14.689550: step 84720, loss = 0.89 (744.0 examples/sec; 0.172 sec/batch)
2017-03-26 00:46:16.425669: step 84730, loss = 0.67 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:46:18.159822: step 84740, loss = 0.58 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:46:19.886246: step 84750, loss = 0.59 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:46:21.611829: step 84760, loss = 0.60 (741.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:46:23.354562: step 84770, loss = 0.64 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:46:25.093683: step 84780, loss = 0.85 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:46:26.827763: step 84790, loss = 0.75 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:46:28.701935: step 84800, loss = 0.78 (683.0 examples/sec; 0.187 sec/batch)
2017-03-26 00:46:30.319055: step 84810, loss = 0.66 (791.5 examples/sec; 0.162 sec/batch)
2017-03-26 00:46:32.057004: step 84820, loss = 0.64 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:46:33.799439: step 84830, loss = 0.76 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:46:35.522857: step 84840, loss = 0.67 (742.7 examples/sec; 0.172 sec/batch)
2017-03-26 00:46:37.261779: step 84850, loss = 0.68 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:46:39.002746: step 84860, loss = 0.79 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:46:40.717405: step 84870, loss = 0.57 (746.5 examples/sec; 0.171 sec/batch)
2017-03-26 00:46:42.461014: step 84880, loss = 0.70 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:46:44.186250: step 84890, loss = 0.72 (741.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:46:46.093421: step 84900, loss = 0.82 (671.3 examples/sec; 0.191 sec/batch)
2017-03-26 00:46:47.712065: step 84910, loss = 0.71 (790.5 examples/sec; 0.162 sec/batch)
2017-03-26 00:46:49.447496: step 84920, loss = 0.69 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:46:51.177364: step 84930, loss = 0.67 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:46:52.905320: step 84940, loss = 0.62 (740.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:46:54.627099: step 84950, loss = 0.65 (743.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:46:56.349888: step 84960, loss = 0.69 (743.0 examples/sec; 0.172 sec/batch)
2017-03-26 00:46:58.081488: step 84970, loss = 0.58 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:46:59.819565: step 84980, loss = 0.66 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:47:01.545491: step 84990, loss = 0.69 (741.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:47:03.454976: step 85000, loss = 0.72 (670.3 examples/sec; 0.191 sec/batch)
2017-03-26 00:47:05.069684: step 85010, loss = 0.52 (792.6 examples/sec; 0.161 sec/batch)
2017-03-26 00:47:06.808963: step 85020, loss = 0.73 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:47:08.536399: step 85030, loss = 0.64 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:47:10.266784: step 85040, loss = 0.64 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:47:12.008669: step 85050, loss = 0.83 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:47:13.732501: step 85060, loss = 0.70 (742.5 examples/sec; 0.172 sec/batch)
2017-03-26 00:47:15.467716: step 85070, loss = 0.81 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:47:17.191069: step 85080, loss = 0.81 (742.8 examples/sec; 0.172 sec/batch)
2017-03-26 00:47:18.928845: step 85090, loss = 0.68 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:47:20.826925: step 85100, loss = 0.80 (674.4 examples/sec; 0.190 sec/batch)
2017-03-26 00:47:22.421722: step 85110, loss = 0.71 (802.5 examples/sec; 0.160 sec/batch)
2017-03-26 00:47:24.164527: step 85120, loss = 0.73 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:47:25.896409: step 85130, loss = 0.82 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:47:27.627474: step 85140, loss = 0.71 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:47:29.360733: step 85150, loss = 0.69 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:47:31.101336: step 85160, loss = 0.67 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:47:32.839554: step 85170, loss = 0.66 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:47:34.572510: step 85180, loss = 0.63 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:47:36.311210: step 85190, loss = 0.79 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:47:38.195493: step 85200, loss = 0.65 (680.0 examples/sec; 0.188 sec/batch)
2017-03-26 00:47:39.813215: step 85210, loss = 0.71 (790.3 examples/sec; 0.162 sec/batch)
2017-03-26 00:47:41.552366: step 85220, loss = 0.66 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:47:43.280716: step 85230, loss = 0.66 (740.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:47:45.024912: step 85240, loss = 0.71 (733.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:47:46.758423: step 85250, loss = 0.79 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:47:48.485120: step 85260, loss = 0.57 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:47:50.223916: step 85270, loss = 0.66 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:47:51.945041: step 85280, loss = 0.68 (743.7 examples/sec; 0.172 sec/batch)
2017-03-26 00:47:53.692114: step 85290, loss = 0.72 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:47:55.589909: step 85300, loss = 0.80 (674.5 examples/sec; 0.190 sec/batch)
2017-03-26 00:47:57.202306: step 85310, loss = 0.69 (793.8 examples/sec; 0.161 sec/batch)
2017-03-26 00:47:58.949098: step 85320, loss = 0.71 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:48:00.678977: step 85330, loss = 0.55 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:48:02.417760: step 85340, loss = 0.67 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:48:04.161560: step 85350, loss = 0.75 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:48:05.883816: step 85360, loss = 0.70 (743.2 examples/sec; 0.172 sec/batch)
2017-03-26 00:48:07.618648: step 85370, loss = 0.77 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:48:09.349186: step 85380, loss = 0.71 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:48:11.086164: step 85390, loss = 0.71 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:48:13.037106: step 85400, loss = 0.70 (656.1 examples/sec; 0.195 sec/batch)
2017-03-26 00:48:14.583401: step 85410, loss = 0.58 (827.8 examples/sec; 0.155 sec/batch)
2017-03-26 00:48:16.319604: step 85420, loss = 0.63 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:48:18.049906: step 85430, loss = 0.70 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:48:19.781209: step 85440, loss = 0.70 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:48:21.518505: step 85450, loss = 0.64 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:48:23.272218: step 85460, loss = 0.64 (729.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:48:24.997289: step 85470, loss = 0.76 (742.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:48:26.728410: step 85480, loss = 0.78 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:48:28.466168: step 85490, loss = 0.71 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:48:30.357062: step 85500, loss = 0.71 (677.2 examples/sec; 0.189 sec/batch)
2017-03-26 00:48:31.979409: step 85510, loss = 0.63 (788.6 examples/sec; 0.162 sec/batch)
2017-03-26 00:48:33.731856: step 85520, loss = 0.74 (730.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:48:35.455534: step 85530, loss = 0.68 (742.6 examples/sec; 0.172 sec/batch)
2017-03-26 00:48:37.189101: step 85540, loss = 0.60 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:48:38.928571: step 85550, loss = 0.63 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:48:40.662699: step 85560, loss = 0.55 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:48:42.399864: step 85570, loss = 0.65 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:48:44.131386: step 85580, loss = 0.57 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:48:45.862086: step 85590, loss = 0.67 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:48:47.829729: step 85600, loss = 0.57 (650.5 examples/sec; 0.197 sec/batch)
2017-03-26 00:48:49.358695: step 85610, loss = 0.72 (837.2 examples/sec; 0.153 sec/batch)
2017-03-26 00:48:51.089497: step 85620, loss = 0.74 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 00:48:52.827825: step 85630, loss = 0.81 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:48:54.571509: step 85640, loss = 0.60 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:48:56.313258: step 85650, loss = 0.64 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:48:58.046264: step 85660, loss = 0.76 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:48:59.782481: step 85670, loss = 0.54 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:49:01.524681: step 85680, loss = 0.74 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:49:03.276301: step 85690, loss = 0.91 (730.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:49:05.160734: step 85700, loss = 0.60 (679.3 examples/sec; 0.188 sec/batch)
2017-03-26 00:49:06.796395: step 85710, loss = 0.65 (782.5 examples/sec; 0.164 sec/batch)
2017-03-26 00:49:08.533289: step 85720, loss = 0.67 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:49:10.271952: step 85730, loss = 0.82 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:49:11.999057: step 85740, loss = 0.74 (741.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:49:13.736140: step 85750, loss = 0.69 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:49:15.471264: step 85760, loss = 0.85 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:49:17.206854: step 85770, loss = 0.84 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:49:18.951601: step 85780, loss = 0.77 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:49:20.681634: step 85790, loss = 0.72 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:49:22.581332: step 85800, loss = 0.81 (673.8 examples/sec; 0.190 sec/batch)
2017-03-26 00:49:24.192853: step 85810, loss = 0.67 (794.3 examples/sec; 0.161 sec/batch)
2017-03-26 00:49:25.920806: step 85820, loss = 0.71 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:49:27.651611: step 85830, loss = 0.82 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:49:29.391370: step 85840, loss = 0.67 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:49:31.122232: step 85850, loss = 0.57 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:49:32.842346: step 85860, loss = 0.70 (744.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:49:34.566617: step 85870, loss = 0.63 (742.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:49:36.306680: step 85880, loss = 0.74 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:49:38.030520: step 85890, loss = 0.58 (742.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:49:39.944029: step 85900, loss = 0.67 (668.9 examples/sec; 0.191 sec/batch)
2017-03-26 00:49:41.540832: step 85910, loss = 0.73 (801.6 examples/sec; 0.160 sec/batch)
2017-03-26 00:49:43.273673: step 85920, loss = 0.76 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:49:45.008868: step 85930, loss = 0.73 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:49:46.748340: step 85940, loss = 0.70 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:49:48.480964: step 85950, loss = 0.62 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:49:50.218679: step 85960, loss = 0.78 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:49:51.973256: step 85970, loss = 0.80 (729.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:49:53.718772: step 85980, loss = 0.74 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:49:55.466135: step 85990, loss = 0.60 (732.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:49:57.376807: step 86000, loss = 0.93 (669.9 examples/sec; 0.191 sec/batch)
2017-03-26 00:49:59.008112: step 86010, loss = 0.70 (784.7 examples/sec; 0.163 sec/batch)
2017-03-26 00:50:00.754692: step 86020, loss = 0.68 (732.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:50:02.495907: step 86030, loss = 0.76 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:50:04.243293: step 86040, loss = 0.70 (732.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:50:05.980440: step 86050, loss = 0.65 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:50:07.722847: step 86060, loss = 0.68 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:50:09.464182: step 86070, loss = 0.83 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:50:11.207596: step 86080, loss = 0.86 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:50:12.941712: step 86090, loss = 0.54 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:50:14.842099: step 86100, loss = 0.56 (673.6 examples/sec; 0.190 sec/batch)
2017-03-26 00:50:16.461275: step 86110, loss = 0.89 (790.5 examples/sec; 0.162 sec/batch)
2017-03-26 00:50:18.192407: step 86120, loss = 0.77 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:50:19.922132: step 86130, loss = 0.83 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:50:21.660123: step 86140, loss = 0.74 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:50:23.411314: step 86150, loss = 0.70 (730.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:50:25.156404: step 86160, loss = 0.77 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:50:26.894103: step 86170, loss = 0.76 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:50:28.629826: step 86180, loss = 0.73 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:50:30.380991: step 86190, loss = 0.80 (730.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:50:32.321209: step 86200, loss = 0.72 (660.0 examples/sec; 0.194 sec/batch)
2017-03-26 00:50:33.888412: step 86210, loss = 0.69 (816.3 examples/sec; 0.157 sec/batch)
2017-03-26 00:50:35.637852: step 86220, loss = 0.72 (731.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:50:37.383011: step 86230, loss = 0.57 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:50:39.140349: step 86240, loss = 0.70 (728.4 examples/sec; 0.176 sec/batch)
2017-03-26 00:50:40.878052: step 86250, loss = 0.78 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:50:42.622012: step 86260, loss = 0.64 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:50:44.352244: step 86270, loss = 0.62 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:50:46.091603: step 86280, loss = 0.70 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:50:47.835413: step 86290, loss = 0.65 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:50:49.738067: step 86300, loss = 0.66 (672.7 examples/sec; 0.190 sec/batch)
2017-03-26 00:50:51.357928: step 86310, loss = 0.68 (790.2 examples/sec; 0.162 sec/batch)
2017-03-26 00:50:53.108383: step 86320, loss = 0.80 (731.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:50:54.893777: step 86330, loss = 0.76 (716.8 examples/sec; 0.179 sec/batch)
2017-03-26 00:50:56.636616: step 86340, loss = 0.63 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:50:58.378200: step 86350, loss = 0.69 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:51:00.124135: step 86360, loss = 0.96 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:51:01.872382: step 86370, loss = 0.71 (732.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:51:03.616930: step 86380, loss = 0.70 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:51:05.357325: step 86390, loss = 0.64 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:51:07.274631: step 86400, loss = 0.74 (667.8 examples/sec; 0.192 sec/batch)
2017-03-26 00:51:08.890214: step 86410, loss = 0.65 (791.9 examples/sec; 0.162 sec/batch)
2017-03-26 00:51:10.646849: step 86420, loss = 0.65 (728.7 examples/sec; 0.176 sec/batch)
2017-03-26 00:51:12.371410: step 86430, loss = 0.73 (742.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:51:14.108546: step 86440, loss = 0.69 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:51:15.859537: step 86450, loss = 0.59 (731.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:51:17.605358: step 86460, loss = 0.77 (733.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:51:19.337760: step 86470, loss = 0.68 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:51:21.081332: step 86480, loss = 0.59 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:51:22.830959: step 86490, loss = 0.67 (731.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:51:24.727878: step 86500, loss = 0.84 (674.8 examples/sec; 0.190 sec/batch)
2017-03-26 00:51:26.364761: step 86510, loss = 0.65 (782.0 examples/sec; 0.164 sec/batch)
2017-03-26 00:51:28.099315: step 86520, loss = 0.67 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:51:29.846060: step 86530, loss = 0.70 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:51:31.595166: step 86540, loss = 0.72 (731.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:51:33.334092: step 86550, loss = 0.64 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:51:35.086511: step 86560, loss = 0.59 (730.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:51:36.815700: step 86570, loss = 0.63 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:51:38.557114: step 86580, loss = 0.74 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:51:40.303526: step 86590, loss = 0.70 (733.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:51:42.196475: step 86600, loss = 0.64 (676.1 examples/sec; 0.189 sec/batch)
2017-03-26 00:51:43.841828: step 86610, loss = 0.62 (778.4 examples/sec; 0.164 sec/batch)
2017-03-26 00:51:45.585718: step 86620, loss = 0.79 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:51:47.325581: step 86630, loss = 0.82 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:51:49.074919: step 86640, loss = 0.63 (731.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:51:50.813840: step 86650, loss = 0.87 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:51:52.555846: step 86660, loss = 0.62 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:51:54.290693: step 86670, loss = 0.61 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:51:56.038072: step 86680, loss = 0.69 (732.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:51:57.773060: step 86690, loss = 0.81 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:51:59.752600: step 86700, loss = 0.90 (646.7 examples/sec; 0.198 sec/batch)
2017-03-26 00:52:01.272573: step 86710, loss = 0.73 (842.0 examples/sec; 0.152 sec/batch)
2017-03-26 00:52:03.008688: step 86720, loss = 0.58 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:52:04.771110: step 86730, loss = 0.63 (726.3 examples/sec; 0.176 sec/batch)
2017-03-26 00:52:06.501370: step 86740, loss = 0.62 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:52:08.231254: step 86750, loss = 0.74 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:52:09.969571: step 86760, loss = 0.60 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:52:11.707784: step 86770, loss = 0.74 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:52:13.447571: step 86780, loss = 0.60 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:52:15.197857: step 86790, loss = 0.78 (731.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:52:17.108477: step 86800, loss = 0.56 (669.9 examples/sec; 0.191 sec/batch)
2017-03-26 00:52:18.724052: step 86810, loss = 0.69 (792.3 examples/sec; 0.162 sec/batch)
2017-03-26 00:52:20.470861: step 86820, loss = 0.58 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:52:22.215038: step 86830, loss = 0.79 (733.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:52:23.960077: step 86840, loss = 0.72 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:52:25.699908: step 86850, loss = 0.76 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:52:27.444135: step 86860, loss = 0.81 (733.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:52:29.188654: step 86870, loss = 0.77 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:52:30.930618: step 86880, loss = 0.81 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:52:32.662285: step 86890, loss = 0.81 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:52:34.554992: step 86900, loss = 0.68 (676.6 examples/sec; 0.189 sec/batch)
2017-03-26 00:52:36.187983: step 86910, loss = 0.80 (783.4 examples/sec; 0.163 sec/batch)
2017-03-26 00:52:37.926543: step 86920, loss = 0.77 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:52:39.673383: step 86930, loss = 0.64 (732.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:52:41.420265: step 86940, loss = 0.73 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:52:43.167924: step 86950, loss = 0.60 (732.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:52:44.918874: step 86960, loss = 0.69 (731.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:52:46.656099: step 86970, loss = 0.80 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:52:48.398868: step 86980, loss = 0.78 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:52:50.123002: step 86990, loss = 0.74 (742.4 examples/sec; 0.172 sec/batch)
2017-03-26 00:52:52.053218: step 87000, loss = 0.71 (663.5 examples/sec; 0.193 sec/batch)
2017-03-26 00:52:53.658145: step 87010, loss = 0.69 (797.0 examples/sec; 0.161 sec/batch)
2017-03-26 00:52:55.407020: step 87020, loss = 0.83 (731.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:52:57.142535: step 87030, loss = 0.73 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:52:58.881263: step 87040, loss = 0.60 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:00.629122: step 87050, loss = 0.78 (732.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:53:02.366339: step 87060, loss = 0.82 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:04.112463: step 87070, loss = 0.70 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:53:05.874714: step 87080, loss = 0.64 (726.3 examples/sec; 0.176 sec/batch)
2017-03-26 00:53:07.609132: step 87090, loss = 0.70 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:53:09.511201: step 87100, loss = 0.58 (673.0 examples/sec; 0.190 sec/batch)
2017-03-26 00:53:11.116676: step 87110, loss = 0.67 (797.3 examples/sec; 0.161 sec/batch)
2017-03-26 00:53:12.862464: step 87120, loss = 0.64 (733.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:53:14.605295: step 87130, loss = 0.71 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:16.343648: step 87140, loss = 0.71 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:18.084006: step 87150, loss = 0.81 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:19.824296: step 87160, loss = 0.68 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:21.568469: step 87170, loss = 0.70 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:23.311652: step 87180, loss = 0.61 (734.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:25.049556: step 87190, loss = 0.76 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:26.943659: step 87200, loss = 0.69 (675.8 examples/sec; 0.189 sec/batch)
2017-03-26 00:53:28.569074: step 87210, loss = 0.71 (787.5 examples/sec; 0.163 sec/batch)
2017-03-26 00:53:30.310689: step 87220, loss = 0.66 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:32.057150: step 87230, loss = 0.83 (733.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:53:33.799683: step 87240, loss = 0.78 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:35.542641: step 87250, loss = 0.89 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:37.289251: step 87260, loss = 0.85 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:53:39.029377: step 87270, loss = 0.79 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:40.759217: step 87280, loss = 0.79 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:53:42.504084: step 87290, loss = 0.69 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:44.478217: step 87300, loss = 0.93 (648.6 examples/sec; 0.197 sec/batch)
2017-03-26 00:53:46.031915: step 87310, loss = 0.83 (823.5 examples/sec; 0.155 sec/batch)
2017-03-26 00:53:47.764116: step 87320, loss = 0.55 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:53:49.502425: step 87330, loss = 0.68 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:51.250646: step 87340, loss = 0.67 (732.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:53:52.973919: step 87350, loss = 0.89 (742.8 examples/sec; 0.172 sec/batch)
2017-03-26 00:53:54.714943: step 87360, loss = 0.71 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:56.451861: step 87370, loss = 0.71 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:53:58.183868: step 87380, loss = 0.67 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:53:59.932139: step 87390, loss = 0.72 (732.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:54:01.841571: step 87400, loss = 0.68 (670.4 examples/sec; 0.191 sec/batch)
2017-03-26 00:54:03.451878: step 87410, loss = 0.65 (794.9 examples/sec; 0.161 sec/batch)
2017-03-26 00:54:05.205526: step 87420, loss = 0.77 (729.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:54:06.952626: step 87430, loss = 0.58 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:54:08.697080: step 87440, loss = 0.69 (733.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:54:10.435935: step 87450, loss = 0.76 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:54:12.184879: step 87460, loss = 0.68 (731.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:54:13.909215: step 87470, loss = 0.67 (742.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:54:15.660332: step 87480, loss = 0.63 (731.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:54:17.392951: step 87490, loss = 0.74 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:54:19.301676: step 87500, loss = 0.75 (670.9 examples/sec; 0.191 sec/batch)
2017-03-26 00:54:20.927528: step 87510, loss = 0.79 (786.9 examples/sec; 0.163 sec/batch)
2017-03-26 00:54:22.685463: step 87520, loss = 0.94 (728.1 examples/sec; 0.176 sec/batch)
2017-03-26 00:54:24.421169: step 87530, loss = 0.73 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:54:26.169948: step 87540, loss = 0.57 (731.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:54:27.904772: step 87550, loss = 0.81 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:54:29.643969: step 87560, loss = 0.85 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:54:31.389246: step 87570, loss = 0.69 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:54:33.130559: step 87580, loss = 0.74 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:54:34.867283: step 87590, loss = 0.75 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:54:36.767049: step 87600, loss = 0.55 (673.8 examples/sec; 0.190 sec/batch)
2017-03-26 00:54:38.401253: step 87610, loss = 0.70 (783.2 examples/sec; 0.163 sec/batch)
2017-03-26 00:54:40.134645: step 87620, loss = 0.69 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:54:41.879152: step 87630, loss = 0.91 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:54:43.613173: step 87640, loss = 0.75 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:54:45.353600: step 87650, loss = 0.64 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:54:47.098414: step 87660, loss = 0.64 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:54:48.830745: step 87670, loss = 0.65 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:54:50.586077: step 87680, loss = 0.81 (729.1 examples/sec; 0.176 sec/batch)
2017-03-26 00:54:52.326153: step 87690, loss = 0.64 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:54:54.213566: step 87700, loss = 0.71 (678.2 examples/sec; 0.189 sec/batch)
2017-03-26 00:54:55.850145: step 87710, loss = 0.58 (782.1 examples/sec; 0.164 sec/batch)
2017-03-26 00:54:57.587449: step 87720, loss = 0.67 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:54:59.327335: step 87730, loss = 0.71 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:01.064553: step 87740, loss = 0.90 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:02.832722: step 87750, loss = 0.58 (723.9 examples/sec; 0.177 sec/batch)
2017-03-26 00:55:04.578937: step 87760, loss = 0.75 (733.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:55:06.319059: step 87770, loss = 0.67 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:08.062866: step 87780, loss = 0.67 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:09.801767: step 87790, loss = 0.70 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:11.726459: step 87800, loss = 0.69 (665.6 examples/sec; 0.192 sec/batch)
2017-03-26 00:55:13.321586: step 87810, loss = 0.67 (801.6 examples/sec; 0.160 sec/batch)
2017-03-26 00:55:15.058396: step 87820, loss = 0.80 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:16.798729: step 87830, loss = 0.72 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:18.541764: step 87840, loss = 0.74 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:20.279943: step 87850, loss = 0.82 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:22.016979: step 87860, loss = 0.65 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:23.771608: step 87870, loss = 0.72 (729.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:55:25.514760: step 87880, loss = 0.87 (734.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:27.259582: step 87890, loss = 0.66 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:29.170905: step 87900, loss = 0.91 (670.1 examples/sec; 0.191 sec/batch)
2017-03-26 00:55:30.796158: step 87910, loss = 0.76 (787.3 examples/sec; 0.163 sec/batch)
2017-03-26 00:55:32.539723: step 87920, loss = 0.66 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:34.284719: step 87930, loss = 0.68 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:55:36.019373: step 87940, loss = 0.59 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:55:37.753867: step 87950, loss = 0.63 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:55:39.501587: step 87960, loss = 0.55 (732.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:55:41.251583: step 87970, loss = 0.60 (731.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:55:42.989659: step 87980, loss = 0.65 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:44.730350: step 87990, loss = 0.69 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:46.646337: step 88000, loss = 0.66 (668.1 examples/sec; 0.192 sec/batch)
2017-03-26 00:55:48.249633: step 88010, loss = 0.78 (798.4 examples/sec; 0.160 sec/batch)
2017-03-26 00:55:49.992010: step 88020, loss = 0.73 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:51.738714: step 88030, loss = 0.76 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:55:53.474985: step 88040, loss = 0.84 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:55.229937: step 88050, loss = 0.86 (729.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:55:56.966298: step 88060, loss = 0.67 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:55:58.705709: step 88070, loss = 0.68 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:56:00.454952: step 88080, loss = 0.64 (731.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:56:02.201004: step 88090, loss = 0.75 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:56:04.123203: step 88100, loss = 0.69 (666.2 examples/sec; 0.192 sec/batch)
2017-03-26 00:56:05.727369: step 88110, loss = 0.70 (797.6 examples/sec; 0.160 sec/batch)
2017-03-26 00:56:07.474851: step 88120, loss = 0.76 (732.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:56:09.219638: step 88130, loss = 0.91 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:56:10.962123: step 88140, loss = 0.67 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:56:12.720169: step 88150, loss = 0.64 (727.9 examples/sec; 0.176 sec/batch)
2017-03-26 00:56:14.459950: step 88160, loss = 0.67 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:56:16.197475: step 88170, loss = 0.66 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:56:17.929809: step 88180, loss = 0.89 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:56:19.681988: step 88190, loss = 0.72 (730.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:56:21.579775: step 88200, loss = 0.67 (674.7 examples/sec; 0.190 sec/batch)
2017-03-26 00:56:23.204900: step 88210, loss = 0.81 (787.3 examples/sec; 0.163 sec/batch)
2017-03-26 00:56:24.945699: step 88220, loss = 0.62 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:56:26.687898: step 88230, loss = 0.55 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:56:28.432252: step 88240, loss = 0.66 (733.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:56:30.177185: step 88250, loss = 0.65 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:56:31.918852: step 88260, loss = 0.70 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:56:33.659709: step 88270, loss = 0.83 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:56:35.410693: step 88280, loss = 0.68 (731.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:56:37.144635: step 88290, loss = 0.89 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:56:39.041040: step 88300, loss = 0.76 (674.8 examples/sec; 0.190 sec/batch)
2017-03-26 00:56:40.660954: step 88310, loss = 0.63 (790.2 examples/sec; 0.162 sec/batch)
2017-03-26 00:56:42.403163: step 88320, loss = 0.75 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:56:44.149674: step 88330, loss = 0.84 (732.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:56:45.882976: step 88340, loss = 0.83 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:56:47.628206: step 88350, loss = 0.75 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:56:49.373963: step 88360, loss = 0.61 (733.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:56:51.122381: step 88370, loss = 0.68 (732.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:56:52.847904: step 88380, loss = 0.73 (741.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:56:54.586223: step 88390, loss = 0.55 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:56:56.490582: step 88400, loss = 0.72 (672.5 examples/sec; 0.190 sec/batch)
2017-03-26 00:56:58.124209: step 88410, loss = 0.69 (783.0 examples/sec; 0.163 sec/batch)
2017-03-26 00:56:59.872183: step 88420, loss = 0.70 (732.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:57:01.618066: step 88430, loss = 0.72 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:57:03.360640: step 88440, loss = 0.57 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:57:05.097785: step 88450, loss = 0.73 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:57:06.838986: step 88460, loss = 0.84 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:57:08.580696: step 88470, loss = 0.69 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 00:57:10.327827: step 88480, loss = 0.71 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:57:12.075776: step 88490, loss = 0.54 (732.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:57:13.969341: step 88500, loss = 0.71 (676.0 examples/sec; 0.189 sec/batch)
2017-03-26 00:57:15.593914: step 88510, loss = 0.73 (787.9 examples/sec; 0.162 sec/batch)
2017-03-26 00:57:17.343388: step 88520, loss = 0.57 (731.6 examples/sec; 0.175 sec/batch)
2017-03-26 00:57:19.091073: step 88530, loss = 0.77 (732.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:57:20.835903: step 88540, loss = 0.71 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:57:22.585345: step 88550, loss = 0.67 (731.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:57:24.330569: step 88560, loss = 0.82 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:57:26.065336: step 88570, loss = 0.67 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:57:27.803264: step 88580, loss = 0.67 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:57:29.551757: step 88590, loss = 0.62 (732.1 examples/sec; 0.175 sec/batch)
2017-03-26 00:57:31.454632: step 88600, loss = 0.80 (672.7 examples/sec; 0.190 sec/batch)
2017-03-26 00:57:33.072563: step 88610, loss = 0.68 (791.4 examples/sec; 0.162 sec/batch)
2017-03-26 00:57:34.814384: step 88620, loss = 0.79 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:57:36.560574: step 88630, loss = 0.56 (733.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:57:38.310340: step 88640, loss = 0.79 (731.5 examples/sec; 0.175 sec/batch)
2017-03-26 00:57:40.036869: step 88650, loss = 0.66 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 00:57:41.780181: step 88660, loss = 0.61 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:57:43.517971: step 88670, loss = 0.85 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:57:45.262660: step 88680, loss = 0.57 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:57:47.003305: step 88690, loss = 0.78 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:57:48.960155: step 88700, loss = 0.54 (654.1 examples/sec; 0.196 sec/batch)
2017-03-26 00:57:50.548161: step 88710, loss = 0.75 (805.7 examples/sec; 0.159 sec/batch)
2017-03-26 00:57:52.279441: step 88720, loss = 0.58 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 00:57:54.020080: step 88730, loss = 0.71 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:57:55.757303: step 88740, loss = 0.64 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:57:57.492537: step 88750, loss = 0.79 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:57:59.226025: step 88760, loss = 0.67 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:58:00.969503: step 88770, loss = 0.67 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:58:02.711811: step 88780, loss = 0.65 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:58:04.453305: step 88790, loss = 0.66 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:58:06.376814: step 88800, loss = 0.68 (665.7 examples/sec; 0.192 sec/batch)
2017-03-26 00:58:07.990469: step 88810, loss = 0.67 (792.7 examples/sec; 0.161 sec/batch)
2017-03-26 00:58:09.742202: step 88820, loss = 0.69 (730.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:58:11.484083: step 88830, loss = 0.72 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:58:13.227269: step 88840, loss = 0.77 (734.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:58:14.972172: step 88850, loss = 0.70 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:58:16.722626: step 88860, loss = 0.70 (731.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:58:18.472099: step 88870, loss = 0.74 (731.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:58:20.211304: step 88880, loss = 0.66 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:58:21.959240: step 88890, loss = 0.73 (732.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:58:23.861674: step 88900, loss = 0.89 (673.0 examples/sec; 0.190 sec/batch)
2017-03-26 00:58:25.486579: step 88910, loss = 0.68 (787.4 examples/sec; 0.163 sec/batch)
2017-03-26 00:58:27.226180: step 88920, loss = 0.81 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:58:28.975277: step 88930, loss = 0.62 (731.8 examples/sec; 0.175 sec/batch)
2017-03-26 00:58:30.713791: step 88940, loss = 0.68 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 00:58:32.447961: step 88950, loss = 0.73 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:58:34.182008: step 88960, loss = 0.65 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:58:35.918802: step 88970, loss = 0.67 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:58:37.658907: step 88980, loss = 0.72 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 00:58:39.408957: step 88990, loss = 0.68 (731.2 examples/sec; 0.175 sec/batch)
2017-03-26 00:58:41.337797: step 89000, loss = 0.63 (663.8 examples/sec; 0.193 sec/batch)
2017-03-26 00:58:42.917322: step 89010, loss = 0.55 (810.1 examples/sec; 0.158 sec/batch)
2017-03-26 00:58:44.662550: step 89020, loss = 0.80 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:58:46.386994: step 89030, loss = 0.67 (742.3 examples/sec; 0.172 sec/batch)
2017-03-26 00:58:48.124398: step 89040, loss = 0.67 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:58:49.869584: step 89050, loss = 0.60 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:58:51.609452: step 89060, loss = 0.86 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 00:58:53.350069: step 89070, loss = 0.85 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 00:58:55.082155: step 89080, loss = 0.71 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:58:56.814636: step 89090, loss = 0.64 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 00:58:58.722082: step 89100, loss = 0.78 (671.4 examples/sec; 0.191 sec/batch)
2017-03-26 00:59:00.330968: step 89110, loss = 0.67 (795.1 examples/sec; 0.161 sec/batch)
2017-03-26 00:59:02.069048: step 89120, loss = 0.65 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 00:59:03.800717: step 89130, loss = 0.68 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 00:59:05.544641: step 89140, loss = 0.57 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:59:07.290013: step 89150, loss = 0.88 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 00:59:09.043592: step 89160, loss = 0.82 (729.9 examples/sec; 0.175 sec/batch)
2017-03-26 00:59:10.768514: step 89170, loss = 0.61 (742.1 examples/sec; 0.172 sec/batch)
2017-03-26 00:59:12.502978: step 89180, loss = 0.69 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:59:14.237530: step 89190, loss = 0.62 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:59:16.203270: step 89200, loss = 0.74 (651.5 examples/sec; 0.196 sec/batch)
2017-03-26 00:59:17.711055: step 89210, loss = 0.66 (848.4 examples/sec; 0.151 sec/batch)
2017-03-26 00:59:19.410391: step 89220, loss = 0.59 (753.2 examples/sec; 0.170 sec/batch)
2017-03-26 00:59:21.146932: step 89230, loss = 0.74 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:59:22.893959: step 89240, loss = 0.79 (732.7 examples/sec; 0.175 sec/batch)
2017-03-26 00:59:24.626073: step 89250, loss = 0.66 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 00:59:26.370502: step 89260, loss = 0.69 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:59:28.102887: step 89270, loss = 0.61 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 00:59:29.839580: step 89280, loss = 0.54 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:59:31.584914: step 89290, loss = 0.72 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 00:59:33.474416: step 89300, loss = 0.72 (677.4 examples/sec; 0.189 sec/batch)
2017-03-26 00:59:35.100911: step 89310, loss = 0.59 (787.1 examples/sec; 0.163 sec/batch)
2017-03-26 00:59:36.843239: step 89320, loss = 0.67 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 00:59:38.584515: step 89330, loss = 0.86 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:59:40.315046: step 89340, loss = 0.64 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 00:59:42.049527: step 89350, loss = 0.70 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 00:59:43.792931: step 89360, loss = 0.73 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:59:45.546251: step 89370, loss = 0.61 (730.0 examples/sec; 0.175 sec/batch)
2017-03-26 00:59:47.289634: step 89380, loss = 0.75 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 00:59:49.031111: step 89390, loss = 0.67 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 00:59:50.938914: step 89400, loss = 0.57 (670.9 examples/sec; 0.191 sec/batch)
2017-03-26 00:59:52.564873: step 89410, loss = 0.64 (787.3 examples/sec; 0.163 sec/batch)
2017-03-26 00:59:54.320674: step 89420, loss = 0.60 (728.9 examples/sec; 0.176 sec/batch)
2017-03-26 00:59:56.052466: step 89430, loss = 0.60 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 00:59:57.793820: step 89440, loss = 0.59 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 00:59:59.540543: step 89450, loss = 0.62 (732.7 examples/sec; 0.175 sec/batch)
2017-03-26 01:00:01.281666: step 89460, loss = 0.75 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:03.042151: step 89470, loss = 0.78 (727.1 examples/sec; 0.176 sec/batch)
2017-03-26 01:00:04.794068: step 89480, loss = 0.75 (730.6 examples/sec; 0.175 sec/batch)
2017-03-26 01:00:06.530156: step 89490, loss = 0.53 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:08.424311: step 89500, loss = 0.84 (675.9 examples/sec; 0.189 sec/batch)
2017-03-26 01:00:10.066070: step 89510, loss = 0.77 (779.5 examples/sec; 0.164 sec/batch)
2017-03-26 01:00:11.799624: step 89520, loss = 0.72 (738.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:00:13.542106: step 89530, loss = 0.74 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:15.287085: step 89540, loss = 0.67 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:17.031657: step 89550, loss = 0.68 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:18.774154: step 89560, loss = 0.76 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:20.496664: step 89570, loss = 0.69 (743.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:00:22.239401: step 89580, loss = 0.61 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:23.982456: step 89590, loss = 0.69 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:25.963914: step 89600, loss = 0.64 (646.0 examples/sec; 0.198 sec/batch)
2017-03-26 01:00:27.514573: step 89610, loss = 0.66 (825.6 examples/sec; 0.155 sec/batch)
2017-03-26 01:00:29.252887: step 89620, loss = 0.70 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:30.992533: step 89630, loss = 0.80 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:32.745578: step 89640, loss = 0.74 (730.2 examples/sec; 0.175 sec/batch)
2017-03-26 01:00:34.476098: step 89650, loss = 0.70 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:00:36.218218: step 89660, loss = 0.74 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:37.960051: step 89670, loss = 0.65 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:39.709301: step 89680, loss = 0.81 (731.7 examples/sec; 0.175 sec/batch)
2017-03-26 01:00:41.441820: step 89690, loss = 0.82 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:00:43.330467: step 89700, loss = 0.67 (677.7 examples/sec; 0.189 sec/batch)
2017-03-26 01:00:44.967415: step 89710, loss = 0.89 (781.9 examples/sec; 0.164 sec/batch)
2017-03-26 01:00:46.704739: step 89720, loss = 0.70 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:48.442759: step 89730, loss = 0.78 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:50.191530: step 89740, loss = 0.88 (731.9 examples/sec; 0.175 sec/batch)
2017-03-26 01:00:51.930381: step 89750, loss = 0.59 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:53.667066: step 89760, loss = 0.65 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:00:55.435304: step 89770, loss = 0.73 (723.9 examples/sec; 0.177 sec/batch)
2017-03-26 01:00:57.182023: step 89780, loss = 0.64 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 01:00:58.931674: step 89790, loss = 0.62 (731.6 examples/sec; 0.175 sec/batch)
2017-03-26 01:01:00.836200: step 89800, loss = 0.68 (673.0 examples/sec; 0.190 sec/batch)
2017-03-26 01:01:02.455467: step 89810, loss = 0.64 (789.3 examples/sec; 0.162 sec/batch)
2017-03-26 01:01:04.202382: step 89820, loss = 0.83 (732.7 examples/sec; 0.175 sec/batch)
2017-03-26 01:01:05.942798: step 89830, loss = 0.67 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:01:07.681254: step 89840, loss = 0.72 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:01:09.424250: step 89850, loss = 0.85 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:01:11.157705: step 89860, loss = 0.64 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:01:12.906894: step 89870, loss = 0.80 (731.8 examples/sec; 0.175 sec/batch)
2017-03-26 01:01:14.639812: step 89880, loss = 0.69 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:01:16.397338: step 89890, loss = 0.90 (728.2 examples/sec; 0.176 sec/batch)
2017-03-26 01:01:18.298084: step 89900, loss = 0.74 (673.7 examples/sec; 0.190 sec/batch)
2017-03-26 01:01:19.913942: step 89910, loss = 0.63 (791.8 examples/sec; 0.162 sec/batch)
2017-03-26 01:01:21.657247: step 89920, loss = 0.82 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:01:23.403137: step 89930, loss = 0.73 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 01:01:25.141774: step 89940, loss = 0.69 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:01:26.888428: step 89950, loss = 0.64 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 01:01:28.634888: step 89960, loss = 0.62 (732.9 examples/sec; 0.175 sec/batch)
2017-03-26 01:01:30.376179: step 89970, loss = 0.61 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:01:32.115612: step 89980, loss = 0.70 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:01:33.865681: step 89990, loss = 0.69 (731.8 examples/sec; 0.175 sec/batch)
2017-03-26 01:01:35.766655: step 90000, loss = 0.69 (673.0 examples/sec; 0.190 sec/batch)
2017-03-26 01:01:37.373799: step 90010, loss = 0.59 (796.4 examples/sec; 0.161 sec/batch)
2017-03-26 01:01:39.119105: step 90020, loss = 0.67 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 01:01:40.859760: step 90030, loss = 0.67 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:01:42.611260: step 90040, loss = 0.67 (730.8 examples/sec; 0.175 sec/batch)
2017-03-26 01:01:44.349473: step 90050, loss = 0.74 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:01:46.094943: step 90060, loss = 0.81 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 01:01:47.830585: step 90070, loss = 0.73 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:01:49.576379: step 90080, loss = 0.62 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 01:01:51.310526: step 90090, loss = 0.68 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:01:53.198543: step 90100, loss = 0.74 (678.0 examples/sec; 0.189 sec/batch)
2017-03-26 01:01:54.831612: step 90110, loss = 0.51 (783.8 examples/sec; 0.163 sec/batch)
2017-03-26 01:01:56.572011: step 90120, loss = 0.66 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:01:58.315939: step 90130, loss = 0.62 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:02:00.056785: step 90140, loss = 0.52 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:02:01.806424: step 90150, loss = 0.65 (731.6 examples/sec; 0.175 sec/batch)
2017-03-26 01:02:03.533035: step 90160, loss = 0.58 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:05.280569: step 90170, loss = 0.86 (732.5 examples/sec; 0.175 sec/batch)
2017-03-26 01:02:07.013091: step 90180, loss = 0.58 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:08.752854: step 90190, loss = 0.76 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:02:10.649289: step 90200, loss = 0.75 (675.0 examples/sec; 0.190 sec/batch)
2017-03-26 01:02:12.267526: step 90210, loss = 0.63 (791.0 examples/sec; 0.162 sec/batch)
2017-03-26 01:02:14.008583: step 90220, loss = 0.61 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:02:15.737794: step 90230, loss = 0.79 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:17.471887: step 90240, loss = 0.93 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:19.188164: step 90250, loss = 0.75 (745.9 examples/sec; 0.172 sec/batch)
2017-03-26 01:02:20.930315: step 90260, loss = 0.68 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:02:22.661062: step 90270, loss = 0.69 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:24.387720: step 90280, loss = 0.64 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:26.118007: step 90290, loss = 0.78 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:28.005795: step 90300, loss = 0.63 (678.0 examples/sec; 0.189 sec/batch)
2017-03-26 01:02:29.621465: step 90310, loss = 0.67 (792.2 examples/sec; 0.162 sec/batch)
2017-03-26 01:02:31.347813: step 90320, loss = 0.66 (741.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:33.079020: step 90330, loss = 0.84 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:34.815361: step 90340, loss = 0.68 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:02:36.550819: step 90350, loss = 0.69 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:02:38.281189: step 90360, loss = 0.57 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:40.020020: step 90370, loss = 0.79 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:02:41.764028: step 90380, loss = 0.65 (733.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:02:43.490081: step 90390, loss = 0.84 (741.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:45.376322: step 90400, loss = 0.61 (678.6 examples/sec; 0.189 sec/batch)
2017-03-26 01:02:46.981722: step 90410, loss = 0.90 (797.3 examples/sec; 0.161 sec/batch)
2017-03-26 01:02:48.708715: step 90420, loss = 0.62 (741.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:50.451724: step 90430, loss = 0.73 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:02:52.182689: step 90440, loss = 0.84 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:53.925713: step 90450, loss = 0.61 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:02:55.658439: step 90460, loss = 0.70 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:57.389730: step 90470, loss = 0.64 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:02:59.124331: step 90480, loss = 0.64 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:03:00.854407: step 90490, loss = 0.72 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:03:02.755610: step 90500, loss = 0.57 (673.3 examples/sec; 0.190 sec/batch)
2017-03-26 01:03:04.374714: step 90510, loss = 0.86 (790.6 examples/sec; 0.162 sec/batch)
2017-03-26 01:03:06.117532: step 90520, loss = 0.64 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:03:07.849830: step 90530, loss = 0.68 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:03:09.574333: step 90540, loss = 0.76 (742.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:03:11.304811: step 90550, loss = 0.68 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:03:13.047364: step 90560, loss = 0.60 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:03:14.781367: step 90570, loss = 0.53 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:03:16.503978: step 90580, loss = 0.65 (743.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:03:18.246301: step 90590, loss = 0.87 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:03:20.111003: step 90600, loss = 0.81 (686.4 examples/sec; 0.186 sec/batch)
2017-03-26 01:03:21.744028: step 90610, loss = 0.71 (783.8 examples/sec; 0.163 sec/batch)
2017-03-26 01:03:23.486306: step 90620, loss = 0.84 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:03:25.222968: step 90630, loss = 0.73 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:03:26.940949: step 90640, loss = 1.12 (745.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:03:28.675658: step 90650, loss = 0.61 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:03:30.417933: step 90660, loss = 0.60 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:03:32.147973: step 90670, loss = 0.82 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:03:33.883840: step 90680, loss = 0.66 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:03:35.614836: step 90690, loss = 0.58 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:03:37.494616: step 90700, loss = 0.89 (681.3 examples/sec; 0.188 sec/batch)
2017-03-26 01:03:39.124716: step 90710, loss = 0.56 (784.7 examples/sec; 0.163 sec/batch)
2017-03-26 01:03:40.866196: step 90720, loss = 0.74 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:03:42.602635: step 90730, loss = 0.51 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:03:44.334209: step 90740, loss = 0.77 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:03:46.065204: step 90750, loss = 0.74 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:03:47.792815: step 90760, loss = 0.69 (740.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:03:49.528210: step 90770, loss = 0.55 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:03:51.273042: step 90780, loss = 0.73 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:03:52.995787: step 90790, loss = 0.68 (743.0 examples/sec; 0.172 sec/batch)
2017-03-26 01:03:54.939688: step 90800, loss = 0.63 (658.9 examples/sec; 0.194 sec/batch)
2017-03-26 01:03:56.461788: step 90810, loss = 0.73 (840.2 examples/sec; 0.152 sec/batch)
2017-03-26 01:03:58.174313: step 90820, loss = 0.68 (747.4 examples/sec; 0.171 sec/batch)
2017-03-26 01:03:59.904473: step 90830, loss = 0.66 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:04:01.646708: step 90840, loss = 0.80 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:04:03.384799: step 90850, loss = 0.85 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:04:05.119541: step 90860, loss = 0.67 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:04:06.843304: step 90870, loss = 0.76 (742.6 examples/sec; 0.172 sec/batch)
2017-03-26 01:04:08.563902: step 90880, loss = 0.61 (743.9 examples/sec; 0.172 sec/batch)
2017-03-26 01:04:10.299139: step 90890, loss = 0.70 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:04:12.200546: step 90900, loss = 0.63 (673.1 examples/sec; 0.190 sec/batch)
2017-03-26 01:04:13.810451: step 90910, loss = 0.63 (795.1 examples/sec; 0.161 sec/batch)
2017-03-26 01:04:15.546095: step 90920, loss = 0.75 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:04:17.285418: step 90930, loss = 0.71 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:04:19.010975: step 90940, loss = 0.72 (741.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:04:20.740210: step 90950, loss = 0.65 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:04:22.465112: step 90960, loss = 0.79 (742.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:04:24.198125: step 90970, loss = 0.72 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:04:25.926845: step 90980, loss = 0.62 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:04:27.659801: step 90990, loss = 0.77 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:04:29.542930: step 91000, loss = 0.58 (679.9 examples/sec; 0.188 sec/batch)
2017-03-26 01:04:31.162845: step 91010, loss = 0.64 (789.9 examples/sec; 0.162 sec/batch)
2017-03-26 01:04:32.898279: step 91020, loss = 0.74 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:04:34.612651: step 91030, loss = 0.56 (746.6 examples/sec; 0.171 sec/batch)
2017-03-26 01:04:36.357234: step 91040, loss = 0.84 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:04:38.098338: step 91050, loss = 0.66 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:04:39.832728: step 91060, loss = 0.61 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:04:41.559109: step 91070, loss = 0.63 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:04:43.285448: step 91080, loss = 0.61 (741.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:04:45.007382: step 91090, loss = 0.59 (743.4 examples/sec; 0.172 sec/batch)
2017-03-26 01:04:46.903259: step 91100, loss = 0.57 (675.4 examples/sec; 0.190 sec/batch)
2017-03-26 01:04:48.507050: step 91110, loss = 0.76 (797.7 examples/sec; 0.160 sec/batch)
2017-03-26 01:04:50.241785: step 91120, loss = 0.73 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:04:51.976076: step 91130, loss = 0.65 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:04:53.699117: step 91140, loss = 0.52 (742.9 examples/sec; 0.172 sec/batch)
2017-03-26 01:04:55.422322: step 91150, loss = 0.75 (742.8 examples/sec; 0.172 sec/batch)
2017-03-26 01:04:57.155815: step 91160, loss = 0.74 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:04:58.894755: step 91170, loss = 0.77 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:05:00.628802: step 91180, loss = 0.71 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:05:02.374802: step 91190, loss = 0.87 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 01:05:04.264953: step 91200, loss = 0.62 (677.2 examples/sec; 0.189 sec/batch)
2017-03-26 01:05:05.894980: step 91210, loss = 0.68 (785.3 examples/sec; 0.163 sec/batch)
2017-03-26 01:05:07.628321: step 91220, loss = 0.66 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:05:09.374115: step 91230, loss = 0.66 (733.2 examples/sec; 0.175 sec/batch)
2017-03-26 01:05:11.110087: step 91240, loss = 0.71 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:05:12.832471: step 91250, loss = 0.88 (743.2 examples/sec; 0.172 sec/batch)
2017-03-26 01:05:14.563948: step 91260, loss = 0.74 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:05:16.280896: step 91270, loss = 0.67 (745.5 examples/sec; 0.172 sec/batch)
2017-03-26 01:05:18.016277: step 91280, loss = 0.72 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:05:19.734517: step 91290, loss = 0.64 (745.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:05:21.647795: step 91300, loss = 0.75 (669.2 examples/sec; 0.191 sec/batch)
2017-03-26 01:05:23.257210: step 91310, loss = 0.67 (794.8 examples/sec; 0.161 sec/batch)
2017-03-26 01:05:24.983236: step 91320, loss = 0.83 (741.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:05:26.718837: step 91330, loss = 0.74 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:05:28.449748: step 91340, loss = 0.74 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:05:30.183955: step 91350, loss = 0.67 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:05:31.914997: step 91360, loss = 0.71 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:05:33.664492: step 91370, loss = 0.65 (731.6 examples/sec; 0.175 sec/batch)
2017-03-26 01:05:35.379850: step 91380, loss = 0.67 (746.2 examples/sec; 0.172 sec/batch)
2017-03-26 01:05:37.111333: step 91390, loss = 0.68 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:05:39.105386: step 91400, loss = 0.70 (642.1 examples/sec; 0.199 sec/batch)
2017-03-26 01:05:40.596724: step 91410, loss = 0.62 (857.9 examples/sec; 0.149 sec/batch)
2017-03-26 01:05:42.318098: step 91420, loss = 0.67 (743.6 examples/sec; 0.172 sec/batch)
2017-03-26 01:05:44.052634: step 91430, loss = 0.70 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:05:45.788571: step 91440, loss = 0.71 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:05:47.527507: step 91450, loss = 0.66 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:05:49.260200: step 91460, loss = 0.59 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:05:51.004893: step 91470, loss = 0.68 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:05:52.728658: step 91480, loss = 0.62 (742.6 examples/sec; 0.172 sec/batch)
2017-03-26 01:05:54.470381: step 91490, loss = 0.68 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:05:56.379899: step 91500, loss = 0.65 (670.3 examples/sec; 0.191 sec/batch)
2017-03-26 01:05:57.969564: step 91510, loss = 0.72 (805.2 examples/sec; 0.159 sec/batch)
2017-03-26 01:05:59.716325: step 91520, loss = 0.81 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 01:06:01.456699: step 91530, loss = 0.84 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:06:03.189746: step 91540, loss = 0.79 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:06:04.923135: step 91550, loss = 0.72 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:06:06.660467: step 91560, loss = 0.72 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:06:08.410652: step 91570, loss = 0.68 (731.3 examples/sec; 0.175 sec/batch)
2017-03-26 01:06:10.141318: step 91580, loss = 0.68 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:06:11.883121: step 91590, loss = 0.74 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:06:13.786814: step 91600, loss = 0.63 (672.4 examples/sec; 0.190 sec/batch)
2017-03-26 01:06:15.381552: step 91610, loss = 0.63 (802.6 examples/sec; 0.159 sec/batch)
2017-03-26 01:06:17.108019: step 91620, loss = 0.66 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:06:18.853259: step 91630, loss = 0.66 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 01:06:20.586146: step 91640, loss = 0.70 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:06:22.326083: step 91650, loss = 0.86 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:06:24.067499: step 91660, loss = 0.72 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:06:25.798780: step 91670, loss = 0.62 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:06:27.542367: step 91680, loss = 0.61 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:06:29.269971: step 91690, loss = 0.66 (740.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:06:31.161403: step 91700, loss = 0.62 (676.7 examples/sec; 0.189 sec/batch)
2017-03-26 01:06:32.785456: step 91710, loss = 0.87 (788.2 examples/sec; 0.162 sec/batch)
2017-03-26 01:06:34.514136: step 91720, loss = 0.66 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:06:36.263870: step 91730, loss = 0.64 (731.5 examples/sec; 0.175 sec/batch)
2017-03-26 01:06:37.995255: step 91740, loss = 0.88 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:06:39.731865: step 91750, loss = 0.75 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:06:41.468723: step 91760, loss = 0.67 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:06:43.211201: step 91770, loss = 0.80 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:06:44.940413: step 91780, loss = 0.65 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:06:46.683443: step 91790, loss = 0.61 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:06:48.577771: step 91800, loss = 0.90 (675.6 examples/sec; 0.189 sec/batch)
2017-03-26 01:06:50.197703: step 91810, loss = 0.52 (790.1 examples/sec; 0.162 sec/batch)
2017-03-26 01:06:51.935302: step 91820, loss = 0.60 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:06:53.658854: step 91830, loss = 0.70 (742.6 examples/sec; 0.172 sec/batch)
2017-03-26 01:06:55.403942: step 91840, loss = 0.55 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 01:06:57.141529: step 91850, loss = 0.72 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:06:58.868620: step 91860, loss = 0.70 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:07:00.611727: step 91870, loss = 0.68 (734.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:07:02.350435: step 91880, loss = 0.77 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:07:04.085898: step 91890, loss = 0.53 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:07:05.988932: step 91900, loss = 0.69 (672.6 examples/sec; 0.190 sec/batch)
2017-03-26 01:07:07.596252: step 91910, loss = 0.65 (796.4 examples/sec; 0.161 sec/batch)
2017-03-26 01:07:09.320286: step 91920, loss = 0.75 (742.4 examples/sec; 0.172 sec/batch)
2017-03-26 01:07:11.062269: step 91930, loss = 0.81 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:07:12.794786: step 91940, loss = 0.80 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:07:14.532337: step 91950, loss = 0.73 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:07:16.257674: step 91960, loss = 0.79 (741.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:07:17.980139: step 91970, loss = 0.81 (743.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:07:19.707252: step 91980, loss = 0.57 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:07:21.447502: step 91990, loss = 0.83 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:07:23.332046: step 92000, loss = 0.68 (679.2 examples/sec; 0.188 sec/batch)
2017-03-26 01:07:24.953034: step 92010, loss = 0.68 (789.6 examples/sec; 0.162 sec/batch)
2017-03-26 01:07:26.690654: step 92020, loss = 0.73 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:07:28.427902: step 92030, loss = 0.62 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:07:30.160372: step 92040, loss = 0.75 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:07:31.906357: step 92050, loss = 0.77 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 01:07:33.633733: step 92060, loss = 0.67 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:07:35.368797: step 92070, loss = 0.75 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:07:37.095285: step 92080, loss = 0.81 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:07:38.832334: step 92090, loss = 0.71 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:07:40.724563: step 92100, loss = 0.78 (676.4 examples/sec; 0.189 sec/batch)
2017-03-26 01:07:42.330804: step 92110, loss = 0.66 (796.9 examples/sec; 0.161 sec/batch)
2017-03-26 01:07:44.075892: step 92120, loss = 0.67 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 01:07:45.807593: step 92130, loss = 0.77 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:07:47.546872: step 92140, loss = 0.59 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:07:49.281695: step 92150, loss = 0.59 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:07:51.030245: step 92160, loss = 0.66 (732.0 examples/sec; 0.175 sec/batch)
2017-03-26 01:07:52.771356: step 92170, loss = 0.67 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:07:54.513542: step 92180, loss = 0.84 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:07:56.247459: step 92190, loss = 0.77 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:07:58.143884: step 92200, loss = 0.63 (675.2 examples/sec; 0.190 sec/batch)
2017-03-26 01:07:59.758549: step 92210, loss = 0.68 (792.4 examples/sec; 0.162 sec/batch)
2017-03-26 01:08:01.495229: step 92220, loss = 0.69 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:08:03.235451: step 92230, loss = 0.72 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:08:04.970349: step 92240, loss = 0.73 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:08:06.695947: step 92250, loss = 0.58 (741.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:08:08.430753: step 92260, loss = 0.67 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:08:10.171751: step 92270, loss = 0.93 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:08:11.916802: step 92280, loss = 0.68 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 01:08:13.634922: step 92290, loss = 0.82 (745.0 examples/sec; 0.172 sec/batch)
2017-03-26 01:08:15.510842: step 92300, loss = 0.71 (682.5 examples/sec; 0.188 sec/batch)
2017-03-26 01:08:17.161788: step 92310, loss = 0.68 (775.1 examples/sec; 0.165 sec/batch)
2017-03-26 01:08:18.894424: step 92320, loss = 0.68 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:08:20.620648: step 92330, loss = 0.74 (741.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:08:22.362677: step 92340, loss = 0.75 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:08:24.104320: step 92350, loss = 0.72 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:08:25.832326: step 92360, loss = 0.82 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:08:27.574556: step 92370, loss = 0.89 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:08:29.310649: step 92380, loss = 0.60 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:08:31.038694: step 92390, loss = 0.85 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:08:32.920108: step 92400, loss = 0.89 (680.3 examples/sec; 0.188 sec/batch)
2017-03-26 01:08:34.555823: step 92410, loss = 0.62 (782.5 examples/sec; 0.164 sec/batch)
2017-03-26 01:08:36.291540: step 92420, loss = 0.68 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:08:38.024558: step 92430, loss = 0.67 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:08:39.767143: step 92440, loss = 0.58 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:08:41.502139: step 92450, loss = 0.89 (737.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:08:43.238707: step 92460, loss = 0.68 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:08:44.969827: step 92470, loss = 0.69 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:08:46.702844: step 92480, loss = 0.71 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:08:48.435821: step 92490, loss = 0.62 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:08:50.325036: step 92500, loss = 0.68 (678.0 examples/sec; 0.189 sec/batch)
2017-03-26 01:08:51.942228: step 92510, loss = 0.75 (790.9 examples/sec; 0.162 sec/batch)
2017-03-26 01:08:53.664646: step 92520, loss = 0.70 (743.0 examples/sec; 0.172 sec/batch)
2017-03-26 01:08:55.394343: step 92530, loss = 0.72 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:08:57.139263: step 92540, loss = 0.72 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:08:58.871040: step 92550, loss = 0.64 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:09:00.609143: step 92560, loss = 0.69 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:09:02.355240: step 92570, loss = 0.61 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 01:09:04.090172: step 92580, loss = 0.67 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:09:05.814247: step 92590, loss = 0.71 (742.4 examples/sec; 0.172 sec/batch)
2017-03-26 01:09:07.703604: step 92600, loss = 0.69 (677.7 examples/sec; 0.189 sec/batch)
2017-03-26 01:09:09.317535: step 92610, loss = 0.70 (792.8 examples/sec; 0.161 sec/batch)
2017-03-26 01:09:11.061375: step 92620, loss = 0.73 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:09:12.800219: step 92630, loss = 0.72 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:09:14.524867: step 92640, loss = 0.68 (742.2 examples/sec; 0.172 sec/batch)
2017-03-26 01:09:16.259758: step 92650, loss = 0.69 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:09:18.000806: step 92660, loss = 0.65 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:09:19.725343: step 92670, loss = 0.58 (742.2 examples/sec; 0.172 sec/batch)
2017-03-26 01:09:21.458614: step 92680, loss = 0.77 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:09:23.184577: step 92690, loss = 0.57 (741.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:09:25.146167: step 92700, loss = 0.84 (652.8 examples/sec; 0.196 sec/batch)
2017-03-26 01:09:26.698699: step 92710, loss = 0.71 (824.0 examples/sec; 0.155 sec/batch)
2017-03-26 01:09:28.432706: step 92720, loss = 0.94 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:09:30.159874: step 92730, loss = 0.63 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:09:31.892621: step 92740, loss = 0.61 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:09:33.628680: step 92750, loss = 0.62 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:09:35.369458: step 92760, loss = 0.71 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:09:37.110152: step 92770, loss = 0.61 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:09:38.831780: step 92780, loss = 0.49 (743.5 examples/sec; 0.172 sec/batch)
2017-03-26 01:09:40.569581: step 92790, loss = 0.71 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:09:42.477805: step 92800, loss = 0.70 (670.7 examples/sec; 0.191 sec/batch)
2017-03-26 01:09:44.086365: step 92810, loss = 0.60 (795.7 examples/sec; 0.161 sec/batch)
2017-03-26 01:09:45.815458: step 92820, loss = 0.67 (740.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:09:47.551317: step 92830, loss = 0.61 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:09:49.287704: step 92840, loss = 0.73 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:09:51.032300: step 92850, loss = 0.70 (733.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:09:52.760240: step 92860, loss = 0.75 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:09:54.493542: step 92870, loss = 1.03 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:09:56.223480: step 92880, loss = 0.71 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:09:57.968807: step 92890, loss = 0.68 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:09:59.876739: step 92900, loss = 0.71 (671.1 examples/sec; 0.191 sec/batch)
2017-03-26 01:10:01.501511: step 92910, loss = 0.69 (787.3 examples/sec; 0.163 sec/batch)
2017-03-26 01:10:03.241538: step 92920, loss = 0.61 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:10:04.984104: step 92930, loss = 0.76 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:10:06.734239: step 92940, loss = 0.77 (731.4 examples/sec; 0.175 sec/batch)
2017-03-26 01:10:08.475785: step 92950, loss = 0.63 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:10:10.204065: step 92960, loss = 0.64 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:10:11.948650: step 92970, loss = 0.80 (733.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:10:13.683093: step 92980, loss = 0.75 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:10:15.399206: step 92990, loss = 0.61 (745.9 examples/sec; 0.172 sec/batch)
2017-03-26 01:10:17.293736: step 93000, loss = 0.65 (675.6 examples/sec; 0.189 sec/batch)
2017-03-26 01:10:18.907132: step 93010, loss = 0.57 (793.4 examples/sec; 0.161 sec/batch)
2017-03-26 01:10:20.639326: step 93020, loss = 0.75 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:10:22.380273: step 93030, loss = 0.64 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:10:24.148346: step 93040, loss = 0.58 (723.9 examples/sec; 0.177 sec/batch)
2017-03-26 01:10:25.888962: step 93050, loss = 0.83 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:10:27.611828: step 93060, loss = 0.90 (742.9 examples/sec; 0.172 sec/batch)
2017-03-26 01:10:29.352604: step 93070, loss = 0.96 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:10:31.099011: step 93080, loss = 0.83 (732.9 examples/sec; 0.175 sec/batch)
2017-03-26 01:10:32.826086: step 93090, loss = 0.73 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:10:34.718265: step 93100, loss = 0.59 (676.5 examples/sec; 0.189 sec/batch)
2017-03-26 01:10:36.327706: step 93110, loss = 0.74 (795.3 examples/sec; 0.161 sec/batch)
2017-03-26 01:10:38.074456: step 93120, loss = 0.69 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 01:10:39.803311: step 93130, loss = 0.69 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:10:41.531610: step 93140, loss = 0.74 (740.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:10:43.268092: step 93150, loss = 0.73 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:10:45.012486: step 93160, loss = 0.82 (733.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:10:46.733390: step 93170, loss = 0.68 (743.8 examples/sec; 0.172 sec/batch)
2017-03-26 01:10:48.465505: step 93180, loss = 0.68 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:10:50.197949: step 93190, loss = 0.81 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:10:52.103428: step 93200, loss = 0.60 (672.1 examples/sec; 0.190 sec/batch)
2017-03-26 01:10:53.726163: step 93210, loss = 0.58 (788.5 examples/sec; 0.162 sec/batch)
2017-03-26 01:10:55.489332: step 93220, loss = 0.77 (725.8 examples/sec; 0.176 sec/batch)
2017-03-26 01:10:57.226106: step 93230, loss = 0.73 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:10:58.981509: step 93240, loss = 0.65 (729.2 examples/sec; 0.176 sec/batch)
2017-03-26 01:11:00.741810: step 93250, loss = 0.60 (727.3 examples/sec; 0.176 sec/batch)
2017-03-26 01:11:02.486600: step 93260, loss = 0.80 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 01:11:04.214923: step 93270, loss = 0.62 (740.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:11:05.938781: step 93280, loss = 0.74 (742.5 examples/sec; 0.172 sec/batch)
2017-03-26 01:11:07.698720: step 93290, loss = 0.72 (727.3 examples/sec; 0.176 sec/batch)
2017-03-26 01:11:09.659804: step 93300, loss = 0.75 (653.0 examples/sec; 0.196 sec/batch)
2017-03-26 01:11:11.220160: step 93310, loss = 0.67 (819.8 examples/sec; 0.156 sec/batch)
2017-03-26 01:11:12.960556: step 93320, loss = 0.69 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:11:14.713693: step 93330, loss = 0.61 (730.1 examples/sec; 0.175 sec/batch)
2017-03-26 01:11:16.461275: step 93340, loss = 0.58 (732.4 examples/sec; 0.175 sec/batch)
2017-03-26 01:11:18.188297: step 93350, loss = 0.66 (741.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:11:19.924956: step 93360, loss = 0.62 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:11:21.674979: step 93370, loss = 0.58 (731.4 examples/sec; 0.175 sec/batch)
2017-03-26 01:11:23.402577: step 93380, loss = 0.77 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:11:25.140049: step 93390, loss = 0.79 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:11:27.049536: step 93400, loss = 0.71 (671.0 examples/sec; 0.191 sec/batch)
2017-03-26 01:11:28.681535: step 93410, loss = 0.61 (783.4 examples/sec; 0.163 sec/batch)
2017-03-26 01:11:30.416702: step 93420, loss = 0.58 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:11:32.146182: step 93430, loss = 0.67 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:11:33.887467: step 93440, loss = 0.66 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:11:35.614763: step 93450, loss = 0.79 (741.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:11:37.355883: step 93460, loss = 0.76 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:11:39.077575: step 93470, loss = 0.55 (743.5 examples/sec; 0.172 sec/batch)
2017-03-26 01:11:40.824186: step 93480, loss = 0.70 (732.9 examples/sec; 0.175 sec/batch)
2017-03-26 01:11:42.551900: step 93490, loss = 0.73 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:11:44.515874: step 93500, loss = 0.64 (651.7 examples/sec; 0.196 sec/batch)
2017-03-26 01:11:46.051773: step 93510, loss = 0.60 (833.4 examples/sec; 0.154 sec/batch)
2017-03-26 01:11:47.785460: step 93520, loss = 0.75 (738.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:11:49.520455: step 93530, loss = 0.55 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:11:51.248824: step 93540, loss = 0.59 (740.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:11:52.983752: step 93550, loss = 0.59 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:11:54.725384: step 93560, loss = 0.68 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:11:56.448798: step 93570, loss = 0.75 (742.7 examples/sec; 0.172 sec/batch)
2017-03-26 01:11:58.184280: step 93580, loss = 0.77 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:11:59.929113: step 93590, loss = 0.76 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:12:01.810649: step 93600, loss = 0.70 (680.5 examples/sec; 0.188 sec/batch)
2017-03-26 01:12:03.434352: step 93610, loss = 0.77 (788.1 examples/sec; 0.162 sec/batch)
2017-03-26 01:12:05.188503: step 93620, loss = 0.66 (729.6 examples/sec; 0.175 sec/batch)
2017-03-26 01:12:06.931624: step 93630, loss = 0.79 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:12:08.662825: step 93640, loss = 0.70 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:12:10.390776: step 93650, loss = 0.84 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:12:12.118809: step 93660, loss = 0.60 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:12:13.849614: step 93670, loss = 0.79 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:12:15.577374: step 93680, loss = 0.71 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:12:17.325786: step 93690, loss = 0.61 (732.1 examples/sec; 0.175 sec/batch)
2017-03-26 01:12:19.209516: step 93700, loss = 0.68 (679.5 examples/sec; 0.188 sec/batch)
2017-03-26 01:12:20.845115: step 93710, loss = 0.74 (782.6 examples/sec; 0.164 sec/batch)
2017-03-26 01:12:22.584749: step 93720, loss = 0.65 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:12:24.327496: step 93730, loss = 0.62 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:12:26.060092: step 93740, loss = 0.72 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:12:27.790376: step 93750, loss = 0.68 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:12:29.530832: step 93760, loss = 0.80 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:12:31.273253: step 93770, loss = 0.62 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:12:32.999367: step 93780, loss = 0.69 (741.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:12:34.737713: step 93790, loss = 0.69 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:12:36.620849: step 93800, loss = 0.78 (679.8 examples/sec; 0.188 sec/batch)
2017-03-26 01:12:38.237155: step 93810, loss = 0.68 (791.8 examples/sec; 0.162 sec/batch)
2017-03-26 01:12:39.992338: step 93820, loss = 0.65 (729.3 examples/sec; 0.176 sec/batch)
2017-03-26 01:12:41.723249: step 93830, loss = 0.79 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:12:43.466884: step 93840, loss = 0.60 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:12:45.209417: step 93850, loss = 0.80 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:12:46.948511: step 93860, loss = 0.71 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:12:48.684913: step 93870, loss = 0.63 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:12:50.410122: step 93880, loss = 0.60 (741.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:12:52.148155: step 93890, loss = 0.56 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:12:54.022307: step 93900, loss = 0.79 (683.2 examples/sec; 0.187 sec/batch)
2017-03-26 01:12:55.658324: step 93910, loss = 0.72 (782.1 examples/sec; 0.164 sec/batch)
2017-03-26 01:12:57.384994: step 93920, loss = 0.75 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:12:59.122485: step 93930, loss = 0.79 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:13:00.860093: step 93940, loss = 0.69 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:13:02.599791: step 93950, loss = 0.59 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:13:04.334883: step 93960, loss = 0.86 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:13:06.066559: step 93970, loss = 0.82 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:13:07.797743: step 93980, loss = 0.76 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:13:09.534222: step 93990, loss = 0.70 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:13:11.422900: step 94000, loss = 0.58 (677.9 examples/sec; 0.189 sec/batch)
2017-03-26 01:13:13.043520: step 94010, loss = 0.79 (789.5 examples/sec; 0.162 sec/batch)
2017-03-26 01:13:14.771582: step 94020, loss = 0.72 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:13:16.512095: step 94030, loss = 0.68 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:13:18.236328: step 94040, loss = 0.57 (742.3 examples/sec; 0.172 sec/batch)
2017-03-26 01:13:19.967757: step 94050, loss = 0.78 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:13:21.694438: step 94060, loss = 0.77 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:13:23.424798: step 94070, loss = 0.74 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:13:25.169888: step 94080, loss = 0.73 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 01:13:26.920670: step 94090, loss = 0.65 (731.1 examples/sec; 0.175 sec/batch)
2017-03-26 01:13:28.891354: step 94100, loss = 0.71 (649.7 examples/sec; 0.197 sec/batch)
2017-03-26 01:13:30.434505: step 94110, loss = 0.73 (829.2 examples/sec; 0.154 sec/batch)
2017-03-26 01:13:32.165690: step 94120, loss = 0.58 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:13:33.893654: step 94130, loss = 0.65 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:13:35.639276: step 94140, loss = 0.69 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 01:13:37.369240: step 94150, loss = 0.72 (739.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:13:39.108885: step 94160, loss = 0.67 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:13:40.839659: step 94170, loss = 0.65 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:13:42.577555: step 94180, loss = 0.59 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:13:44.322791: step 94190, loss = 0.63 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 01:13:46.199689: step 94200, loss = 0.63 (682.0 examples/sec; 0.188 sec/batch)
2017-03-26 01:13:47.824773: step 94210, loss = 0.69 (787.7 examples/sec; 0.162 sec/batch)
2017-03-26 01:13:49.558966: step 94220, loss = 0.66 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:13:51.298429: step 94230, loss = 0.62 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:13:53.033388: step 94240, loss = 0.64 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:13:54.760315: step 94250, loss = 0.73 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:13:56.503677: step 94260, loss = 0.60 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:13:58.237075: step 94270, loss = 0.61 (738.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:13:59.969066: step 94280, loss = 0.75 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:14:01.706642: step 94290, loss = 0.65 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:14:03.653545: step 94300, loss = 0.89 (657.5 examples/sec; 0.195 sec/batch)
2017-03-26 01:14:05.234581: step 94310, loss = 0.66 (809.6 examples/sec; 0.158 sec/batch)
2017-03-26 01:14:06.957782: step 94320, loss = 0.69 (742.8 examples/sec; 0.172 sec/batch)
2017-03-26 01:14:08.697775: step 94330, loss = 0.76 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:14:10.454569: step 94340, loss = 0.69 (728.6 examples/sec; 0.176 sec/batch)
2017-03-26 01:14:12.173487: step 94350, loss = 0.68 (744.7 examples/sec; 0.172 sec/batch)
2017-03-26 01:14:13.912947: step 94360, loss = 0.79 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:14:15.648675: step 94370, loss = 0.80 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:14:17.386901: step 94380, loss = 0.68 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:14:19.117289: step 94390, loss = 0.76 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:14:21.005050: step 94400, loss = 0.62 (678.1 examples/sec; 0.189 sec/batch)
2017-03-26 01:14:22.626834: step 94410, loss = 0.68 (789.2 examples/sec; 0.162 sec/batch)
2017-03-26 01:14:24.357996: step 94420, loss = 0.77 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:14:26.078180: step 94430, loss = 0.70 (744.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:14:27.819613: step 94440, loss = 0.90 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:14:29.548230: step 94450, loss = 0.67 (740.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:14:31.288355: step 94460, loss = 0.79 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:14:33.016109: step 94470, loss = 0.81 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:14:34.738393: step 94480, loss = 0.69 (743.2 examples/sec; 0.172 sec/batch)
2017-03-26 01:14:36.470757: step 94490, loss = 0.67 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:14:38.425288: step 94500, loss = 0.66 (655.1 examples/sec; 0.195 sec/batch)
2017-03-26 01:14:39.975754: step 94510, loss = 0.66 (825.2 examples/sec; 0.155 sec/batch)
2017-03-26 01:14:41.711185: step 94520, loss = 0.60 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:14:43.443732: step 94530, loss = 0.60 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:14:45.179327: step 94540, loss = 0.68 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:14:46.911932: step 94550, loss = 0.82 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:14:48.654858: step 94560, loss = 0.71 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:14:50.388907: step 94570, loss = 0.67 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:14:52.124755: step 94580, loss = 0.54 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:14:53.852904: step 94590, loss = 0.62 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:14:55.739894: step 94600, loss = 0.77 (678.6 examples/sec; 0.189 sec/batch)
2017-03-26 01:14:57.378405: step 94610, loss = 0.71 (780.9 examples/sec; 0.164 sec/batch)
2017-03-26 01:14:59.106342: step 94620, loss = 0.63 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:15:00.842481: step 94630, loss = 0.67 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:02.583916: step 94640, loss = 0.61 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:04.331549: step 94650, loss = 0.63 (732.4 examples/sec; 0.175 sec/batch)
2017-03-26 01:15:06.071902: step 94660, loss = 0.68 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:07.809653: step 94670, loss = 0.77 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:09.549864: step 94680, loss = 0.64 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:11.285511: step 94690, loss = 0.60 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:13.158845: step 94700, loss = 0.67 (683.5 examples/sec; 0.187 sec/batch)
2017-03-26 01:15:14.795237: step 94710, loss = 0.84 (781.9 examples/sec; 0.164 sec/batch)
2017-03-26 01:15:16.533856: step 94720, loss = 0.84 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:18.264354: step 94730, loss = 0.74 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:15:19.989005: step 94740, loss = 0.71 (742.2 examples/sec; 0.172 sec/batch)
2017-03-26 01:15:21.725620: step 94750, loss = 0.67 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:23.461246: step 94760, loss = 0.75 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:25.198413: step 94770, loss = 0.66 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:26.939106: step 94780, loss = 0.66 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:28.662493: step 94790, loss = 0.81 (742.7 examples/sec; 0.172 sec/batch)
2017-03-26 01:15:30.628476: step 94800, loss = 0.83 (651.3 examples/sec; 0.197 sec/batch)
2017-03-26 01:15:32.156628: step 94810, loss = 0.67 (837.2 examples/sec; 0.153 sec/batch)
2017-03-26 01:15:33.897323: step 94820, loss = 0.70 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:35.628843: step 94830, loss = 0.68 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:15:37.372000: step 94840, loss = 0.63 (734.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:39.098442: step 94850, loss = 0.61 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:15:40.822023: step 94860, loss = 0.64 (742.6 examples/sec; 0.172 sec/batch)
2017-03-26 01:15:42.559619: step 94870, loss = 0.67 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:44.306372: step 94880, loss = 0.67 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 01:15:46.046267: step 94890, loss = 0.68 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:47.977328: step 94900, loss = 0.67 (662.8 examples/sec; 0.193 sec/batch)
2017-03-26 01:15:49.581990: step 94910, loss = 0.60 (797.7 examples/sec; 0.160 sec/batch)
2017-03-26 01:15:51.322509: step 94920, loss = 0.70 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:53.079991: step 94930, loss = 0.54 (728.3 examples/sec; 0.176 sec/batch)
2017-03-26 01:15:54.829142: step 94940, loss = 0.66 (731.8 examples/sec; 0.175 sec/batch)
2017-03-26 01:15:56.564538: step 94950, loss = 0.85 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:15:58.294343: step 94960, loss = 0.73 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:16:00.053167: step 94970, loss = 0.63 (727.8 examples/sec; 0.176 sec/batch)
2017-03-26 01:16:01.797670: step 94980, loss = 0.65 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:16:03.549870: step 94990, loss = 0.76 (730.2 examples/sec; 0.175 sec/batch)
2017-03-26 01:16:05.442259: step 95000, loss = 0.74 (676.4 examples/sec; 0.189 sec/batch)
2017-03-26 01:16:07.068459: step 95010, loss = 0.69 (787.3 examples/sec; 0.163 sec/batch)
2017-03-26 01:16:08.799040: step 95020, loss = 0.70 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:16:10.528121: step 95030, loss = 0.65 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:16:12.262860: step 95040, loss = 0.67 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:16:14.000803: step 95050, loss = 0.64 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:16:15.738046: step 95060, loss = 0.65 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:16:17.475585: step 95070, loss = 0.82 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:16:19.213737: step 95080, loss = 0.80 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:16:20.955863: step 95090, loss = 0.63 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:16:22.861906: step 95100, loss = 0.56 (671.8 examples/sec; 0.191 sec/batch)
2017-03-26 01:16:24.475426: step 95110, loss = 0.87 (792.9 examples/sec; 0.161 sec/batch)
2017-03-26 01:16:26.211392: step 95120, loss = 0.63 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:16:27.944014: step 95130, loss = 0.64 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:16:29.672155: step 95140, loss = 0.65 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:16:31.398149: step 95150, loss = 0.64 (741.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:16:33.140058: step 95160, loss = 0.71 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:16:34.880196: step 95170, loss = 0.60 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:16:36.621816: step 95180, loss = 0.78 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:16:38.351491: step 95190, loss = 0.76 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:16:40.258651: step 95200, loss = 0.63 (671.3 examples/sec; 0.191 sec/batch)
2017-03-26 01:16:41.870121: step 95210, loss = 0.67 (794.0 examples/sec; 0.161 sec/batch)
2017-03-26 01:16:43.603489: step 95220, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:16:45.337742: step 95230, loss = 0.64 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:16:47.072229: step 95240, loss = 0.64 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:16:48.815347: step 95250, loss = 0.66 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:16:50.552186: step 95260, loss = 0.59 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:16:52.281606: step 95270, loss = 0.58 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:16:54.019135: step 95280, loss = 0.65 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:16:55.761419: step 95290, loss = 0.70 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:16:57.652373: step 95300, loss = 0.83 (676.9 examples/sec; 0.189 sec/batch)
2017-03-26 01:16:59.268102: step 95310, loss = 0.62 (792.2 examples/sec; 0.162 sec/batch)
2017-03-26 01:17:01.004492: step 95320, loss = 0.77 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:17:02.747306: step 95330, loss = 0.66 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:17:04.476097: step 95340, loss = 0.85 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:17:06.212739: step 95350, loss = 0.58 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:17:07.949100: step 95360, loss = 0.77 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:17:09.683943: step 95370, loss = 0.67 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:17:11.419249: step 95380, loss = 0.74 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:17:13.151402: step 95390, loss = 0.68 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:17:15.058124: step 95400, loss = 0.78 (671.3 examples/sec; 0.191 sec/batch)
2017-03-26 01:17:16.651621: step 95410, loss = 0.64 (803.3 examples/sec; 0.159 sec/batch)
2017-03-26 01:17:18.375851: step 95420, loss = 0.74 (742.3 examples/sec; 0.172 sec/batch)
2017-03-26 01:17:20.117873: step 95430, loss = 0.69 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:17:21.842620: step 95440, loss = 0.67 (742.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:17:23.573429: step 95450, loss = 0.71 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:17:25.303905: step 95460, loss = 0.68 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:17:27.037350: step 95470, loss = 0.66 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:17:28.778445: step 95480, loss = 0.66 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:17:30.518880: step 95490, loss = 0.70 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:17:32.390159: step 95500, loss = 0.65 (684.3 examples/sec; 0.187 sec/batch)
2017-03-26 01:17:34.019640: step 95510, loss = 0.67 (785.2 examples/sec; 0.163 sec/batch)
2017-03-26 01:17:35.751635: step 95520, loss = 0.65 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:17:37.480793: step 95530, loss = 0.80 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:17:39.213668: step 95540, loss = 0.79 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:17:40.945133: step 95550, loss = 0.58 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:17:42.685408: step 95560, loss = 0.61 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:17:44.438050: step 95570, loss = 0.87 (730.3 examples/sec; 0.175 sec/batch)
2017-03-26 01:17:46.180671: step 95580, loss = 0.63 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:17:47.909504: step 95590, loss = 0.77 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:17:49.806818: step 95600, loss = 0.72 (674.6 examples/sec; 0.190 sec/batch)
2017-03-26 01:17:51.437404: step 95610, loss = 0.68 (785.0 examples/sec; 0.163 sec/batch)
2017-03-26 01:17:53.166200: step 95620, loss = 0.55 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:17:54.905316: step 95630, loss = 0.79 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:17:56.637647: step 95640, loss = 0.70 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:17:58.373247: step 95650, loss = 0.54 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:18:00.112263: step 95660, loss = 0.79 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:18:01.851149: step 95670, loss = 0.82 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:18:03.581653: step 95680, loss = 0.68 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:18:05.320138: step 95690, loss = 0.52 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:18:07.279776: step 95700, loss = 0.66 (653.2 examples/sec; 0.196 sec/batch)
2017-03-26 01:18:08.825530: step 95710, loss = 0.76 (828.1 examples/sec; 0.155 sec/batch)
2017-03-26 01:18:10.559125: step 95720, loss = 0.79 (738.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:18:12.300511: step 95730, loss = 0.78 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:18:14.037550: step 95740, loss = 0.60 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:18:15.771111: step 95750, loss = 0.81 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:18:17.500908: step 95760, loss = 0.61 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:18:19.252743: step 95770, loss = 0.83 (730.7 examples/sec; 0.175 sec/batch)
2017-03-26 01:18:20.988551: step 95780, loss = 0.64 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:18:22.726245: step 95790, loss = 0.71 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:18:24.606727: step 95800, loss = 0.65 (681.0 examples/sec; 0.188 sec/batch)
2017-03-26 01:18:26.227711: step 95810, loss = 0.74 (789.1 examples/sec; 0.162 sec/batch)
2017-03-26 01:18:27.971344: step 95820, loss = 0.57 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:18:29.701507: step 95830, loss = 0.61 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:18:31.429337: step 95840, loss = 0.58 (740.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:18:33.163106: step 95850, loss = 0.64 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:18:34.898604: step 95860, loss = 0.77 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:18:36.641842: step 95870, loss = 0.63 (734.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:18:38.378303: step 95880, loss = 0.87 (737.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:18:40.123634: step 95890, loss = 0.72 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 01:18:41.999062: step 95900, loss = 0.69 (682.5 examples/sec; 0.188 sec/batch)
2017-03-26 01:18:43.633801: step 95910, loss = 0.75 (783.0 examples/sec; 0.163 sec/batch)
2017-03-26 01:18:45.359640: step 95920, loss = 0.74 (741.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:18:47.089979: step 95930, loss = 0.55 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:18:48.831878: step 95940, loss = 0.64 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:18:50.556634: step 95950, loss = 0.82 (742.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:18:52.305282: step 95960, loss = 0.72 (732.0 examples/sec; 0.175 sec/batch)
2017-03-26 01:18:54.032803: step 95970, loss = 0.64 (740.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:18:55.761507: step 95980, loss = 0.57 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:18:57.498185: step 95990, loss = 0.86 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:18:59.387502: step 96000, loss = 0.65 (677.7 examples/sec; 0.189 sec/batch)
2017-03-26 01:19:01.021302: step 96010, loss = 0.56 (783.1 examples/sec; 0.163 sec/batch)
2017-03-26 01:19:02.749234: step 96020, loss = 0.64 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:19:04.491190: step 96030, loss = 0.83 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:19:06.222678: step 96040, loss = 0.63 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:19:07.968684: step 96050, loss = 0.71 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 01:19:09.692607: step 96060, loss = 0.76 (742.2 examples/sec; 0.172 sec/batch)
2017-03-26 01:19:11.433302: step 96070, loss = 0.71 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:19:13.178994: step 96080, loss = 0.74 (733.2 examples/sec; 0.175 sec/batch)
2017-03-26 01:19:14.906395: step 96090, loss = 0.79 (741.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:19:16.800620: step 96100, loss = 0.74 (675.7 examples/sec; 0.189 sec/batch)
2017-03-26 01:19:18.406287: step 96110, loss = 0.61 (797.2 examples/sec; 0.161 sec/batch)
2017-03-26 01:19:20.136440: step 96120, loss = 0.75 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:19:21.871477: step 96130, loss = 0.60 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:19:23.612941: step 96140, loss = 0.67 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:19:25.349261: step 96150, loss = 0.77 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:19:27.094259: step 96160, loss = 0.72 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 01:19:28.841079: step 96170, loss = 0.73 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 01:19:30.590858: step 96180, loss = 0.65 (731.4 examples/sec; 0.175 sec/batch)
2017-03-26 01:19:32.328395: step 96190, loss = 0.72 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:19:34.220219: step 96200, loss = 0.63 (676.7 examples/sec; 0.189 sec/batch)
2017-03-26 01:19:35.857799: step 96210, loss = 0.65 (781.5 examples/sec; 0.164 sec/batch)
2017-03-26 01:19:37.590612: step 96220, loss = 0.73 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:19:39.330939: step 96230, loss = 0.83 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:19:41.071285: step 96240, loss = 0.69 (735.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:19:42.802469: step 96250, loss = 0.72 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:19:44.556520: step 96260, loss = 0.56 (729.7 examples/sec; 0.175 sec/batch)
2017-03-26 01:19:46.289539: step 96270, loss = 0.73 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:19:48.020949: step 96280, loss = 0.75 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:19:49.760084: step 96290, loss = 0.76 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:19:51.642381: step 96300, loss = 0.74 (680.0 examples/sec; 0.188 sec/batch)
2017-03-26 01:19:53.268715: step 96310, loss = 0.57 (787.0 examples/sec; 0.163 sec/batch)
2017-03-26 01:19:55.010287: step 96320, loss = 0.73 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:19:56.753426: step 96330, loss = 0.62 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:19:58.492399: step 96340, loss = 0.77 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:20:00.233984: step 96350, loss = 0.64 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:20:01.991873: step 96360, loss = 0.66 (728.2 examples/sec; 0.176 sec/batch)
2017-03-26 01:20:03.726905: step 96370, loss = 0.65 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:20:05.468022: step 96380, loss = 0.72 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:20:07.204346: step 96390, loss = 0.60 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:20:09.112736: step 96400, loss = 0.80 (670.9 examples/sec; 0.191 sec/batch)
2017-03-26 01:20:10.749119: step 96410, loss = 0.63 (782.3 examples/sec; 0.164 sec/batch)
2017-03-26 01:20:12.475223: step 96420, loss = 0.60 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:20:14.201590: step 96430, loss = 0.90 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:20:15.935119: step 96440, loss = 0.69 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:20:17.674282: step 96450, loss = 0.80 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:20:19.408189: step 96460, loss = 0.79 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:20:21.143321: step 96470, loss = 0.80 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:20:22.889216: step 96480, loss = 0.68 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 01:20:24.630073: step 96490, loss = 0.76 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:20:26.513309: step 96500, loss = 0.76 (679.5 examples/sec; 0.188 sec/batch)
2017-03-26 01:20:28.133991: step 96510, loss = 0.61 (789.8 examples/sec; 0.162 sec/batch)
2017-03-26 01:20:29.880616: step 96520, loss = 0.62 (732.8 examples/sec; 0.175 sec/batch)
2017-03-26 01:20:31.632249: step 96530, loss = 0.75 (730.7 examples/sec; 0.175 sec/batch)
2017-03-26 01:20:33.377166: step 96540, loss = 0.75 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:20:35.119381: step 96550, loss = 0.76 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:20:36.850025: step 96560, loss = 0.93 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:20:38.588623: step 96570, loss = 0.84 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:20:40.323040: step 96580, loss = 0.84 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:20:42.041014: step 96590, loss = 0.65 (745.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:20:43.936283: step 96600, loss = 0.80 (675.4 examples/sec; 0.190 sec/batch)
2017-03-26 01:20:45.554995: step 96610, loss = 0.65 (790.7 examples/sec; 0.162 sec/batch)
2017-03-26 01:20:47.290662: step 96620, loss = 0.64 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:20:49.015206: step 96630, loss = 0.64 (742.2 examples/sec; 0.172 sec/batch)
2017-03-26 01:20:50.747157: step 96640, loss = 0.70 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:20:52.476076: step 96650, loss = 0.76 (740.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:20:54.234595: step 96660, loss = 0.74 (727.9 examples/sec; 0.176 sec/batch)
2017-03-26 01:20:55.997892: step 96670, loss = 0.63 (725.9 examples/sec; 0.176 sec/batch)
2017-03-26 01:20:57.729152: step 96680, loss = 0.68 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:20:59.460849: step 96690, loss = 0.60 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:21:01.356543: step 96700, loss = 0.65 (675.2 examples/sec; 0.190 sec/batch)
2017-03-26 01:21:02.984637: step 96710, loss = 0.80 (786.2 examples/sec; 0.163 sec/batch)
2017-03-26 01:21:04.720203: step 96720, loss = 0.68 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:21:06.449747: step 96730, loss = 0.69 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:21:08.182589: step 96740, loss = 0.64 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:21:09.931493: step 96750, loss = 0.75 (731.9 examples/sec; 0.175 sec/batch)
2017-03-26 01:21:11.674412: step 96760, loss = 0.69 (734.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:21:13.412305: step 96770, loss = 0.59 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:21:15.145596: step 96780, loss = 0.64 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:21:16.868693: step 96790, loss = 0.61 (742.9 examples/sec; 0.172 sec/batch)
2017-03-26 01:21:18.774463: step 96800, loss = 0.71 (671.6 examples/sec; 0.191 sec/batch)
2017-03-26 01:21:20.387974: step 96810, loss = 0.73 (793.3 examples/sec; 0.161 sec/batch)
2017-03-26 01:21:22.121689: step 96820, loss = 0.64 (738.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:21:23.856622: step 96830, loss = 0.73 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:21:25.583129: step 96840, loss = 0.62 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:21:27.330159: step 96850, loss = 0.64 (732.7 examples/sec; 0.175 sec/batch)
2017-03-26 01:21:29.069807: step 96860, loss = 0.64 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:21:30.805762: step 96870, loss = 0.69 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:21:32.552068: step 96880, loss = 0.62 (733.0 examples/sec; 0.175 sec/batch)
2017-03-26 01:21:34.293737: step 96890, loss = 0.63 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:21:36.230209: step 96900, loss = 0.62 (661.0 examples/sec; 0.194 sec/batch)
2017-03-26 01:21:37.798961: step 96910, loss = 0.70 (816.1 examples/sec; 0.157 sec/batch)
2017-03-26 01:21:39.533479: step 96920, loss = 0.61 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:21:41.266993: step 96930, loss = 0.71 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:21:43.008770: step 96940, loss = 0.54 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:21:44.756051: step 96950, loss = 0.71 (732.6 examples/sec; 0.175 sec/batch)
2017-03-26 01:21:46.494008: step 96960, loss = 0.55 (736.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:21:48.222144: step 96970, loss = 0.71 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:21:49.956870: step 96980, loss = 0.81 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:21:51.693838: step 96990, loss = 0.80 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:21:53.654388: step 97000, loss = 0.65 (653.2 examples/sec; 0.196 sec/batch)
2017-03-26 01:21:55.187111: step 97010, loss = 0.63 (834.6 examples/sec; 0.153 sec/batch)
2017-03-26 01:21:56.937490: step 97020, loss = 0.63 (731.3 examples/sec; 0.175 sec/batch)
2017-03-26 01:21:58.672071: step 97030, loss = 0.70 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:00.403972: step 97040, loss = 0.52 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:02.138842: step 97050, loss = 0.54 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:03.882926: step 97060, loss = 0.62 (733.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:22:05.614690: step 97070, loss = 0.75 (739.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:07.344893: step 97080, loss = 0.72 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:09.080492: step 97090, loss = 0.60 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:22:10.972612: step 97100, loss = 0.73 (676.8 examples/sec; 0.189 sec/batch)
2017-03-26 01:22:12.595036: step 97110, loss = 0.56 (788.5 examples/sec; 0.162 sec/batch)
2017-03-26 01:22:14.322065: step 97120, loss = 0.79 (741.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:16.066412: step 97130, loss = 0.62 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:22:17.792305: step 97140, loss = 0.71 (741.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:19.528412: step 97150, loss = 0.73 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:22:21.275294: step 97160, loss = 0.67 (732.7 examples/sec; 0.175 sec/batch)
2017-03-26 01:22:23.013895: step 97170, loss = 0.71 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:22:24.743563: step 97180, loss = 0.71 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:26.480418: step 97190, loss = 0.71 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:22:28.375104: step 97200, loss = 0.62 (675.7 examples/sec; 0.189 sec/batch)
2017-03-26 01:22:29.981058: step 97210, loss = 0.79 (796.8 examples/sec; 0.161 sec/batch)
2017-03-26 01:22:31.713673: step 97220, loss = 0.86 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:33.441919: step 97230, loss = 0.72 (740.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:35.184889: step 97240, loss = 0.72 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:22:36.927817: step 97250, loss = 0.79 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:22:38.678029: step 97260, loss = 0.74 (731.3 examples/sec; 0.175 sec/batch)
2017-03-26 01:22:40.410698: step 97270, loss = 0.72 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:42.137359: step 97280, loss = 0.53 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:43.879184: step 97290, loss = 0.69 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:22:45.837893: step 97300, loss = 0.73 (653.8 examples/sec; 0.196 sec/batch)
2017-03-26 01:22:47.365843: step 97310, loss = 0.81 (837.2 examples/sec; 0.153 sec/batch)
2017-03-26 01:22:49.080210: step 97320, loss = 0.53 (746.6 examples/sec; 0.171 sec/batch)
2017-03-26 01:22:50.830019: step 97330, loss = 0.79 (731.5 examples/sec; 0.175 sec/batch)
2017-03-26 01:22:52.562484: step 97340, loss = 0.80 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:54.296980: step 97350, loss = 0.84 (738.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:56.020832: step 97360, loss = 0.61 (742.5 examples/sec; 0.172 sec/batch)
2017-03-26 01:22:57.751244: step 97370, loss = 0.65 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:22:59.491737: step 97380, loss = 0.64 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:23:01.224643: step 97390, loss = 0.58 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:23:03.173224: step 97400, loss = 0.70 (656.9 examples/sec; 0.195 sec/batch)
2017-03-26 01:23:04.738221: step 97410, loss = 0.65 (817.9 examples/sec; 0.157 sec/batch)
2017-03-26 01:23:06.470266: step 97420, loss = 0.77 (739.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:23:08.205677: step 97430, loss = 0.70 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:23:09.942890: step 97440, loss = 0.74 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:23:11.684178: step 97450, loss = 0.59 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:23:13.412319: step 97460, loss = 0.72 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:23:15.131629: step 97470, loss = 0.76 (744.5 examples/sec; 0.172 sec/batch)
2017-03-26 01:23:16.867511: step 97480, loss = 0.72 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:23:18.606237: step 97490, loss = 0.64 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:23:20.493138: step 97500, loss = 0.74 (678.4 examples/sec; 0.189 sec/batch)
2017-03-26 01:23:22.114603: step 97510, loss = 0.63 (789.4 examples/sec; 0.162 sec/batch)
2017-03-26 01:23:23.864332: step 97520, loss = 0.86 (731.7 examples/sec; 0.175 sec/batch)
2017-03-26 01:23:25.600338: step 97530, loss = 0.72 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:23:27.331015: step 97540, loss = 0.67 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:23:29.064158: step 97550, loss = 0.65 (738.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:23:30.791090: step 97560, loss = 0.78 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:23:32.540382: step 97570, loss = 0.70 (731.7 examples/sec; 0.175 sec/batch)
2017-03-26 01:23:34.267485: step 97580, loss = 0.64 (741.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:23:36.007071: step 97590, loss = 0.60 (735.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:23:37.890381: step 97600, loss = 0.70 (679.9 examples/sec; 0.188 sec/batch)
2017-03-26 01:23:39.508281: step 97610, loss = 0.65 (790.6 examples/sec; 0.162 sec/batch)
2017-03-26 01:23:41.253311: step 97620, loss = 0.64 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 01:23:42.996393: step 97630, loss = 0.56 (734.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:23:44.736000: step 97640, loss = 0.89 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:23:46.476498: step 97650, loss = 0.59 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:23:48.210333: step 97660, loss = 0.68 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:23:49.948558: step 97670, loss = 0.59 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:23:51.674826: step 97680, loss = 0.63 (741.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:23:53.407247: step 97690, loss = 0.67 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:23:55.355635: step 97700, loss = 0.68 (657.2 examples/sec; 0.195 sec/batch)
2017-03-26 01:23:56.898688: step 97710, loss = 0.55 (829.1 examples/sec; 0.154 sec/batch)
2017-03-26 01:23:58.613917: step 97720, loss = 0.57 (746.3 examples/sec; 0.172 sec/batch)
2017-03-26 01:24:00.349318: step 97730, loss = 0.59 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:24:02.080789: step 97740, loss = 0.77 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:24:03.809605: step 97750, loss = 0.64 (740.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:24:05.538272: step 97760, loss = 0.75 (740.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:24:07.271798: step 97770, loss = 0.71 (738.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:24:09.002616: step 97780, loss = 0.63 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:24:10.742226: step 97790, loss = 0.75 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:24:12.624861: step 97800, loss = 0.79 (679.9 examples/sec; 0.188 sec/batch)
2017-03-26 01:24:14.257804: step 97810, loss = 0.85 (783.9 examples/sec; 0.163 sec/batch)
2017-03-26 01:24:15.988788: step 97820, loss = 0.75 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:24:17.720067: step 97830, loss = 0.91 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:24:19.462264: step 97840, loss = 0.81 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:24:21.202088: step 97850, loss = 0.75 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:24:22.933696: step 97860, loss = 0.61 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:24:24.670679: step 97870, loss = 0.62 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:24:26.411732: step 97880, loss = 0.66 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:24:28.143409: step 97890, loss = 0.81 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:24:30.022333: step 97900, loss = 0.68 (681.2 examples/sec; 0.188 sec/batch)
2017-03-26 01:24:31.643084: step 97910, loss = 0.67 (789.8 examples/sec; 0.162 sec/batch)
2017-03-26 01:24:33.385707: step 97920, loss = 0.64 (734.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:24:35.115913: step 97930, loss = 0.67 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:24:36.856641: step 97940, loss = 0.57 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:24:38.589259: step 97950, loss = 0.61 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:24:40.330893: step 97960, loss = 0.86 (734.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:24:42.065665: step 97970, loss = 0.81 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:24:43.809452: step 97980, loss = 0.67 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:24:45.554306: step 97990, loss = 0.61 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:24:47.457399: step 98000, loss = 0.72 (672.8 examples/sec; 0.190 sec/batch)
2017-03-26 01:24:49.062592: step 98010, loss = 0.67 (797.1 examples/sec; 0.161 sec/batch)
2017-03-26 01:24:50.800595: step 98020, loss = 0.64 (736.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:24:52.545955: step 98030, loss = 0.59 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 01:24:54.284476: step 98040, loss = 0.67 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:24:56.008009: step 98050, loss = 0.87 (742.6 examples/sec; 0.172 sec/batch)
2017-03-26 01:24:57.748711: step 98060, loss = 0.70 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:24:59.477993: step 98070, loss = 0.63 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:25:01.208641: step 98080, loss = 0.81 (739.6 examples/sec; 0.173 sec/batch)
2017-03-26 01:25:02.952184: step 98090, loss = 0.69 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:25:04.844556: step 98100, loss = 0.68 (676.4 examples/sec; 0.189 sec/batch)
2017-03-26 01:25:06.465912: step 98110, loss = 0.62 (789.5 examples/sec; 0.162 sec/batch)
2017-03-26 01:25:08.199970: step 98120, loss = 0.72 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:25:09.924376: step 98130, loss = 0.72 (742.3 examples/sec; 0.172 sec/batch)
2017-03-26 01:25:11.665513: step 98140, loss = 0.74 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:25:13.403089: step 98150, loss = 0.80 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:25:15.127668: step 98160, loss = 0.75 (742.2 examples/sec; 0.172 sec/batch)
2017-03-26 01:25:16.857274: step 98170, loss = 0.78 (740.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:25:18.599667: step 98180, loss = 0.78 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:25:20.343405: step 98190, loss = 0.59 (734.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:25:22.328865: step 98200, loss = 0.55 (644.7 examples/sec; 0.199 sec/batch)
2017-03-26 01:25:23.823833: step 98210, loss = 0.67 (856.2 examples/sec; 0.149 sec/batch)
2017-03-26 01:25:25.548771: step 98220, loss = 0.65 (742.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:25:27.279830: step 98230, loss = 0.68 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:25:29.024939: step 98240, loss = 0.73 (733.5 examples/sec; 0.175 sec/batch)
2017-03-26 01:25:30.770979: step 98250, loss = 0.76 (733.1 examples/sec; 0.175 sec/batch)
2017-03-26 01:25:32.503785: step 98260, loss = 0.57 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:25:34.234123: step 98270, loss = 0.66 (739.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:25:35.970143: step 98280, loss = 0.78 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:25:37.708313: step 98290, loss = 0.69 (736.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:25:39.598830: step 98300, loss = 0.80 (677.1 examples/sec; 0.189 sec/batch)
2017-03-26 01:25:41.219551: step 98310, loss = 0.69 (789.8 examples/sec; 0.162 sec/batch)
2017-03-26 01:25:42.957979: step 98320, loss = 0.73 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:25:44.698670: step 98330, loss = 0.68 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:25:46.439477: step 98340, loss = 0.55 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:25:48.191883: step 98350, loss = 0.62 (730.3 examples/sec; 0.175 sec/batch)
2017-03-26 01:25:49.933488: step 98360, loss = 0.77 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:25:51.664903: step 98370, loss = 0.65 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:25:53.411100: step 98380, loss = 0.67 (733.0 examples/sec; 0.175 sec/batch)
2017-03-26 01:25:55.144841: step 98390, loss = 0.72 (738.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:25:57.109080: step 98400, loss = 0.80 (652.0 examples/sec; 0.196 sec/batch)
2017-03-26 01:25:58.652873: step 98410, loss = 0.71 (828.6 examples/sec; 0.154 sec/batch)
2017-03-26 01:26:00.394191: step 98420, loss = 0.81 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:26:02.126776: step 98430, loss = 0.76 (738.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:26:03.841289: step 98440, loss = 0.64 (746.6 examples/sec; 0.171 sec/batch)
2017-03-26 01:26:05.587797: step 98450, loss = 0.62 (732.9 examples/sec; 0.175 sec/batch)
2017-03-26 01:26:07.312585: step 98460, loss = 0.66 (742.2 examples/sec; 0.172 sec/batch)
2017-03-26 01:26:09.045943: step 98470, loss = 0.69 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:26:10.779486: step 98480, loss = 0.70 (738.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:26:12.514188: step 98490, loss = 0.65 (737.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:26:14.411872: step 98500, loss = 0.67 (674.8 examples/sec; 0.190 sec/batch)
2017-03-26 01:26:16.027241: step 98510, loss = 0.64 (791.9 examples/sec; 0.162 sec/batch)
2017-03-26 01:26:17.756468: step 98520, loss = 0.75 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:26:19.497406: step 98530, loss = 0.64 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:26:21.225084: step 98540, loss = 0.83 (740.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:26:22.973192: step 98550, loss = 0.73 (732.2 examples/sec; 0.175 sec/batch)
2017-03-26 01:26:24.723460: step 98560, loss = 0.83 (731.3 examples/sec; 0.175 sec/batch)
2017-03-26 01:26:26.443941: step 98570, loss = 0.66 (744.0 examples/sec; 0.172 sec/batch)
2017-03-26 01:26:28.187863: step 98580, loss = 0.77 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:26:29.915616: step 98590, loss = 0.75 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:26:31.818565: step 98600, loss = 0.83 (672.9 examples/sec; 0.190 sec/batch)
2017-03-26 01:26:33.434147: step 98610, loss = 0.58 (791.9 examples/sec; 0.162 sec/batch)
2017-03-26 01:26:35.173975: step 98620, loss = 0.81 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:26:36.905036: step 98630, loss = 0.70 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:26:38.648861: step 98640, loss = 0.70 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:26:40.387874: step 98650, loss = 0.73 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:26:42.120706: step 98660, loss = 0.63 (738.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:26:43.857701: step 98670, loss = 0.60 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:26:45.580276: step 98680, loss = 0.72 (743.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:26:47.317655: step 98690, loss = 0.71 (736.9 examples/sec; 0.174 sec/batch)
2017-03-26 01:26:49.257459: step 98700, loss = 0.69 (659.8 examples/sec; 0.194 sec/batch)
2017-03-26 01:26:50.824778: step 98710, loss = 0.76 (816.7 examples/sec; 0.157 sec/batch)
2017-03-26 01:26:52.573354: step 98720, loss = 0.53 (732.0 examples/sec; 0.175 sec/batch)
2017-03-26 01:26:54.295192: step 98730, loss = 0.70 (743.4 examples/sec; 0.172 sec/batch)
2017-03-26 01:26:56.046076: step 98740, loss = 0.76 (731.1 examples/sec; 0.175 sec/batch)
2017-03-26 01:26:57.777360: step 98750, loss = 0.68 (739.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:26:59.501858: step 98760, loss = 0.61 (742.2 examples/sec; 0.172 sec/batch)
2017-03-26 01:27:01.246769: step 98770, loss = 0.60 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:27:02.971533: step 98780, loss = 0.95 (742.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:27:04.717469: step 98790, loss = 0.69 (733.3 examples/sec; 0.175 sec/batch)
2017-03-26 01:27:06.613417: step 98800, loss = 0.75 (675.0 examples/sec; 0.190 sec/batch)
2017-03-26 01:27:08.232076: step 98810, loss = 0.73 (791.0 examples/sec; 0.162 sec/batch)
2017-03-26 01:27:09.981957: step 98820, loss = 0.71 (731.3 examples/sec; 0.175 sec/batch)
2017-03-26 01:27:11.724250: step 98830, loss = 0.58 (734.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:27:13.473953: step 98840, loss = 0.68 (731.6 examples/sec; 0.175 sec/batch)
2017-03-26 01:27:15.204288: step 98850, loss = 0.70 (739.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:27:16.939094: step 98860, loss = 0.64 (737.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:27:18.683403: step 98870, loss = 0.77 (733.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:27:20.427708: step 98880, loss = 0.60 (733.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:27:22.169593: step 98890, loss = 0.69 (734.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:27:24.055348: step 98900, loss = 0.58 (678.8 examples/sec; 0.189 sec/batch)
2017-03-26 01:27:25.687138: step 98910, loss = 0.62 (784.4 examples/sec; 0.163 sec/batch)
2017-03-26 01:27:27.418623: step 98920, loss = 0.69 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:27:29.145332: step 98930, loss = 0.62 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:27:30.886383: step 98940, loss = 0.55 (735.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:27:32.608866: step 98950, loss = 0.66 (743.1 examples/sec; 0.172 sec/batch)
2017-03-26 01:27:34.332155: step 98960, loss = 0.74 (742.8 examples/sec; 0.172 sec/batch)
2017-03-26 01:27:36.063334: step 98970, loss = 0.64 (739.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:27:37.817557: step 98980, loss = 0.68 (729.7 examples/sec; 0.175 sec/batch)
2017-03-26 01:27:39.551759: step 98990, loss = 0.79 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:27:41.441772: step 99000, loss = 0.78 (677.5 examples/sec; 0.189 sec/batch)
2017-03-26 01:27:43.063067: step 99010, loss = 0.71 (789.1 examples/sec; 0.162 sec/batch)
2017-03-26 01:27:44.802757: step 99020, loss = 0.68 (735.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:27:46.536877: step 99030, loss = 0.71 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:27:48.272103: step 99040, loss = 0.61 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:27:50.003727: step 99050, loss = 0.71 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:27:51.737959: step 99060, loss = 0.71 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:27:53.473091: step 99070, loss = 0.68 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:27:55.208474: step 99080, loss = 0.76 (737.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:27:56.954234: step 99090, loss = 0.68 (733.2 examples/sec; 0.175 sec/batch)
2017-03-26 01:27:58.856320: step 99100, loss = 0.58 (673.2 examples/sec; 0.190 sec/batch)
2017-03-26 01:28:00.483524: step 99110, loss = 0.69 (786.6 examples/sec; 0.163 sec/batch)
2017-03-26 01:28:02.223012: step 99120, loss = 0.81 (735.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:28:03.961863: step 99130, loss = 0.66 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:28:05.701626: step 99140, loss = 0.58 (735.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:28:07.432311: step 99150, loss = 0.71 (739.5 examples/sec; 0.173 sec/batch)
2017-03-26 01:28:09.191778: step 99160, loss = 0.60 (727.5 examples/sec; 0.176 sec/batch)
2017-03-26 01:28:10.939427: step 99170, loss = 0.83 (732.4 examples/sec; 0.175 sec/batch)
2017-03-26 01:28:12.671038: step 99180, loss = 0.58 (739.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:28:14.407595: step 99190, loss = 0.86 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:28:16.306051: step 99200, loss = 0.60 (674.2 examples/sec; 0.190 sec/batch)
2017-03-26 01:28:17.930285: step 99210, loss = 0.69 (787.7 examples/sec; 0.162 sec/batch)
2017-03-26 01:28:19.679149: step 99220, loss = 0.67 (731.9 examples/sec; 0.175 sec/batch)
2017-03-26 01:28:21.421514: step 99230, loss = 0.67 (734.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:28:23.163035: step 99240, loss = 0.80 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:28:24.902101: step 99250, loss = 0.67 (736.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:28:26.638313: step 99260, loss = 0.59 (737.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:28:28.381713: step 99270, loss = 0.59 (734.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:28:30.110861: step 99280, loss = 0.75 (740.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:28:31.849200: step 99290, loss = 0.63 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:28:33.740510: step 99300, loss = 0.70 (676.8 examples/sec; 0.189 sec/batch)
2017-03-26 01:28:35.364529: step 99310, loss = 0.59 (788.2 examples/sec; 0.162 sec/batch)
2017-03-26 01:28:37.102985: step 99320, loss = 0.77 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:28:38.837174: step 99330, loss = 0.72 (738.1 examples/sec; 0.173 sec/batch)
2017-03-26 01:28:40.564737: step 99340, loss = 0.67 (740.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:28:42.292600: step 99350, loss = 0.66 (740.8 examples/sec; 0.173 sec/batch)
2017-03-26 01:28:44.028379: step 99360, loss = 0.64 (737.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:28:45.762102: step 99370, loss = 0.82 (738.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:28:47.516438: step 99380, loss = 0.78 (729.6 examples/sec; 0.175 sec/batch)
2017-03-26 01:28:49.251572: step 99390, loss = 0.77 (737.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:28:51.230901: step 99400, loss = 0.78 (646.7 examples/sec; 0.198 sec/batch)
2017-03-26 01:28:52.763880: step 99410, loss = 0.65 (835.0 examples/sec; 0.153 sec/batch)
2017-03-26 01:28:54.493216: step 99420, loss = 0.74 (740.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:28:56.230013: step 99430, loss = 0.75 (737.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:28:57.956400: step 99440, loss = 0.57 (741.4 examples/sec; 0.173 sec/batch)
2017-03-26 01:28:59.701645: step 99450, loss = 0.57 (733.4 examples/sec; 0.175 sec/batch)
2017-03-26 01:29:01.442851: step 99460, loss = 0.68 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:29:03.172526: step 99470, loss = 0.66 (740.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:29:04.917368: step 99480, loss = 0.76 (733.6 examples/sec; 0.174 sec/batch)
2017-03-26 01:29:06.654578: step 99490, loss = 0.76 (736.8 examples/sec; 0.174 sec/batch)
2017-03-26 01:29:08.541559: step 99500, loss = 0.75 (678.3 examples/sec; 0.189 sec/batch)
2017-03-26 01:29:10.176363: step 99510, loss = 0.65 (783.0 examples/sec; 0.163 sec/batch)
2017-03-26 01:29:11.925228: step 99520, loss = 0.67 (731.9 examples/sec; 0.175 sec/batch)
2017-03-26 01:29:13.676317: step 99530, loss = 0.74 (731.0 examples/sec; 0.175 sec/batch)
2017-03-26 01:29:15.397900: step 99540, loss = 0.55 (743.5 examples/sec; 0.172 sec/batch)
2017-03-26 01:29:17.130263: step 99550, loss = 0.54 (738.9 examples/sec; 0.173 sec/batch)
2017-03-26 01:29:18.852922: step 99560, loss = 0.64 (743.0 examples/sec; 0.172 sec/batch)
2017-03-26 01:29:20.601853: step 99570, loss = 0.73 (731.9 examples/sec; 0.175 sec/batch)
2017-03-26 01:29:22.340816: step 99580, loss = 0.70 (736.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:29:24.076890: step 99590, loss = 0.62 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:29:25.993560: step 99600, loss = 0.68 (668.1 examples/sec; 0.192 sec/batch)
2017-03-26 01:29:27.567033: step 99610, loss = 0.64 (813.1 examples/sec; 0.157 sec/batch)
2017-03-26 01:29:29.304558: step 99620, loss = 0.78 (736.7 examples/sec; 0.174 sec/batch)
2017-03-26 01:29:31.043262: step 99630, loss = 0.83 (736.2 examples/sec; 0.174 sec/batch)
2017-03-26 01:29:32.771251: step 99640, loss = 0.75 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:29:34.518574: step 99650, loss = 0.80 (732.5 examples/sec; 0.175 sec/batch)
2017-03-26 01:29:36.242444: step 99660, loss = 0.85 (742.5 examples/sec; 0.172 sec/batch)
2017-03-26 01:29:37.983249: step 99670, loss = 0.72 (735.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:29:39.719358: step 99680, loss = 0.71 (737.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:29:41.441114: step 99690, loss = 0.66 (743.4 examples/sec; 0.172 sec/batch)
2017-03-26 01:29:43.409416: step 99700, loss = 0.50 (650.3 examples/sec; 0.197 sec/batch)
2017-03-26 01:29:44.931933: step 99710, loss = 0.70 (840.7 examples/sec; 0.152 sec/batch)
2017-03-26 01:29:46.634835: step 99720, loss = 0.71 (751.7 examples/sec; 0.170 sec/batch)
2017-03-26 01:29:48.348501: step 99730, loss = 0.68 (746.9 examples/sec; 0.171 sec/batch)
2017-03-26 01:29:50.063648: step 99740, loss = 0.76 (746.3 examples/sec; 0.172 sec/batch)
2017-03-26 01:29:51.790920: step 99750, loss = 0.84 (741.0 examples/sec; 0.173 sec/batch)
2017-03-26 01:29:53.500295: step 99760, loss = 0.73 (748.8 examples/sec; 0.171 sec/batch)
2017-03-26 01:29:55.213177: step 99770, loss = 0.62 (747.3 examples/sec; 0.171 sec/batch)
2017-03-26 01:29:56.937340: step 99780, loss = 0.61 (742.5 examples/sec; 0.172 sec/batch)
2017-03-26 01:29:58.656804: step 99790, loss = 0.64 (744.3 examples/sec; 0.172 sec/batch)
2017-03-26 01:30:00.537980: step 99800, loss = 0.71 (680.8 examples/sec; 0.188 sec/batch)
2017-03-26 01:30:02.127923: step 99810, loss = 0.75 (804.6 examples/sec; 0.159 sec/batch)
2017-03-26 01:30:03.862005: step 99820, loss = 0.75 (738.2 examples/sec; 0.173 sec/batch)
2017-03-26 01:30:05.564738: step 99830, loss = 0.64 (751.7 examples/sec; 0.170 sec/batch)
2017-03-26 01:30:07.278402: step 99840, loss = 0.68 (746.9 examples/sec; 0.171 sec/batch)
2017-03-26 01:30:08.993531: step 99850, loss = 0.59 (746.4 examples/sec; 0.171 sec/batch)
2017-03-26 01:30:10.710788: step 99860, loss = 0.67 (745.3 examples/sec; 0.172 sec/batch)
2017-03-26 01:30:12.416967: step 99870, loss = 0.76 (750.2 examples/sec; 0.171 sec/batch)
2017-03-26 01:30:14.117750: step 99880, loss = 0.69 (752.6 examples/sec; 0.170 sec/batch)
2017-03-26 01:30:15.845865: step 99890, loss = 0.73 (740.7 examples/sec; 0.173 sec/batch)
2017-03-26 01:30:17.735568: step 99900, loss = 0.70 (677.4 examples/sec; 0.189 sec/batch)
2017-03-26 01:30:19.358203: step 99910, loss = 0.68 (788.8 examples/sec; 0.162 sec/batch)
2017-03-26 01:30:21.099107: step 99920, loss = 0.65 (735.4 examples/sec; 0.174 sec/batch)
2017-03-26 01:30:22.834424: step 99930, loss = 0.61 (737.5 examples/sec; 0.174 sec/batch)
2017-03-26 01:30:24.578331: step 99940, loss = 0.61 (734.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:30:26.316724: step 99950, loss = 0.55 (736.3 examples/sec; 0.174 sec/batch)
2017-03-26 01:30:28.043862: step 99960, loss = 0.64 (741.3 examples/sec; 0.173 sec/batch)
2017-03-26 01:30:29.785104: step 99970, loss = 0.73 (735.0 examples/sec; 0.174 sec/batch)
2017-03-26 01:30:31.526447: step 99980, loss = 0.63 (735.1 examples/sec; 0.174 sec/batch)
2017-03-26 01:30:33.271150: step 99990, loss = 0.76 (733.7 examples/sec; 0.174 sec/batch)
